<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>HW1: Regression, Cross-Validation, and Regularization | Introduction to Machine Learning
</title>
  <link rel="canonical" href="https://www.cs.tufts.edu/comp/135/2020f/hw1.html">



  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <script src="https://www.cs.tufts.edu/comp/135/2020f/theme/js/icsFormatter.js"></script>

  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/style.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/custom.css">


<meta name="description" content="Status: RELEASED. Due date: Wed. 09/30 11:59pm Anywhere on Earth (Thu 10/01 at 07:59am in Boston) Overview In this HW, you'll complete two parts in order: First, complete several code tasks, writing Python code based on provided starter code. You'll submit your code as a ZIP â€¦">
</head>

<body>
  <header class="header">
    <nav class="navbar navbar-expand-lg navbar-expand-md navbar-light bg-light">
    <div class="container">
    <div class="row display-flex">
        <div class="col-2 col-sm-2 d-md-none"><!-- hidden if md or lg -->
        <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#my_collapsing_navbar"
            aria-controls="my_collapsing_navbar"
            aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon mw-100"></span>
        </button>
        </div>

        <div class="d-none d-md-block col-md-2">
          <a href="https://www.cs.tufts.edu/comp/135/2020f/">
            <img class="img-fluid mw-100" src=https://www.cs.tufts.edu/comp/135/2020f/images/tufts_ml_logo.png alt="Introduction to Machine Learning">
          </a>
        </div>

        <div class="col-10 col-sm-10 col-md-10">
          <h1 class="text-left" style="word-break:'break-all'">
            <a href="https://www.cs.tufts.edu/comp/135/2020f/">Introduction to Machine Learning</a>
          </h1>

          <p class="text-muted text-left d-none d-md-block mw-100">
            Tufts CS COMP 135 Intro ML | Fall 2020
          </p>



          <div class="collapse navbar-collapse" id="my_collapsing_navbar">
                <ul class="navbar-nav">
                  <li class="nav-item text-left">
                    <a href="index.html">Syllabus</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="schedule.html">Schedule</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="assignments.html">Assignments</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="office_hours.html">Office Hours</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="resources.html">Resources</a>
                  </li>

                </ul>
          </div>


        </div>

    </div>
    </div>
    </nav>

  </header>

  <div class="main">
    <div class="container">
      <h1>HW1: Regression, Cross-Validation, and Regularization
</h1>
      <hr>
<article class="article">
  <div class="content">
        <p style="text-align:right">Last modified: 2020-09-19 13:50 </p>
    <p><strong>Status: RELEASED. </strong></p>
<p><strong>Due date</strong>: Wed. 09/30 11:59pm Anywhere on Earth (Thu 10/01 at 07:59am in Boston)</p>
<h2>Overview</h2>
<p>In this HW, you'll complete two parts in order:</p>
<ul>
<li>First, complete several <em>code tasks</em>, writing Python code based on provided starter code. You'll submit your code as a ZIP to the autograder link below.</li>
<li>Second, complete some <em>analysis</em> tasks (which use your code) and write a <em>report</em>. You'll submit this PDF report separately.</li>
</ul>
<p>As much as possible, we have tried to <em>decouple</em> these parts, so you may successfully complete the report even if some of your code doesn't work.</p>
<p><strong>Turn-in links</strong>:</p>
<ul>
<li>PDF report turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/667550/">https://www.gradescope.com/courses/173055/assignments/667550/</a></li>
<li>ZIP file of source code turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/666260/">https://www.gradescope.com/courses/173055/assignments/666260/</a></li>
<li>Then complete your reflection here: <a href="https://forms.gle/XSuvETkLiv46WQak6">https://forms.gle/XSuvETkLiv46WQak6</a></li>
<li>
<ul>
<li>Reflection requires a <code>tufts.edu</code> G-Suite account: <a href="http://systems.eecs.tufts.edu/logging-into-g-suite/">http://systems.eecs.tufts.edu/logging-into-g-suite/</a></li>
</ul>
</li>
</ul>
<p><strong>Files to Turn In:</strong></p>
<p>PDF report:</p>
<ul>
<li>Prepare a short PDF report (no more than 5 pages; ideally 1 page per problem below).</li>
<li>This document will be manually graded.</li>
<li>Can use your favorite report writing tool (Word or G Docs or LaTeX or ....)</li>
<li>Should be <strong>human-readable</strong>. Do not include code. Do NOT just export a jupyter notebook to PDF.</li>
<li>Should have each subproblem <a href="https://www.youtube.com/watch?v=KMPoby5g_nE&amp;feature=youtu.be&amp;t=43">marked via the in-browser Gradescope annotation tool</a>)</li>
</ul>
<p>ZIP file of source code submitted to autograder should contain:</p>
<ul>
<li>cross_validation.py</li>
<li>performance_metrics.py</li>
<li>LeastSquaresLinearRegression.py</li>
<li>hw1.ipynb</li>
</ul>
<p><strong>Evaluation Rubric:</strong></p>
<p>See the PDF submission portal on Gradescope for the point values of each problem. Generally, tasks with more coding/effort will earn more potential points.</p>
<p><strong>Jump to</strong>: 
<a href="#code-tasks">Code Tasks</a> &nbsp; <a href="#starter-code">Starter Code</a> &nbsp; <a href="#dataset">Dataset</a> &nbsp;
<a href="#problem-1">Report Problem 1</a> &nbsp; <a href="#problem-2">Report Problem 2</a>  &nbsp; <a href="#problem-3">Report Problem 3</a></p>
<h2>Background</h2>
<p>To complete this HW, you'll need some specific knowledge from the following sessions of class:</p>
<ul>
<li>Training Linear Regression (day03)</li>
<li>Polynomial Feature Transformations (day04)</li>
<li>Cross Validation (day04)</li>
<li>Regularized Linear Regression (day05)</li>
</ul>
<h2><a name="code-tasks">Code Tasks </a></h2>
<h4><a name="starter-code"> Starter Code </a></h4>
<p>See the hw1 folder of the public assignments repo for this class:</p>
<p><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw1">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw1</a></p>
<p>This starter code includes a notebook to help you organize your analysis, plus several <code>.py</code> files for core functionality you need to implement yourself.</p>
<h4><a name="code-task-1"> Code Task 1: Edit <code>performance_metrics.py</code> to implement <code>calc_mean_squared_error</code> and <code>calc_mean_absolute_error</code> methods </a></h4>
<p>See the starter code here: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw1/performance_metrics.py">performance_metrics.py</a>.</p>
<p><strong> Task 1(a) </strong> : Implement <code>calc_mean_squared_error</code></p>
<p><strong> Task 1(b) </strong> : Implement <code>calc_mean_absolute_error</code></p>
<p>See the starter code for example inputs and the expected output for both functions.</p>
<h4><a name="code-task-2"> Code Task 2: Edit <code>LeastSquaresLinearRegression.py</code> to implement <code>fit</code> and <code>predict</code> </a></h4>
<p>See the starter code here: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw1/LeastSquaresLinearRegression.py">LeastSquaresLinearRegression.py</a>.
This file defines a <code>LeastSquaresLinearRegressor</code> class with the two key methods of the usual sklearn regression API: <code>fit</code> and <code>predict</code>.</p>
<p>You will edit this file to complete the <code>fit</code> and the <code>predict</code> methods, which will demonstrate your understanding of what goes on "inside" sklearn-like regressor objects.</p>
<p><strong> Task 2(a) </strong> : The <code>fit</code> method should take in a labeled dataset <span class="math">\(\{x_n, y_n\}_{n=1}^N\)</span> and instantiate two instance attributes</p>
<ul>
<li><code>w_F</code> : 1D numpy array, shape (n_features = F,)
    Represents the 'weights'
    Contains float64 entries of the weight coefficients</li>
<li><code>b</code> : scalar float
    Represents the 'bias' or 'intercept'.</li>
</ul>
<p>Nothing should be returned. You're updating the internal state of the object.</p>
<p>These attributes should be set using the formulas discussed in class (day03) for solving the "least squares" optimization problem (finding <span class="math">\(w\)</span> and <span class="math">\(b\)</span> values that minimize squared error on the training set).</p>
<p>Hint: Within a Python class, you can set an attribute like <code>self.b = 1.0</code>. </p>
<p><strong> Task 2(b) </strong> : The <code>predict</code> method** should take in an array of feature vectors <span class="math">\(\{x_n\}_{n=1}^N\)</span> and produce (return) the predicted responses <span class="math">\(\{ \hat{y}(x_n) \}_{n=1}^N\)</span></p>
<p>Recall that for linear regression, we've defined the prediction function as:</p>
<div class="math">$$
\hat{y}(x_n) = b + w^T x_n = b + \sum_{f=1}^F w_f x_{nf}
$$</div>
<h4><a name="code-task-3"> Code Task 3: Edit <code>cross_validation.py</code> to randomly divide data into splits and estimate training and heldout error </a></h4>
<p>See the starter code here: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw1/cross_validation.py">cross_validation.py</a>.</p>
<p><strong> Task 3(a) </strong> : Implement the <code>make_train_and_test_row_ids_for_n_fold_cv</code> function</p>
<p>This function should consume the number of examples, the desired number of folds, and a pseudo-random number generator.
Then, it will produce, for each of the desired number of folds, arrays of <em>integers</em> indicating which rows of the dataset belong to the training set, and which belong to the test set.</p>
<p>See the starter code for detailed specification.</p>
<p><strong> Task 3(b) </strong> : Implement the <code>train_models_and_calc_scores_for_n_fold_cv</code> function</p>
<p>This function will use the procedure from 3(a) to determine the different "folds", and then train a separate model at each fold and return that model's training error and heldout error.</p>
<p>See the starter code for detailed specification.</p>
<h2><a name="dataset"> Dataset: Miles-per-Gallon efficiency of Vehicles </a></h2>
<p>You have been given a data set containing gas mileage, horsepower, and other information for 395 makes and models of vehicles.  For each vehicle, we have the following information:  </p>
<table>
<thead>
<tr>
<th>column name</th>
<th>type</th>
<th>unit</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>horsepower</td>
<td>numeric</td>
<td>hp</td>
<td>engine horsepower</td>
</tr>
<tr>
<td>weight</td>
<td>numeric</td>
<td>lb.</td>
<td>vehicle weight</td>
</tr>
<tr>
<td>cylinders</td>
<td>numeric</td>
<td>#</td>
<td>number of engine cylinders, from 4 to 8</td>
</tr>
<tr>
<td>displacement</td>
<td>numeric</td>
<td>cu. inches</td>
<td>engine displacement</td>
</tr>
<tr>
<td>mpg</td>
<td>numeric</td>
<td>mi. / gal</td>
<td>vehicle miles per gallon</td>
</tr>
</tbody>
</table>
<p>You have been asked to build a predictor for vehicle mileage (mpg) as a function of other vehicle characteristics. </p>
<p>In the starter code, we have provided an existing train/validation/test split of this dataset, stored on-disk in comma-separated-value (CSV) files: x_train.csv, y_train.csv, x_valid.csv, y_valid.csv, x_test.csv, and y_test.csv.  </p>
<p>Get the data here: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw1/data_auto">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw1/data_auto</a></p>
<h2><a name="problem-1">Problem 1: Polynomial Regression - Model Selection on a Fixed Validation Set </a></h2>
<p>For this problem, use the provided training set and validation set</p>
<ul>
<li><code>x_train.csv</code> and <code>y_train.csv</code> contain features and outcomes for 192 examples</li>
<li><code>x_valid.csv</code> and <code>y_valid.csv</code> contain features and outcomes for 100 examples</li>
</ul>
<p>Your goal is to determine which polynomial transformation yields the best predictive performance.</p>
<p>Follow the starter notebook. Your code should chain together the <code>PolynomialFeatures</code> and <code>LinearRegression</code> class provided by <code>sklearn</code>.</p>
<p><strong> Implementation Step 1A: </strong>
Fit a linear regression model to a polynomial feature transformation of the provided training set at each of these possible degrees: [1, 2, 3, 4, 5, 6, 7]. For each hyperparameter setting, record the training set error and the validation set error.</p>
<p><strong> Implementation Step 1B: </strong> 
Select the model hyperparameters that <em>minimize</em> your fixed validation set error. Using your already-trained LinearRegression model with these best hyperparameters, compute error on the <em>test</em> set. Save this test set error value for later.</p>
<p><strong> Figure 1 in Report: </strong>
Make a line plot of mean-squared error on y-axis vs. polynomial degree on x-axis.
Show two lines, one for error on training set (use style 'b:', a dotted blue line) and another line for error on validation set (use style 'rs-', a solid red line with square markers). Set the y-axis limits between [0, 70].</p>
<p><strong> Short Answer 1a in Report: </strong> 
If your goal is to select a model that will generalize well to new data from the same distribution, which polynomial degree do you recommend based on this assessment?
Are there other degrees that seem to give nearly the same performance?</p>
<p><strong> Short Answer 1b in Report: </strong>
At some point, the mean squared error on the training set <em>should</em> become very close to zero (say, within 0.5 or so). 
At what degree value do you observe this happening for this <em>particular</em> training dataset? 
What technical argument can you make to justify this (e.g. for this dataset, why is training error zero at X degrees but not X-1 degrees?)</p>
<p><strong> Short Answer 1c in Report: </strong>
You'll notice that our pipelines include a <em>preprocessing</em> step that rescales each feature column to be in the unit interval from 0 to 1. 
Why is this necessary for this particular dataset?
What happens (in terms of both training error and test error) if this step is omitted?</p>
<p><strong> Short Answer 1d in Report: </strong>
Consider the model with degree 6. Print out its intercept coeficient value, as well as the minimum and maximum weight coeficient value (out of all the features).
What do you notice about these values? How might they be connected to the training and validation set performance you observe in Figure 1?</p>
<h2><a name="problem-2">Problem 2: Polynomial Regression - Model Selection with Cross-Validation </a></h2>
<p>For this problem, you'll again use the provided training set and validation sets. However, you'll <em>merge</em> these into a large "development" set that contains 292 examples total.</p>
<p>We'll then use 10-fold cross validation to obtain good estimates of heldout performance.</p>
<p>Your goal is again to determine which polynomial transformation yields the best predictive performance.</p>
<p>Follow the starter notebook. Your code should chain together the <code>PolynomialFeatures</code> and <code>LinearRegression</code> class provided by <code>sklearn</code>.</p>
<p><strong> Implementation Step 2A: </strong>
For each possible polynomial degree used in Problem 1, train and evaluate a linear regression model across the entire train+validation set using 10-fold cross validation. Use the CV methods you implemented in <code>cross_validation.py</code>. Your 10-fold CV procedure will give you an estimate of the training error and heldout validation error (averaged across all folds). </p>
<p><strong> Implementation Step 2B: </strong>
Select the model hyperparameters that <em>minimize</em> your estimated cross-validation error. Using these best hyperparameters, retrain the model using the full development set and then compute that (retrained) model's error on the test set.
Save this test set error value for later.</p>
<p><strong> Figure 2 in Report: </strong>
Make a line plot of mean-squared error on y-axis vs. polynomial degree on x-axis.
Show two lines, one for error on training set (use style 'bd:', a dotted blue line with diamond markers) and another line for error on validation set (use style 'rs-', a solid red line with square markers). Set the y-axis limits between [0, 70]. Your code should chain together the <code>PolynomialFeatures</code> and <code>LinearRegression</code> class provided by <code>sklearn</code>, while using your own implementation of <code>cross_validation.py</code>.</p>
<p><strong> Short Answer 2a in Report: </strong> 
If your goal is to select hyperparameters for your pipeline that will generalize well to new data from the same distribution, which polynomial degree do you recommend based on this assessment?
Are there other degrees that seem to give nearly the same performance?
What (if anything) changed from 1a?</p>
<p><strong> Short Answer 2b in Report: </strong>
What are two benefits of using cross validation when compared to a fixed validation set (as in Problem 1)?</p>
<p><strong> Short Answer 2c in Report: </strong>
What are two drawbacks to using cross validation when compared to a fixed validation set (as in Problem 1)?</p>
<p><strong> Short Answer 2d in Report: </strong>
Remember, your task is to develop models that will accurately predict miles per gallon given some basic features of car engines.
Suppose your available data is augmented so that each example is associated with a specific <em>manufacturer</em> (e.g. each row of your development set could be labeled 'Toyota' or 'Ford' or 'Hyundai'). 
You have data labeled with 10 different manufacturers.
You'd like your prediction to be accurate for new manufacturers, that you do not have available in your training set.
(Thus, your regression model should not use manufacturer label as a feature).
How would you suggest we change our cross validation procedure to do better at this task?</p>
<h2><a name="problem-3">Problem 3: Polynomial Regression with L2 Regularization - Model Selection with Cross-Validation </a></h2>
<p>For this problem, you'll again use 10 fold CV to estimate heldout data.</p>
<p>However, now we'll look at a regularized version of linear regression, that may be better at avoiding overfitting.</p>
<p>Follow the starter notebook. Throughout problem 3, your code should chain together the <code>PolynomialFeatures</code> and <code>Ridge</code> class provided by <code>sklearn</code>.</p>
<p><strong> Implementation Step 3A: </strong>
Consider the following set of possible <code>alpha</code> hyperparameters for <code>Ridge</code>:</p>
<div class="highlight"><pre><span></span><span class="n">alpha_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span> <span class="o">#</span> <span class="mi">10</span><span class="o">^-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="o">^-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="o">^-</span><span class="mi">4</span><span class="p">,</span> <span class="p">...</span> <span class="mi">10</span><span class="o">^-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="o">^</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="o">^</span><span class="mi">1</span><span class="p">,</span> <span class="p">...</span> <span class="mi">10</span><span class="o">^</span><span class="mi">6</span>
</pre></div>


<p>For each possible <code>alpha</code> value as well as each possible polynomial degree used in Problem 1, train and evaluate a <code>Ridge</code> regression model across the entire train+validation set using 10-fold cross validation. Use the CV methods you implemented in <code>cross_validation.py</code>. For each possible hyperparameter (alpha value and degree value), your 10-fold CV procedure will give you an estimate of the training error and heldout validation error (averaged across all folds). </p>
<p><strong> Implementation Step 3B: </strong>
Select the model hyperparameters that <em>minimize</em> your estimated cross-validation error. Using these best hyperparameters, retrain the model using the full development set (concatenating the predefined training and validation sets). Then compute that (retrained) model's error on the test set.
Save this test set error value for later.</p>
<p><strong> Figure 3 in Report: </strong>
Show the training and validation error as a function of degree, for 3 possible <code>alpha</code> values: 1e-5, 0.1, and 1000. 
Make one figure with 3 subplots, one per alpha value (see example starter notebook). 
In each subplot, make a line plot with mean-squared error on y-axis vs. polynomial degree on x-axis.
Show two lines, one for error on training set (use style 'bd:', a dotted blue line with diamond markers) and another line for error on validation set (use style 'rs-', a solid red line with square markers). Set the y-axis limits between [0, 70].</p>
<p><strong> Short Answer 3a in Report: </strong>
If your goal is to select hyperparameters for your pipeline that will generalize well to new data from the same distribution, which polynomial degree and alpha values do you recommend based on this assessment?
Are there other values that seem to give nearly the same performance?</p>
<p><strong> Short Answer 3b in Report: </strong>
Your colleague suggests that you can determine the regularization strength <code>alpha</code> by minimizing the following loss on the <em>training</em> set:</p>
<div class="math">$$
\text{min}_{w \in \mathbb{R}^F, b \in \mathbb{R}, \alpha \ge 0}
\quad \sum_{n=1}^N (y_n - \hat{y}(x_n, w, b))^2 + \alpha \sum_{f=1}^F w_f^2
$$</div>
<p>What value of <span class="math">\(\alpha\)</span> would you pick if you did this? Why is this problematic if your goal is to generalize to new data well?</p>
<h2><a name="problem-4">Problem 4: Comparison of methods on the test set</a></h2>
<p><strong> Table 4 in Report: </strong></p>
<p>In one neat table, please compare the <em>test set</em> root-mean-squared-error (RMSE) performance for the following regressors:</p>
<ul>
<li>Baseline: A predictor that always guesses the <em>mean</em> <span class="math">\(y\)</span> value of the training set, regardless of the new test input</li>
<li>The best LinearRegression pipeline selected to minimize fixed validation set error (from 1B)</li>
<li>The best LinearRegression pipeline selected to minimize 10-fold cross validation set error (from 2B)</li>
<li>The best Ridge pipeline selected to minimize 10-fold cross validation set error (from 3B)</li>
</ul>
<p>Include a caption that summarizes the major conclusions that a reader should take away from this comparison.</p>
<h2>Helpful hints and best practices for preparing a report</h2>
<p>Across all the problems here, be sure that:</p>
<ul>
<li>All plots include readable axes labels and legends if needed when multiple lines are shown.</li>
<li>All figures include <em>captions</em> providing complete sentence summaries of the figure.</li>
<li>Generally, all tables should only report floating-point values up to 3 decimal places in precision.</li>
<li>
<ul>
<li>That is, if your error is 17.123456789, just display "17.123". Make it easy on your reader's eyes. </li>
</ul>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">



        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          MIT License
          /
          <a href="https://github.com/tufts-ml/tufts_ml_website">
          Source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">Powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>