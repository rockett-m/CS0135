{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "published-superior",
   "metadata": {},
   "source": [
    "## Using sklearn for basic data transformation, cross-validation\n",
    "\n",
    "Many common machine learning operations and procedures are already encoded in various `sklearn` libraries.  Here, we see one way of handling a couple basic tasks—data transformation and cross-validation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sublime-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-tribune",
   "metadata": {},
   "source": [
    "### A built-in data set\n",
    "\n",
    "There are a number of existing data-sets in `sklearn`, many drawn from real-world data-sources. Here, we use the Wisconsin breast cancer set, which allows high-accuracy classification with pretty simple models:  \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "\n",
    "Here, the data is loaded into a dataframe; in basic form, it consists of 569 data-points, each characterized by 30 real-valued features in a number of different units. For more information about the data, see the UCI Machine Learning Repository:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "built-stevens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_breast_cancer(as_frame = True)\n",
    "frame = dataset.frame\n",
    "X = frame.iloc[:,:-1]\n",
    "y = frame.iloc[:,-1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-burns",
   "metadata": {},
   "source": [
    "### A basic perceptron model: one test\n",
    "\n",
    "A simple perceptron model will often do an OK job on this data.  Performance can vary quite a bit, however, depending upon the exact test/train split we get, which by default is randomized across runs.  Performance can also be hampered somewhat by the fact that the original data is not scaled, and displays different orders of magnitude.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "derived-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Classify with base data, 1 split\n",
      "-----------------\n",
      "Train accuracy:  0.9090909090909091\n",
      "Test accuracy:  0.9254385964912281\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------\\nClassify with base data, 1 split\\n-----------------\")\n",
    "\n",
    "# Start by converting data to Numpy arrays for convenience\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "model = Perceptron()\n",
    "model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(pred_train, y_train)\n",
    "acc_test = accuracy_score(pred_test, y_test)\n",
    "print(\"Train accuracy: \", acc_train)\n",
    "print(\"Test accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-norfolk",
   "metadata": {},
   "source": [
    "### Scaling data features\n",
    "\n",
    "We can use the exact same test/train split, but scale all our features to the $[0,1]$ range, based upon the training set (i.e., each is scaled according to the maximum/minimum values in that set).  This simulates what we would do in practice, since our incoming test data would not be known in advance.\n",
    "\n",
    "Scaling to uniform range tends to give significantly better performance, since coefficient-weights on large-magnitude features are less likely to exert undue influence in the solution process.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "creative-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Classify with scaled data, 1 split\n",
      "-----------------\n",
      "Train accuracy:  0.9824046920821115\n",
      "Test accuracy:  0.9692982456140351\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------\\nClassify with scaled data, 1 split\\n-----------------\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "pred_test = model.predict(X_scaled_test)\n",
    "\n",
    "acc_train = accuracy_score(pred_train, y_train)\n",
    "acc_test = accuracy_score(pred_test, y_test)\n",
    "print(\"Train accuracy: \", acc_train)\n",
    "print(\"Test accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-lloyd",
   "metadata": {},
   "source": [
    "### Cross-validation testing\n",
    "\n",
    "Rather than a single randomized test/train split, we can automate the process somewhat by using $k$-fold cross validation techniques.  Like most things, there are a number of ways of handling this; this is one that is pretty basic, using a `KFold` object to generate splits of our data automatically.  Here, we do this for our basic, non-scaled data, using 5 folds.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "physical-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Classify with base data, 5 folds\n",
      "-----------------\n",
      "Train accuracy:  0.7714285714285715\n",
      "Test accuracy:  0.8771929824561403\n",
      "Train accuracy:  0.8945054945054945\n",
      "Test accuracy:  0.8859649122807017\n",
      "Train accuracy:  0.865934065934066\n",
      "Test accuracy:  0.8771929824561403\n",
      "Train accuracy:  0.9032967032967033\n",
      "Test accuracy:  0.9298245614035088\n",
      "Train accuracy:  0.9166666666666666\n",
      "Test accuracy:  0.911504424778761\n",
      "\n",
      "Average train accuracy:  0.8703663003663002\n",
      "Average test accuracy:  0.8963359726750506\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "\n",
    "k = 5\n",
    "kfold = KFold(n_splits=k)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(pred_train, y_train)\n",
    "    acc_test = accuracy_score(pred_test, y_test)\n",
    "    print(\"Train accuracy: \", acc_train)\n",
    "    print(\"Test accuracy: \", acc_test)\n",
    "    \n",
    "    train_scores.append(acc_train)\n",
    "    test_scores.append(acc_test)\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", np.average(train_scores))\n",
    "print(\"Average test accuracy: \", np.average(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-premiere",
   "metadata": {},
   "source": [
    "### Combining data transformation and cross-validation\n",
    "\n",
    "We can also do our $k$-fold cross-validation of data after scaling each feature to $[0,1]$.  This gives us the best accuracy and most robust expected performance.  We scale the entire data-set first, and then do the cross-validation by splitting it up.\n",
    "    \n",
    "**NB**: the `KFold.split()` function can handle data in either pandas data-frame or \n",
    "    basic array-based format, and several more, like basic Python lists, so we could have \n",
    "    left it in pandas form, only changing a bit how we would index elements.  \n",
    "    In general, most of sklearn is pretty good at handling all sorts of basic linear data.  \n",
    "    See the entry for 'array-like' at:\n",
    "    \n",
    "https://scikit-learn.org/stable/glossary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "undefined-outreach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Classify with scaled data, 5 folds\n",
      "-----------------\n",
      "Train accuracy:  0.9846153846153847\n",
      "Test accuracy:  0.956140350877193\n",
      "Train accuracy:  0.9406593406593406\n",
      "Test accuracy:  0.9210526315789473\n",
      "Train accuracy:  0.978021978021978\n",
      "Test accuracy:  0.9736842105263158\n",
      "Train accuracy:  0.978021978021978\n",
      "Test accuracy:  0.9824561403508771\n",
      "Train accuracy:  0.9649122807017544\n",
      "Test accuracy:  0.9646017699115044\n",
      "\n",
      "Average train accuracy:  0.9692461924040872\n",
      "Average test accuracy:  0.9595870206489675\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------\\nClassify with scaled data, 5 folds\\n-----------------\")\n",
    "\n",
    "# If we like, we can combine the fit() and transform() calls \n",
    "# into a single fit_transform() call.\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_idx,:], X_scaled[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(pred_train, y_train)\n",
    "    acc_test = accuracy_score(pred_test, y_test)\n",
    "    print(\"Train accuracy: \", acc_train)\n",
    "    print(\"Test accuracy: \", acc_test)\n",
    "    \n",
    "    train_scores.append(acc_train)\n",
    "    test_scores.append(acc_test)\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", np.average(train_scores))\n",
    "print(\"Average test accuracy: \", np.average(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-northwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
