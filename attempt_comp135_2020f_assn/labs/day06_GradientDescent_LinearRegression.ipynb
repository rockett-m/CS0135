{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day06: Gradient Descent for Linear Regression\n",
    "\n",
    "# Objectives\n",
    "\n",
    "* Learn how to fit weight parameters of Linear Regression to a simple dataset via gradient descent\n",
    "* Understand impact of step size\n",
    "* Understand impact of initialization\n",
    "\n",
    "\n",
    "# Outline\n",
    "* [Part 1: Loss and Gradient for 1-dim. Linear Regression](#part1)\n",
    "* [Part 2: Gradient Descent Algorithm in a few lines of Python](#part2)\n",
    "* [Part 3: Debugging with Trace Plots](#part3)\n",
    "* [Part 4: Selecting the step size](#part4)\n",
    "* [Part 5: Selecting the initialization](#part5)\n",
    "* [Part 6: Using SciPy's built-in routines](#part6)\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "\n",
    "* Gradient descent is a simple algorithm that can be implemented in a few lines of Python\n",
    "* * Practical issues include selecting step size and initialization\n",
    "* Step size matters a lot\n",
    "* * Need to select carefully for each problem\n",
    "\n",
    "* Initialization of the parameters can matter too!\n",
    "\n",
    "* scipy offers some useful tools for gradient-based optimization\n",
    "* * scipy's toolbox cannot do scalable \"stochastic\" methods (requires a modest size dataset, not too big)\n",
    "* * \"L-BFGS-B\" method is highly recommended if you have your loss and gradient functions available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple dataset:   y = 1.234 * x + noise\n",
    "\n",
    "We will *intentionally* create a toy dataset where we know that a good solution has slope near 1.234.\n",
    "\n",
    "Naturally, the best slope for the finite dataset of N=100 examples we create won't be exactly 1.234 (because of the noise added plus the fact that our dataset size is limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(N=100, slope=1.234, noise_stddev=0.1, random_state=0):\n",
    "    random_state = np.random.RandomState(int(random_state))\n",
    "\n",
    "    # input features\n",
    "    x_N = np.linspace(-2, 2, N)\n",
    "    \n",
    "    # output features\n",
    "    y_N = slope * x_N + random_state.randn(N) * noise_stddev\n",
    "    \n",
    "    return x_N, y_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_N, y_N = create_dataset(N=50, noise_stddev=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "plt.plot(x_N, y_N, 'k.');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Gradient Descent for 1-dim. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "Consider the *simplest* linear regression model. A single weight parameter $w \\in \\mathbb{R}$ representing the slope of the prediction line. No bias/intercept.\n",
    "\n",
    "To make predictions, we just compute the weight multiplied by the input feature\n",
    "$$\n",
    "\\hat{y}(x) = w \\cdot x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the total *squared error* across all N observed data examples (input features $x_n$, output responses $y_n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\min_{w \\in \\mathbb{R}} ~~ &\\ell(w)\n",
    "    \\\\\n",
    "    \\text{calc_loss}(w) = \\ell(w) &= \\sum_{n=1}^N (y_n - w x_n)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1A: Complete the code below\n",
    "\n",
    "You should make it match the math expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(w):\n",
    "    ''' Compute loss for slope-only least-squares linear regression\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    w : float\n",
    "        Value of slope parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Sum of squared error loss at provided w value\n",
    "    '''\n",
    "    yhat_N = x_N * w\n",
    "    sum_squared_error = 0.0 # todo compute the sum of squared error between y and yhat\n",
    "    return sum_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the gradient function\n",
    "\n",
    "\\begin{align}\n",
    "\\text{calc_grad}(w) = \\ell'(w) &= \\frac{\\partial}{\\partial w} [ \\sum_{n=1}^N (y_n - w x_n)^2] \n",
    "\\\\\n",
    "&= \\sum_{n=1}^N 2 (y_n - w x_n) (-x_n)\n",
    "\\\\\n",
    "&= 2 \\sum_{n=1}^N (w x_n - y_n) (x_n)\n",
    "\\\\\n",
    "&= 2  w \\left( \\sum_{n=1}^N x_n^2 \\right) - 2 \\sum_{n=1}^N y_n x_n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we've implemented the gradient calculation in code for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(w):\n",
    "    ''' Compute gradient for slope-only least-squares linear regression\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    w : float\n",
    "        Value of slope parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    g : float\n",
    "        Value of derivative of loss function at provided w value\n",
    "    '''\n",
    "    g = 2.0 * w * np.sum(np.square(x_N)) - 2.0 * np.sum(x_N * y_N)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss evaluated at each w from -3 to 8\n",
    "\n",
    "We should see a \"bowl\" shape with one *global* minima, because our optimization problem is \"convex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grid = np.linspace(-3, 8, 300) # create array of 300 values between -3 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grid = np.asarray([calc_loss(w) for w in w_grid])\n",
    "plt.plot(w_grid, loss_grid, 'b.-');\n",
    "plt.xlabel('w');\n",
    "plt.ylabel('loss(w)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1b: Visually, at what value of $w$ does the loss function have a minima? Is it near where you would expect (hint: look above for the \"true\" slope value used to generate the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1c: Write NumPy code to identify which entry in the w_grid array corresponds to the lowest entry in the loss_grid array\n",
    "\n",
    "Hint: use np.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check: plot gradient evaluated at each w from -3 to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_grid = np.asarray([calc_grad(w) for w in w_grid])\n",
    "plt.plot(w_grid, grad_grid, 'b.-');\n",
    "plt.xlabel('w');\n",
    "plt.ylabel('grad(w)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1d: Visually, at what value of $w$ does the gradient function cross zero? Is it the same place as the location of the minimum in the loss above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO interpret the graph above and write your answer here, then discuss with your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1d: Numerically, at which value of w does grad_grid cross zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might try to estimate numerically where the gradient crosses zero.\n",
    "\n",
    "We could do this in a few steps:\n",
    "\n",
    "1) Compute the distance from each gradient in `grad_grid` to 0.0 (we could use just absolute distance)\n",
    "\n",
    "2) Find the index of `grad_grid` with smallest distance (using `np.argmin`)\n",
    "\n",
    "3) Plug that index into `w_grid` to get the $w$ value corresponding to that zero-crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_from_zero_G = np.abs(grad_grid - 0.0)\n",
    "\n",
    "zero_cross_index = 0 # TODO fix me for step 2 above\n",
    "\n",
    "print(\"Zero crossing occurs at w = %.4f\" % w_grid[0]) # TODO fix me for step 3 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent (GD) as an algorithm in Python\n",
    "\n",
    "\n",
    "### Define minimize_via_grad_descent algorithm\n",
    "\n",
    "Can you understand what each step of this algorithm does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_via_grad_descent(calc_loss, calc_grad, init_w=0.0, step_size=0.001, max_iters=100):\n",
    "    ''' Perform minimization of provided loss function via gradient descent\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    calc_loss : function\n",
    "    calc_grad : function\n",
    "    init_w : float\n",
    "    step_size : float\n",
    "    max_iters : positive int\n",
    "    \n",
    "    Return\n",
    "    ----\n",
    "    wopt: float\n",
    "        array of optimized weights that approximately gives the least error\n",
    "    info_dict : dict\n",
    "        Contains information about the optimization procedure useful for debugging\n",
    "        Entries include:\n",
    "        * trace_loss_list : list of loss values\n",
    "        * trace_grad_list : list of gradient values\n",
    "    '''\n",
    "    w = 1.0 * init_w \n",
    "    grad = calc_grad(w)\n",
    "\n",
    "    # Create some lists to track progress over time (for debugging)\n",
    "    trace_loss_list = []\n",
    "    trace_w_list = []\n",
    "    trace_grad_list = []\n",
    "\n",
    "    for iter_id in range(max_iters):\n",
    "        if iter_id > 0:\n",
    "            w = w - step_size * grad\n",
    "        \n",
    "        loss = calc_loss(w)\n",
    "        grad = calc_grad(w)    \n",
    "\n",
    "        print(\"  iter %5d/%d | w  % 13.5f | loss % 13.4f | grad % 13.4f\" % (\n",
    "            iter_id, max_iters, w, loss, grad))\n",
    "    \n",
    "        trace_loss_list.append(loss)\n",
    "        trace_w_list.append(w)\n",
    "        trace_grad_list.append(grad)\n",
    "    \n",
    "    wopt = w\n",
    "    info_dict = dict(\n",
    "        trace_loss_list=trace_loss_list,\n",
    "        trace_w_list=trace_w_list, \n",
    "        trace_grad_list=trace_grad_list)\n",
    "    \n",
    "    return wopt, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2a: Which line of the above function does the *parameter update* happen?\n",
    "\n",
    "Remember, in math, the parameter update of gradient descent is this:\n",
    "$$\n",
    "w \\gets w - \\alpha \\nabla_w \\ell(w)\n",
    "$$\n",
    "\n",
    "where $\\alpha > 0$ is the step size.\n",
    "\n",
    "In words, this math says *move* the parameter $w$ from its current value a *small step* in the \"downhill\" direction (indicated by gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO write down here which line above *you* think it is, then discuss with your group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it! Run GD with step_size = 0.001\n",
    "\n",
    "Running the cell below will have the following effects:\n",
    "\n",
    "1) one line will be printed for every iteration, indicating the current w value and its associated loss\n",
    "\n",
    "2) the \"optimal\" value of w will be stored in the variable named `wopt` returned by this function\n",
    "\n",
    "3) a dictionary of information useful for debugging will be stored in the `info_dict` returned by this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wopt, info_dict = minimize_via_grad_descent(calc_loss, calc_grad, step_size=0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2b: Does it appear from the *loss* values in trace above that the GD procedure converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2c: Does it appear from the *parameter* values in trace above that the GD procedure converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2d: What exactly is the gradient of the returned \"optimal\" value of w?\n",
    "\n",
    "Use your `calc_grad` function to check the result. What is the gradient of the returned `wopt`?\n",
    "\n",
    "Does this look totally converged? Can you find a $w$ value that would be even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call calc_grad on the return value from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Diagnostic plots for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some trace functions.\n",
    "\n",
    "Whenever you run gradient descent, an *excellent* debugging strategy is the ability to plot the loss, the gradient magnitude, and the parameter of interest at every step of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=False, figsize=(18,3.6))\n",
    "\n",
    "axes[0].plot(info_dict['trace_loss_list']);\n",
    "axes[0].set_title('loss');\n",
    "axes[1].plot(info_dict['trace_grad_list']);\n",
    "axes[1].set_title('grad');\n",
    "axes[2].plot(info_dict['trace_w_list']);\n",
    "axes[2].set_title('w');\n",
    "\n",
    "plt.xlim([0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3a: What value do we expect the *loss* to converge to? Should it always be zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3b: What value do we expect the *gradient* to converge to? Should it always be zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Larger step sizes\n",
    "\n",
    "## Try with larger step_size = 0.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wopt, info_dict = minimize_via_grad_descent(calc_loss, calc_grad, step_size=0.014);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=False, figsize=(12,3))\n",
    "\n",
    "axes[0].plot(info_dict['trace_loss_list'], '.-');\n",
    "axes[0].set_title('loss');\n",
    "axes[1].plot(info_dict['trace_grad_list'], '.-');\n",
    "axes[1].set_title('grad');\n",
    "axes[2].plot(info_dict['trace_w_list'], '.-');\n",
    "axes[2].set_title('w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4a: What happens here? How is this step size different than in Part 3 above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO discuss with your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with even larger step size 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wopt, info_dict = minimize_via_grad_descent(calc_loss, calc_grad, step_size=0.1, max_iters=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3b: What happens here with this even larger step size? Is it converging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3c: What is the largest step size you can get to converge reasonably?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try some other step sizes here\n",
    "wopt, info_dict = minimize_via_grad_descent(calc_loss, calc_grad, step_size=0) # TODO fix step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Sensitivity to initial conditions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5a: Try to call the defined procedure with a different initial condition for $w$. What happens?\n",
    "\n",
    "You could try $w = 5.0$ or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try some other initial condition for init_w\n",
    "wopt2, info_dict2 = minimize_via_grad_descent(calc_loss, calc_grad, init_w=0, step_size=0.001, max_iters=10) # TODO fix step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5b: Try again with another initial value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try some other initial condition for init_w\n",
    "wopt3, info_dict3 = minimize_via_grad_descent(calc_loss, calc_grad, init_w=0, step_size=0.001, max_iters=10) # TODO fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5c: Make a trace plot\n",
    "\n",
    "Make a trace plot showing convergence from multiple different starting values for $w$. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Using scipy's built-in gradient optimization tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at SciPy's built in minimization toolbox\n",
    "\n",
    "<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize>\n",
    "\n",
    "We'll use \"L-BFGS\", a second-order method that uses the function and its gradient.\n",
    "\n",
    "This is a \"quasi-newton\" method, which you can get an intuition for here:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = scipy.optimize.minimize(calc_loss, 0.0, jac=calc_grad, method='L-BFGS-B')\n",
    "\n",
    "# Returns an object with several fields, let's print the result to get an idea\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(result.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_w = result.x\n",
    "print(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
