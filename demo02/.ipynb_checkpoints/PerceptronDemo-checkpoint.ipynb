{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our own perceptron learner\n",
    "\n",
    "Here, we will see an implementation of perceptron learning, from scratch, on a synthetic classification data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "from random import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define initial weights\n",
    "Here, we set each initial weight randomly as $w_i \\in (-1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight():\n",
    "    weight = random()\n",
    "    if (random() < 0.5):\n",
    "        weight = -weight\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the separator line, given weights\n",
    "In general, given $\\mathbf{w} = (w_0, w_1, \\ldots, w_n)$ we want to define the separator line using the basic linear solution $\\mathbf{w} \\cdot \\mathbf{x} = 0$.\n",
    "\n",
    "Assuming two-dimensional data: $\\mathbf{x} = (x_1, x_2) \\qquad \\mathbf{w} = (w_0, w_1, w_2)$\n",
    "\n",
    "The equation of interest is: $w_0 + w_1 x_1 + w_2 x_2 = 0$\n",
    "\n",
    "We can then use basic algebra to solve for the $x_1$ and $x_2$ intercepts: \n",
    "$x_1^i = -w_0 / w_1 \\qquad x_2^i = -w_0 / w_2$\n",
    "\n",
    "Solving for the slope of the line between the intercepts gives us the separator: \n",
    "$x_2 = -(w_1/w_2)x_1 + -(w_0/w_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_separator(weights):\n",
    "    slope = -(weights[1] / weights[2])\n",
    "    intercept = -(weights[0] / weights[2]) \n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build simple prediction function\n",
    "For the perceptron, given a set of weights $\\mathbf{w}$, we simply compute the linear sum $\\mathbf{w} \\cdot \\mathbf{x}$, and use the threshold hypothesis function:\n",
    "$$h_\\mathbf{w} =\n",
    "\\begin{cases}\n",
    "    1 & \\mathbf{w} \\cdot \\mathbf{x} \\geq 0 \\\\\n",
    "    0 & \\text{else } (\\mathbf{w} \\cdot \\mathbf{x} < 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(x_data, weights):\n",
    "    linear_sum = weights[0]\n",
    "    for i in range(1, len(weights)):\n",
    "        linear_sum += weights[i] * x_data[i-1]\n",
    "\n",
    "    return 1 if linear_sum >= 0.0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron learning with one-item stochastic updates\n",
    "The learning algorithm iterates until it either has a perfect linear separator, or until it reaches some maximum number of iterations.  (You could play with that parameter, as well as the value of the learning rate `alpha`, including making the latter adaptive, diminishing over time.)\n",
    "\n",
    "On each iteration, we take some misclassified element $\\mathbf{x}_i$ and then update each weight using:\n",
    "$$w_j \\leftarrow w_j + \\alpha(y_i - h_\\mathbf{w}(\\mathbf{x}_i)) \\times x_{i,j}$$\n",
    "(and remembering that we treat the bias weight $w_0$ as if there is some dummy feature $x_0 = 1$).\n",
    "\n",
    "**Note**: It can be tempting to think of this process of updates to minimize error as \"gradient descent.\" While the basic intuition is not different, this would not technically be true.  In gradient descent, as can be done in linear regression or other approaches we see later (e.g. logistic regression), we have a smoothly differentiable loss function, and the rules to minimize the weights are derived according to principles of calculus, based upon the derivative of the loss with respect to the hypothesis function that generates our regrssion/classification outputs.  \n",
    "\n",
    "In the case of the perceptron, however, our hypothesis function involves applying a hard 0/1 threshold, and is not smoothly differentiable (in most places the derivative of the loss is 0, else undefined).  Instead, a different proof can be supplied to show that the iterative process of doing weight updates steadily reduces the magnitude of overall error.  Our goal is the same—reduction in error by updating weights—but the mathematics behind it are importantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_update(x_data, y_data, weights, alpha=0.1, max_iterations=10_000):\n",
    "    iter = 0\n",
    "    predictions = [make_prediction(x, weights) for x in x_data]\n",
    "    incorrect = np.nonzero(predictions != y_data)[0]\n",
    "    \n",
    "    while (iter < max_iterations) and incorrect.size:\n",
    "        wrong_index = np.random.choice(incorrect)\n",
    "        loss = y_data[wrong_index] - predictions[wrong_index]\n",
    "        weights[0] += alpha * loss\n",
    "        for i in range(1, len(weights)):\n",
    "            weights[i] += alpha * loss * x_data[wrong_index][i-1]      \n",
    "        iter += 1\n",
    "        predictions = [make_prediction(x, weights) for x in x_data]\n",
    "        incorrect = np.nonzero(predictions != y_N)[0]\n",
    "        \n",
    "    print(\"Iterations to convergence: \", iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple dataset   \n",
    "We create a simple two-dimensional data-set of 50 points $\\mathbf{x} = (x_1, x_2)$, using the `sklearn` function `make_classification`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
    "\n",
    "This is pretty self-explanatory; the data is randomized into classes with numeric labels starting at 0, and the `class_sep` parameter controls how easy it is to separate the data, with bigger numbers meaning that we are more likely to get linearly separable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_N, y_N = make_classification(n_samples=50, n_features=2, n_redundant=0, \n",
    "                               n_clusters_per_class=1, class_sep=1.1)\n",
    "class0_x_N = x_N[y_N==0]\n",
    "class1_x_N = x_N[y_N==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(class0_x_N[:,0], class0_x_N[:,1], c='r', marker='x', label='Class 0')\n",
    "plt.scatter(class1_x_N[:,0], class1_x_N[:,1], c='b', marker='o', label='Class 1')\n",
    "plt.legend()\n",
    "plt.title(\"Initial data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([random_weight() for i in range(len(x_N[0]) + 1)])\n",
    "print(\"Initial weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(class0_x_N[:,0], class0_x_N[:,1], c='r', marker='x', label='Class 0')\n",
    "plt.scatter(class1_x_N[:,0], class1_x_N[:,1], c='b', marker='o', label='Class 1')\n",
    "\n",
    "top, bottom = plt.gca().get_ylim()\n",
    "slope, intercept = define_separator(weights)\n",
    "x1s = np.array(plt.gca().get_xlim())\n",
    "x2s = slope * x1s + intercept\n",
    "plt.plot(x1s, x2s, label='Separator')\n",
    "plt.gca().set_ylim(top, bottom)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Data with random separator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_update(x_N, y_N, weights)\n",
    "print(\"Final weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(class0_x_N[:,0], class0_x_N[:,1], c='r', marker='x', label='Class 0')\n",
    "plt.scatter(class1_x_N[:,0], class1_x_N[:,1], c='b', marker='o', label='Class 1');\n",
    "\n",
    "top, bottom = plt.gca().get_ylim()\n",
    "slope, intercept = define_separator(weights)\n",
    "x1s = np.array(plt.gca().get_xlim())\n",
    "x2s = slope * x1s + intercept\n",
    "plt.plot(x1s, x2s, label='Separator')\n",
    "plt.gca().set_ylim(top, bottom)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Data with final separator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average performance\n",
    "Here, we consider how well our simple classifier does over mutliple possible iterations, starting from a number of different initial weights, chosen randomly each time, on different random data sets.  Overall performance is quite strong, especially considering that some sets may not be linerly separable, and error is inevitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy = []\n",
    "for i in range(1, 20):\n",
    "    x_N, y_N = make_classification(n_samples=50, n_features=2, n_redundant=0, \n",
    "                               n_clusters_per_class=1, class_sep=1.1)\n",
    "    weights = np.array([random_weight() for i in range(len(x_N[0]) + 1)])\n",
    "    perceptron_update(x_N, y_N, weights)\n",
    "    \n",
    "    predictions = [make_prediction(x, weights) for x in x_N]\n",
    "    correct = np.nonzero(predictions == y_N)[0]\n",
    "    basic_accuracy = len(correct) / len(x_N)\n",
    "    all_accuracy.append(basic_accuracy)\n",
    "    print(\"Accuracy:\", basic_accuracy)\n",
    "    \n",
    "    \n",
    "print(\"\\nOverall accuracy: %0.03f\" % (np.average(all_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
