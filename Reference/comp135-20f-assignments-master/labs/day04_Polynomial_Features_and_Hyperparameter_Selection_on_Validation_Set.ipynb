{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 135 day 04: Linear Regression with Polynomial Features and Hyperparameter Selection on a fixed validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "* Learn how to compute polynomial features\n",
    "* Try out selecting the polynomial degree on a fixed validation set\n",
    "\n",
    "* Learn how to use sklearn's built-in Polynomial feature transformer\n",
    "* Learn how to use sklearn pipelines to compose useful elementary transformations and predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "* [Part 1: Linear Regression on Sine Wave dataset](#part1)\n",
    "* [Part 2: Trying Polynomial Features + Linear Regression by hand](#part2)\n",
    "* [Part 3: Selecting degree on Fixed Validation Set](#part3)\n",
    "* [Part 4: Polynomial Feature transformer](#part4)\n",
    "* [Part 5: Pipelines](#part5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "* Hyperparameter selection is important to avoid overfitting (and underfitting)\n",
    "* We cannot use the training set alone to select hyperparameters\n",
    "* We should use a separate validation set to compute the error we use to select hyperparameters (or do cross-validation)\n",
    "* Sklearn has many built-in features (Pipelines, feature transformers, etc) that make your life easier if you know how to use them well\n",
    "* * They all follow a standard interface (aka \"API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import the pandas (data management library)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a simple 'sine-wave' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "prng = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_prediction_function(x):\n",
    "    return np.sin(2.1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 9        # num training examples (to estimate weight parameters)\n",
    "\n",
    "x_N = np.linspace(-3, 3, N) + 0.09 * prng.randn(N)\n",
    "y_N = true_prediction_function(x_N) + 0.06 * prng.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 15      # num validation examples (to select hyperparameters)\n",
    "\n",
    "x_va_L = np.linspace(-3.1, 3.1, L)\n",
    "y_va_L = true_prediction_function(x_va_L) + 0.06 * prng.randn(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 500      # num testing examples (to show \"true\" generalization)\n",
    "\n",
    "x_te_M = np.linspace(-3.5, 3.5, M)\n",
    "y_te_M = true_prediction_function(x_te_M) + 0.06 * prng.randn(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_te_M, y_te_M, 'r.', label='test set')\n",
    "plt.plot(x_N, y_N, 'bs', markersize=10, label='training set')\n",
    "plt.plot(x_va_L, y_va_L, 'cd', markersize=10, label='validation set')\n",
    "\n",
    "plt.legend(loc='upper right');\n",
    "plt.ylim([-1.5, 1.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression\n",
    "\n",
    "First, let's try to fit plain old linear regression to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regr = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Can you fit a linear regression and see what its predictions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regr.fit(x_N, y_N) # TODO fix me. Read error message below. Why does this break?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b: Can you make predictions using the model above on the heldout test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_te_M = np.zeros(M) # TODO something like \"lin_regr.predict(x_te_M)\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_te_M, yhat_te_M, 'b.-', label='predictions from LR')\n",
    "plt.plot(x_N, y_N, 'bs', markersize=10, label='training set')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.0, 0.5))\n",
    "plt.ylim([-1.5, 1.5]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Polynomial Features\n",
    "\n",
    "Given our scalar feature $x_i \\in \\mathbb{R}$, we want to TRANSFORM it to a $F$-degree polynomial feature vector (of size $F+1$)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_0(x_i) &= [1]\n",
    "\\\\\n",
    "\\phi_1(x_i) &= [1 ~~ x_i]\n",
    "\\\\\n",
    "\\phi_2(x_i) &= [1 ~~ x_i ~~ x_i^2]\n",
    "\\\\\n",
    "\\phi_3(x_i) &= [1 ~~ x_i ~~ x_i^2 ~~ x_i^3] \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_F(x_i) &= [1 ~~ x_i ~~ x_i^2 ~~ x_i^3 ~~ \\ldots x_i^F]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And then use these transformed features to do linear regression.\n",
    "\n",
    "That is, we'll learn weights for each of the entries of the vector produced by $\\phi_F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a: FIX this function so it returns a polynomial feature expansion like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_features(x_N1, degree=0):\n",
    "    ''' Transform some 1-dim feature array into polynomial features\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    x_N1 : 2D array, shape (N, 1)\n",
    "        Input features of a 1d regression problem\n",
    "    degree : int\n",
    "        Indicates degree of polynomial\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    phi_NG : 2D array, shape (N, G)\n",
    "        where G = degree + 1\n",
    "    '''\n",
    "    assert x_N1.ndim == 2\n",
    "    N, _ = x_N1.shape\n",
    "    \n",
    "    if degree == 0:\n",
    "        phi_NG = np.ones((N, 1))\n",
    "    elif degree >= 1:\n",
    "        # stack together horizontally a column of all ones and the x array\n",
    "        phi_NG = np.hstack(\n",
    "            [np.ones((N, 1))]\n",
    "            + [x_N1.copy() for d in range(degree)]  # TODO WHAT TO DO HERE??\n",
    "            )\n",
    "    return phi_NG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array to try things out as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x_N1 = np.asarray([[0.], [1], [2], [3]])\n",
    "print(my_x_N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_poly_features(my_x_N1, degree = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_poly_features(my_x_N1, degree = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_poly_features(my_x_N1, degree = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_poly_features(my_x_N1, degree = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hyperparameter Selection for Linear Regression with Poly. Features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_N1 = np.reshape(x_N, (N, 1))\n",
    "x_va_L1 = np.reshape(x_va_L, (L, 1))\n",
    "x_te_M1 = np.reshape(x_te_M, (M, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3a: Edit code below to properly do the polynomial feature transformation (use your function above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_h, ax_grid = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(20,6))\n",
    "\n",
    "for ii, degree in enumerate([1, 2, 3, 4, 5, 6, 7, 8]):\n",
    "    cur_ax = ax_grid.reshape((8,))[ii]\n",
    "\n",
    "    # Please Carefully Think through why we set \"fit_intercept=False\" here:\n",
    "    # This is not because we don't want bias (i.e. set it to zero).\n",
    "    # The reason is the bias(intercept) to be fitted is included as degree 0 of the polynomial.\n",
    "    lin_regr = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "    \n",
    "    phi_NG = x_N1       # TODO fix to transform x with your function \"make_poly_features\" above!\n",
    "    phi_va_LG = x_va_L1 # TODO fix to transform validation features\n",
    "    phi_te_MG = x_te_M1 # TODO fix to transform test features\n",
    "\n",
    "    # Train the model\n",
    "    lin_regr.fit(phi_NG, y_N)\n",
    "\n",
    "    # Make predictions\n",
    "    yhat_N = lin_regr.predict(phi_NG)\n",
    "    yhat_va_L = lin_regr.predict(phi_va_LG)\n",
    "    yhat_te_M = lin_regr.predict(phi_te_MG)\n",
    "    \n",
    "    # Evaluate the ERROR on training set and test set\n",
    "    tr_err = sklearn.metrics.mean_squared_error(y_N, yhat_N)\n",
    "    va_err = sklearn.metrics.mean_squared_error(y_va_L, yhat_va_L)\n",
    "    te_err = sklearn.metrics.mean_squared_error(y_te_M, yhat_te_M)\n",
    "    \n",
    "    print(\"degree %d | train error %10.3f | valid error %10.3f | test error %10.3f\" % (\n",
    "        degree, tr_err, va_err, te_err))\n",
    "    \n",
    "    # Plot the predictions on current axis\n",
    "    cur_ax.plot(x_te_M1, yhat_te_M, 'b-');\n",
    "    cur_ax.plot(x_N, y_N, 'bs');\n",
    "    \n",
    "    cur_ax.set_title('degree %d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3b: Which degree offers the lowest error on the validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3c: Which degree offers the lowest error on the test dataset?\n",
    "\n",
    "WARNING: if you really used this to pick your degree you could not later use the test set to fairly report your model's ability to generalize. Once you use the test set to train or select, you can't reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Using sklearn's built in Polynomial feature transform\n",
    "\n",
    "You can read up here:\n",
    "\n",
    "<https://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features>\n",
    "\n",
    "Or dive in! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "my_x_N1 = np.arange(3).reshape(3, 1)\n",
    "print(\"Original Features my_x_N1\")\n",
    "print(my_x_N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "phi_NG = poly.fit_transform(my_x_N1) # call 'fit_transform' to perform the transformation\n",
    "\n",
    "print(\"Transformed Features phi_NG using 2-degree polynomial\")\n",
    "print(phi_NG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(5)\n",
    "phi_NG = poly.fit_transform(my_x_N1)\n",
    "\n",
    "print(\"Transformed Features phi_NG using 5-degree polynomial\")\n",
    "print(phi_NG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with more than one feature (e.g. each x vector is 2-dim.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x_N2 = np.asarray([[0, 3], [1, 4], [2, 5]])\n",
    "print(\"Original Features my_x_N2\")\n",
    "print(my_x_N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(1)\n",
    "phi_NG = poly.fit_transform(my_x_N2)\n",
    "\n",
    "print(\"Transformed Features phi_NG using 1-degree polynomial\")\n",
    "print(phi_NG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "phi_NG = poly.fit_transform(my_x_N2)\n",
    "\n",
    "print(\"Transformed Features phi_NG using 2-degree polynomial\")\n",
    "print(phi_NG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, we can ask the feature transformer how it came up with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.get_feature_names(['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try 3-rd degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "phi_NG = poly.fit_transform(my_x_N2)\n",
    "\n",
    "print(\"Transformed Features phi_NG using 3-degree polynomial\")\n",
    "print(phi_NG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.get_feature_names(['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Using sklearn's built in Pipeline system\n",
    "\n",
    "Often, we want to compose together some feature transformations and some predictor (like Linear Regression)\n",
    "\n",
    "Sklearn lets us do that easily with a Pipeline\n",
    "\n",
    "Read up here: \n",
    "\n",
    "<https://scikit-learn.org/stable/modules/compose.html#pipeline>\n",
    "\n",
    "The key advantages of pipelines here are \n",
    "\n",
    "**Convenience** \n",
    "\n",
    "A pipeline is one object that handles everything\n",
    "\n",
    "* every time we call fit, we first transform the raw features \"x\" into the transformed ones \"phi\", and then we do linear regression fitting using those transformed features\n",
    "* every time we call predict, we first transform the raw features \"x\" into the transformed ones \"phi\", and then use linear regression to predict\n",
    "\n",
    "Otherwise, we'd have to call `make_poly_features' so many times.\n",
    "\n",
    "**Correctness**\n",
    "\n",
    "Pipelines help us avoid having data leaks (e.g. have test data impact the transformation we learn). More about this later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a pipeline, we just provide a list of steps.\n",
    "\n",
    "Each STEP is just a tuple:\n",
    "* first entry is the string name of the step (can be anything you want)\n",
    "* second entry is the constructed sklearn estimator that will implement that step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.Pipeline([\n",
    "    (\"step1\", sklearn.preprocessing.PolynomialFeatures()),\n",
    "    (\"step2\", sklearn.linear_model.LinearRegression()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use a pipeline like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_grid = plt.subplots(nrows=3, ncols=4, sharex=True, sharey=True, figsize=(20,9))\n",
    "\n",
    "for ii, degree in enumerate([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]):\n",
    "\n",
    "    cur_ax = ax_grid.reshape((12,))[ii]\n",
    "    \n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        (\"step1\", sklearn.preprocessing.PolynomialFeatures(degree)), # create custom Poly featurizer\n",
    "        (\"step2\", sklearn.linear_model.LinearRegression()),\n",
    "        ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(x_N1, y_N)                # Can use raw x here, unlike above\n",
    "\n",
    "    # Make predictions\n",
    "    yhat_N = pipeline.predict(x_N1)        # Can use raw x here too!\n",
    "    yhat_va_L = pipeline.predict(x_va_L1)\n",
    "    yhat_te_M = pipeline.predict(x_te_M1)\n",
    "    \n",
    "    # Evaluate the ERROR on training set and test set\n",
    "    tr_err = sklearn.metrics.mean_squared_error(y_N, yhat_N)\n",
    "    va_err = sklearn.metrics.mean_squared_error(y_va_L, yhat_va_L)\n",
    "    te_err = sklearn.metrics.mean_squared_error(y_te_M, yhat_te_M)\n",
    "    \n",
    "    print(\"degree %3d | train error % 10.3f | valid error %10.3f | test error % 10.3f\" % (\n",
    "        degree, tr_err, va_err, te_err))\n",
    "    \n",
    "    # Plot the predictions\n",
    "    cur_ax.plot(x_te_M1, y_te_M, 'r.');\n",
    "    cur_ax.plot(x_te_M1, yhat_te_M, 'b-');\n",
    "    cur_ax.plot(x_N, y_N, 'bs');\n",
    "    cur_ax.set_title('degree %d' % degree);\n",
    "    cur_ax.set_ylim([-2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5a: Which degree offers would you select based on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 5b: What do you notice about the error INSIDE the bounds of observed training data versus OUTSIDE the bounds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
