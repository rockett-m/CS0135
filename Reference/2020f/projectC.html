<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Project C: Recommendation Systems via Matrix Factorization | Introduction to Machine Learning
</title>
  <link rel="canonical" href="https://www.cs.tufts.edu/comp/135/2020f/projectC.html">



  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <script src="https://www.cs.tufts.edu/comp/135/2020f/theme/js/icsFormatter.js"></script>

  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/style.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/custom.css">


<meta name="description" content="Status: RELEASED. Updates 2020-12-07 : Updated Bonus problem to include a new short answer question "5c" about responsible AI Due date Leaderboard closes on Fri. Dec. 18 at 11:59pm AoE (Anywhere on Earth). Report is due Mon. Dec. 21 at 11:59pm AoE (Anywhere on Earth). (Tue 12/22 at â€¦">
</head>

<body>
  <header class="header">
    <nav class="navbar navbar-expand-lg navbar-expand-md navbar-light bg-light">
    <div class="container">
    <div class="row display-flex">
        <div class="col-2 col-sm-2 d-md-none"><!-- hidden if md or lg -->
        <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#my_collapsing_navbar"
            aria-controls="my_collapsing_navbar"
            aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon mw-100"></span>
        </button>
        </div>

        <div class="d-none d-md-block col-md-2">
          <a href="https://www.cs.tufts.edu/comp/135/2020f/">
            <img class="img-fluid mw-100" src=https://www.cs.tufts.edu/comp/135/2020f/images/tufts_ml_logo.png alt="Introduction to Machine Learning">
          </a>
        </div>

        <div class="col-10 col-sm-10 col-md-10">
          <h1 class="text-left" style="word-break:'break-all'">
            <a href="https://www.cs.tufts.edu/comp/135/2020f/">Introduction to Machine Learning</a>
          </h1>

          <p class="text-muted text-left d-none d-md-block mw-100">
            Tufts CS COMP 135 Intro ML | Fall 2020
          </p>



          <div class="collapse navbar-collapse" id="my_collapsing_navbar">
                <ul class="navbar-nav">
                  <li class="nav-item text-left">
                    <a href="index.html">Syllabus</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="schedule.html">Schedule</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="assignments.html">Assignments</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="office_hours.html">Office Hours</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="resources.html">Resources</a>
                  </li>

                </ul>
          </div>


        </div>

    </div>
    </div>
    </nav>

  </header>

  <div class="main">
    <div class="container">
      <h1>Project C: Recommendation Systems via Matrix Factorization
</h1>
      <hr>
<article class="article">
  <div class="content">
        <p style="text-align:right">Last modified: 2020-12-20 16:04 </p>
    <p><strong>Status: RELEASED.</strong></p>
<p><strong>Updates</strong></p>
<ul>
<li>2020-12-07 : Updated Bonus problem to include a new short answer question "5c" about responsible AI</li>
</ul>
<p><strong>Due date</strong></p>
<ul>
<li>Leaderboard closes on Fri. Dec. 18 at 11:59pm AoE (Anywhere on Earth).</li>
<li>Report is due Mon. Dec. 21 at 11:59pm AoE (Anywhere on Earth).  (Tue 12/22 at 07:59am in Boston).</li>
</ul>
<p>No late days allowed due to the strict end of the semester.</p>
<p><strong>Jump to</strong>:</p>
<p>&nbsp; <a href="#background">Background</a> 
&nbsp; <a href="#code">Code</a> 
&nbsp; <a href="#datasets">Datasets</a> 
&nbsp; <a href="#problem1">Problem 1</a> 
&nbsp; <a href="#problem2">Problem 2</a>
&nbsp; <a href="#problem3">Problem 3</a>
&nbsp; <a href="#problem4">Problem 4</a>
&nbsp; <a href="#bonus-problem">Bonus Problem</a>
&nbsp; <a href="#rubric">Rubric for Evaluating PDF Report</a></p>
<p><strong>Turn-in links</strong>:</p>
<ul>
<li>
<p>PDF report: <a href="https://www.gradescope.com/courses/173055/assignments/880988">https://www.gradescope.com/courses/173055/assignments/880988</a></p>
</li>
<li>
<p>ZIP file containing <a href="#leaderboard-instructions">Problem 4 predictions</a>: <a href="https://www.gradescope.com/courses/173055/assignments/880999">https://www.gradescope.com/courses/173055/assignments/880999</a></p>
</li>
<li>
<p>Reflection Form: <a href="https://forms.gle/r4VFnbgrBa9d7ZDV6">https://forms.gle/r4VFnbgrBa9d7ZDV6</a></p>
</li>
</ul>
<p><strong>Starter Code and Dataset Links:</strong></p>
<p><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/projectC">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/projectC</a></p>
<h2>Overview</h2>
<p>This is a four week project with lots of open-ended programming. Get started right away!</p>
<h4>Team Formation</h4>
<p>In this project, you can work in teams of 2 people, or (if you prefer) individually. 
Individual teams still need to complete all the parts below. We want to incentivize you to work in pairs.</p>
<p>If you need help finding teammates, please post to our "Finding a Partner for Project C" post on Piazza.</p>
<p>By the start of the second week (by end of day Wed 12/02), you should have identified your partner.</p>
<h4>Work to Complete</h4>
<p>As a team, you will work on several problems that study different representations of recommendation systems.</p>
<ul>
<li>Problem 1 looks at a baseline factorization model with one scalar parameter.</li>
<li>Problem 2 looks at a baseline factorization model with one learned scalar per item.</li>
<li>Problem 3 looks at a factorization model with a learned vector embedding per item.</li>
</ul>
<p>Throughout Problems 1, 2, and 3, you will practice the <em>development cycle</em> of models trained with SGD:</p>
<ul>
<li>
<ul>
<li>Develop a coherent loss function for your application</li>
</ul>
</li>
<li>
<ul>
<li>Use automatic differentiation toolboxes to compute the gradient</li>
</ul>
</li>
<li>
<ul>
<li>Attempt multiple runs of SGD to train models until you get a satisfactory performing model</li>
</ul>
</li>
<li>
<ul>
<li>Select among several model complexity hyperparameters to get the best generalization performance</li>
</ul>
</li>
</ul>
<p>Then, for Problem 4, you have much more open-ended freedom. </p>
<ul>
<li>Build the best model you can, and try to get to the top of the leaderboard!</li>
</ul>
<h4><a id="#what-to-turn-in"> What to Turn In </a></h4>
<p>Each team will prepare <em>one</em> PDF report covering all problems below.</p>
<ul>
<li>Prepare a short PDF report (no more than ~7 pages).</li>
<li>This document will be manually graded according to our <a href="#rubric">rubric</a></li>
<li>Can use your favorite report writing tool (Word or G Docs or LaTeX or ....)</li>
<li>Should be <strong>human-readable</strong>. Do not include code. Do NOT just export a jupyter notebook to PDF.</li>
<li>Should have each subproblem <a href="https://www.youtube.com/watch?v=KMPoby5g_nE&amp;feature=youtu.be&amp;t=43">marked via the in-browser Gradescope annotation tool</a>)</li>
</ul>
<p>Each team will prepare one ZIP file of the leaderboard-test-set predictions as a leaderboard submission for Problem 4. This ZIP file will contain just one file:</p>
<ul>
<li>predicted_ratings_leaderboard.txt : a plain text file</li>
<li>
<ul>
<li>One line for each line in the released dataset <code>ratings_masked_leaderboard_set.csv</code></li>
</ul>
</li>
<li>
<ul>
<li>Can be loaded in with <code>np.loadtxt()</code> as a valid 1d array of floats with shape (10000,)</li>
</ul>
</li>
</ul>
<p>Each <em>individual</em> will turn in a <em>reflection form</em> (after completing the report).</p>
<ul>
<li>Link: <a href="https://forms.gle/r4VFnbgrBa9d7ZDV6">https://forms.gle/r4VFnbgrBa9d7ZDV6</a></li>
</ul>
<h2><a name="background">Background</a></h2>
<p>For this project, you are given a large dataset of the ratings that 943 users have given to 1682 movies. </p>
<p>We'd like to build a <em>recommendation system</em> to help guess which movies a user will like.</p>
<p><em>Input</em>: The input to our prediction system is a pair <span class="math">\(i,j\)</span>, which denotes a specific user id (denoted by index <span class="math">\(i\)</span>, one of 943 possibilities) and movie id (denoted by index <span class="math">\(j\)</span>, one of 1682 possibilities).</p>
<p><em>Output</em>: The output of our predictor (e.g. the quantity produced by calling <code>model.predict(i,j)</code>) will be a scalar rating <span class="math">\(\hat{y}_{ij} \in \mathbb{R}\)</span>. For each possible user-movie pair <span class="math">\(i,j\)</span>, we'd like <span class="math">\(\hat{y}_{ij}\)</span> to be as close as possible to the "real" 5-star rating <span class="math">\(y_{ij}\)</span> that the user <span class="math">\(i\)</span> gave the movie <span class="math">\(j\)</span>. We can represent any five star rating with an integer in the set <span class="math">\(\mathcal{Y} = \{1, 2, 3, 4, 5\}\)</span>: a rating of 1 is the worst possible, a rating of 5 is the best.</p>
<p>We can think of our entire set of observed ratings as a big 2D matrix <span class="math">\(Y\)</span>, which has 943 rows and 1682 columns.
Formally, we can write <span class="math">\(Y \in \mathcal{Y}^{943 \times 1682}\)</span>.</p>
<p>Of course, not all users have seen and rated all movies, so some true ratings are simply unknown to us. Thus, if we examined our true rating matrix <span class="math">\(Y\)</span>, <em>most entries would be missing</em>. It is impossible to know if our guesses for missing entries are any good. </p>
<p>We thus concentrate only the <strong>observed</strong> entries of <span class="math">\(Y\)</span>. We can represent our dataset more compactly as a list <span class="math">\(\mathcal{I}\)</span> of all distinct <em>observed</em> pairs of input (user_id <span class="math">\(i\)</span>, item_id <span class="math">\(j\)</span>) and output (rating <span class="math">\(y_{ij}\)</span>) in the observed dataset. </p>
<h3><a name='datasets'> Dataset </a></h3>
<p>We will use the MovieLens 100K dataset. This data set consists of:</p>
<ul>
<li>100,000 ratings (1-5 stars) from 943 users on 1682 movies. </li>
<li>Each user has rated at least 20 movies.</li>
<li>Some movies have ratings from only a few users.</li>
<li>Ratings were collected on a 5-star scale. Each rating is one of 5 possible integer values -- 1, 2, 3, 4, or 5 -- with 5 being 'best' and 1 being 'worst'.</li>
<li>Simple demographic info for the users (age, gender, etc.) are available</li>
</ul>
<p>For more information about the original dataset, see <a href="http://files.grouplens.org/datasets/movielens/ml-100k-README.txt">http://files.grouplens.org/datasets/movielens/ml-100k-README.txt</a>. We are grateful to the dataset creators and the University of Minnesota for making this data publicly available.</p>
<p>We've provided a clean preprocessed version of this dataset here:
<a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/projectC/data_movie_lens_100k">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/projectC/data_movie_lens_100k</a></p>
<p>We've provided one CSV file, <code>ratings_all_development_set.csv</code>, to give you all the data you need to <em>develop</em> and <em>evaluate</em> models. (Another CSV file for the leaderboard will be discussed later in Problem 4).</p>
<p>Each row of this file specifies 3 things:</p>
<ul>
<li>user_id <span class="math">\(i\)</span> : an integer in <span class="math">\(\{0, 1, 2, ... 942\}\)</span></li>
<li>item_id <span class="math">\(j\)</span> : an integer in <span class="math">\(\{0, 1, 2, ... 1681\}\)</span></li>
<li>observed 5-star rating <span class="math">\(y_{ij}\)</span> : an integer in {1, 2, 3, 4, 5}$</li>
</ul>
<p>The first few rows of <code>ratings_all_development_set.csv</code> are:</p>
<div class="highlight"><pre><span></span><span class="n">user_id</span><span class="p">,</span><span class="n">item_id</span><span class="p">,</span><span class="n">rating</span>
<span class="mi">772</span><span class="p">,</span><span class="mi">36</span><span class="p">,</span><span class="mi">3</span>
<span class="mi">471</span><span class="p">,</span><span class="mi">228</span><span class="p">,</span><span class="mi">5</span>
<span class="mi">641</span><span class="p">,</span><span class="mi">401</span><span class="p">,</span><span class="mi">4</span>
<span class="mi">312</span><span class="p">,</span><span class="mi">98</span><span class="p">,</span><span class="mi">4</span>
<span class="p">...</span>
</pre></div>


<p><em>Train/validation/test splits</em>: 
As usual, we can take the large development dataset <span class="math">\(\mathcal{I}\)</span> (which is provided to you in random order) and divide it into training, validation, and test sets: <span class="math">\(\mathcal{I}^{\text{train}}\)</span>, <span class="math">\(\mathcal{I}^{\text{valid}}\)</span>, and 
<span class="math">\(\mathcal{I}^{\text{test}}\)</span>. See the starter code in <code>train_valid_test_loader.py</code>, which you can use throughout Problems 1-5.</p>
<h3>Background: Evaluation Performance Metric</h3>
<p>Your goal is to predict the ratings of all user-movie pairs well.</p>
<p>To measure ultimately rank models by prediction quality, we'll use <em>mean absolute error</em>. This is useful for our five-star rating task, since it is nicely interpretable (e.g. an MAE of 0.5 means we're within a 1/2-star of the answer on average). Plus, it is not overly sensitive to outliers. </p>
<p>For a given dataset <span class="math">\(\mathcal{I}\)</span> of triples <span class="math">\(i, j, y_{ij}\)</span>, recall that MAE is computed as:
</p>
<div class="math">$$
\text{MAE}(y, \hat{y}) =
    \frac{1}{|\mathcal{I}|}
    \sum_{i,j \in \mathcal{I}} | y_{ij} - \hat{y}_{ij} |
$$</div>
<h3>Background: SGD, Automatic Differentiation and <code>autograd</code></h3>
<p>Across Problem 1, Problem 2, and Problem 3, you'll develop your own Python code to build a series of increasingly more powerful models to perform recommendation. For each model, we will view training as an optimization problem, and we'll solve it with <em>stochastic gradient descent</em>. </p>
<p>Recall that we spent HW3 analyzing an existing <code>sklearn</code> implementation of SGD for MLP classifiers. You'll now get some experience writing your <em>own</em> loss function and using SGD to optimize that loss.</p>
<p>How do we use SGD for our problem? At each update step, you'll grab a <em>minibatch</em> of data (a random subset of the observed entries in our ratings matrix <span class="math">\(Y\)</span>), and compute gradient estimates for parameters with respect to this batch.</p>
<p>How will we compute gradients? We've provided some starter code to help you. To save lots of effort and help us explore many possible models, we'll use the <a href="https://github.com/HIPS/autograd">autograd</a> Python package to perform <em>automatic differentiation</em>.</p>
<p>For a solid introduction to using <code>autograd</code>, see the <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/labs/day23_AutogradForGradientDescent.ipynb">Autograd For Gradient Descent lab from Monday 11/30</a> </p>
<h4>Starter Code organization</h4>
<p>For each model of interest (Problem 1, Problem 2, Problem 3, all defined mathematically below), we have created a separate file defining the python <code>class</code> for that model. For examples in your starter code, see:</p>
<ul>
<li><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/projectC/CollabFilterMeanOnly.py">CollabFilterMeanOnly.py</a> for Problem 1</li>
<li><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/projectC/CollabFilterOneScalarPerItem.py">CollabFilterOneScalarPerItem.py</a> for Problem 2.</li>
<li><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/projectC/CollabFilterOneScalarPerVector.py">CollabFilterOneScalarPerVector.py</a> for Problem 3.</li>
</ul>
<p>Your task is to define several methods for each model class, which allow us to perform prediction given fixed parameters, and compute the loss used for gradient-based training of parameters.</p>
<p>All these models will be subclasses of a "base" class <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/projectC/AbstractBaseCollabFilterSGD.py">AbstractBaseCollabFilterSGD</a>, which contains logic to construct and fit collaborative filtering models to data via stochastic gradient descent. Using this base class, you don't need to write SGD yourself, or even how to compute the gradient! Please do read through this base class carefully to understand how it works.</p>
<p>Below, we review the methods/attributes you'll need to write yourself, and the methods/attributes you only need to use that are provided in the base class. </p>
<h4>Methods you'll need to write</h4>
<p>To complete the implementation of a given model, we'll follow the following pattern:</p>
<ul>
<li>Use the instance attribute <code>param_dict</code> to store all model parameters</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">param_dict</span> <span class="o">:</span> <span class="n">dict</span>
    <span class="n">Keys</span> <span class="n">are</span> <span class="n">string</span> <span class="n">names</span> <span class="n">of</span> <span class="n">parameters</span>
    <span class="n">Values</span> <span class="n">are</span> <span class="o">*</span><span class="n">autograd</span><span class="o">.</span><span class="na">numpy</span> <span class="n">arrays</span><span class="o">*</span> <span class="n">of</span> <span class="n">parameter</span> <span class="n">values</span>    
</pre></div>


<ul>
<li>Define method <code>predict</code> to make predictions:</li>
<li>
<ul>
<li>Input: Specific user-movie example pairs, indicated by integer ids</li>
</ul>
</li>
<li>
<ul>
<li>Output: Predicted ratings for each example</li>
</ul>
</li>
<li>
<p>Define method <code>calc_loss_wrt_parameter_dict</code> to compute the loss to minimize with SGD:</p>
</li>
<li>
<ul>
<li>Input: a minibatch of training data, a parameter dict</li>
</ul>
</li>
<li>
<ul>
<li>Output: scalar float, indicating the loss on the batch given the parameters</li>
</ul>
</li>
<li>
<p>Define method <code>init_parameter_dict</code> to initialize the param_dict attribute to random values:</p>
</li>
<li>
<ul>
<li>Input: Number of possible users, Number of possible items</li>
</ul>
</li>
<li>
<ul>
<li>Output: None, internal attribute <code>param_dict</code> updated</li>
</ul>
</li>
</ul>
<p>The steps above are <em>all</em> you need to do for each possible model. Each class <em>inherits</em> a complete <code>fit</code> method from the predefined <code>AbstractBaseCollabFilterSGD</code>, which knows how to perform SGD given the pieces above.</p>
<h4>Methods you'll need to understand and use (but not edit)</h4>
<p>You should read through the provided <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/projectC/AbstractBaseCollabFilterSGD.py">AbstractBaseCollabFilterSGD.py</a>, to be sure you understand what's going on.</p>
<ul>
<li><code>__init__</code> : Constructor</li>
</ul>
<p>This is where the user defines the <code>batch_size</code>, the <code>step_size</code> (learning rate), and the number of epochs <code>n_epochs</code> to complete during training. This constructor can also define the regularization strength <code>alpha</code> and the number of hidden factors <code>n_factors</code> for Problem 3.</p>
<ul>
<li><code>fit</code> : Method to fit model parameters to provided data</li>
</ul>
<p>At the bottom of each starter code file, you can see example code for calling <code>fit</code> for that model. </p>
<p>When the user calls <code>fit</code>, the model parameters are initialized to random values, and then updated iteratively via SGD to improve the loss. These updates proceed until the desired number of epochs are performed.</p>
<p><strong>Epoch - A Definition</strong>: An epoch is a unit of training progress in minibatch learning. One epoch is complete when our stochastic gradient descent has processed enough minibatches such that the total number of examples seen is "equivalent" to the size of the entire training dataset. </p>
<p>Within our <code>fit</code> implementation, we compute and store performance metrics at various checkpoints throughout the training process, including:</p>
<ul>
<li>at the initial parameters (before any updates)</li>
<li>every 1/4 of an epoch for epochs 0 - 2</li>
<li>every 1/2 of an epoch for epochs 2 - 8</li>
<li>every 1 of an epoch for epochs 8 - 32</li>
<li>every 2 epochs for 32 - 128</li>
<li>every 4 epochs after that</li>
</ul>
<p>These metrics helps us monitor progress as learning progresses, which is especially rapid in early epochs as the poor random initialization is improved.</p>
<h4>Attributes available after calling <code>fit</code> that trace performance</h4>
<p>We have recorded and stored useful diagnostic metrics computed at various checkpoints throughout the training procedure. These help us "trace" what happens throughout learning, so we call them trace performance metrics.</p>
<p>After fitting a model, you'll have the following trace attributes available to you:</p>
<div class="highlight"><pre><span></span><span class="n">trace_epoch</span> <span class="o">:</span> <span class="mi">1</span><span class="n">D</span> <span class="n">array</span><span class="o">-</span><span class="n">like</span>
    <span class="n">Contains</span> <span class="n">the</span> <span class="n">epochs</span> <span class="o">(</span><span class="n">fractional</span><span class="o">)</span> <span class="n">where</span> <span class="n">model</span> <span class="n">performance</span> <span class="n">was</span> <span class="n">assessed</span><span class="o">.</span>
    <span class="n">Value</span> <span class="n">of</span> <span class="mf">0.0</span> <span class="n">indicates</span> <span class="n">the</span> <span class="n">initial</span> <span class="n">model</span> <span class="n">parameters</span> <span class="o">(</span><span class="n">before</span> <span class="n">any</span> <span class="n">gradient</span> <span class="n">updates</span><span class="o">).</span>
    <span class="n">Value</span> <span class="n">of</span> <span class="mf">0.1</span> <span class="n">indicates</span> <span class="n">that</span> <span class="n">the</span> <span class="n">total</span> <span class="n">training</span> <span class="n">examples</span> <span class="n">seen</span> <span class="n">represents</span> <span class="mi">10</span><span class="o">%</span> <span class="n">of</span> <span class="n">the</span> <span class="n">size</span> <span class="n">of</span> <span class="n">the</span> <span class="n">training</span> <span class="kd">set</span><span class="o">.</span>

<span class="n">trace_loss</span> <span class="o">:</span> <span class="mi">1</span><span class="n">D</span> <span class="n">array</span><span class="o">-</span><span class="n">like</span>
    <span class="n">Contains</span> <span class="n">training</span> <span class="n">loss</span> <span class="o">(</span><span class="n">at</span> <span class="n">current</span> <span class="n">batch</span> <span class="n">only</span><span class="o">)</span> <span class="n">whenever</span> <span class="n">model</span> <span class="n">was</span> <span class="n">assessed</span><span class="o">.</span>
    <span class="n">This</span> <span class="k">is</span> <span class="n">reported</span> <span class="k">as</span> <span class="n">an</span> <span class="n">average</span> <span class="n">per</span> <span class="n">example</span> <span class="k">in</span> <span class="n">the</span> <span class="n">current</span> <span class="n">batch</span><span class="o">.</span>

<span class="n">trace_mae_train</span> <span class="o">:</span> <span class="mi">1</span><span class="n">D</span> <span class="n">array</span><span class="o">-</span><span class="n">like</span>
    <span class="n">MAE</span> <span class="n">assessed</span> <span class="n">on</span> <span class="n">entire</span> <span class="n">training</span> <span class="kd">set</span> <span class="n">whenever</span> <span class="n">model</span> <span class="n">assessed</span><span class="o">.</span>

<span class="n">trace_mae_valid</span> <span class="o">:</span> <span class="mi">1</span><span class="n">D</span> <span class="n">array</span><span class="o">-</span><span class="n">like</span>
    <span class="n">MAE</span> <span class="n">assessed</span> <span class="n">on</span> <span class="n">entire</span> <span class="n">validation</span> <span class="kd">set</span> <span class="n">whenever</span> <span class="n">model</span> <span class="n">assessed</span><span class="o">.</span>
</pre></div>


<p>So for example, to plot training MAE vs. epochs completed, you could do:</p>
<div class="highlight"><pre><span></span><span class="o">#</span> <span class="k">After</span> <span class="n">calling</span> <span class="n">fit</span><span class="p">...</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">model</span><span class="p">.</span><span class="n">trace_epoch</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trace_mae_train</span><span class="p">,</span> <span class="s1">&#39;.-&#39;</span><span class="p">)</span>
</pre></div>


<h3><a name='code'>Starter Code and Code Restrictions</a></h3>
<p>For this assignment, you are limited to the following Python packages for performing machine learning related functionality: </p>
<ul>
<li>All Problems: Default packages in your environment: numpy, scipy, sklearn, etc.</li>
<li>All Problems: <code>autograd</code> package for automatic differentiation</li>
<li>Problem 4 onward: <code>surprise</code> package for recommendation, or any other packages you want</li>
</ul>
<p>You can INSTALL the <code>surprise</code> package as follows:</p>
<div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">comp135_2020f_env</span>
<span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="k">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scikit</span><span class="o">-</span><span class="n">surprise</span>
</pre></div>


<p>Your staff found in late November 2020 that scikit-surprise version <code>1.1.1</code> worked fine with the existing comp135 environment.</p>
<p>For any package you use, please consult the documentation websites or other external web resources. However, you should <em>understand</em> every line of the code you use and not simply copy-paste without thinking carefully.</p>
<p>Remember to keep the course <a href="index.html#collaboration-policy">collaboration policy</a> in mind: do your own work! (with/without a partner as required).</p>
<h2><a name="problem1"> Problem 1: Simple Baseline Model with SGD and Autograd</a></h2>
<p>To get used to developing models using our autograd framework, we'll consider the simplest possible baseline model "M1": a model that makes the <em>same</em> scalar prediction for a movie's rating no matter what user or movie is considered. This model has one scalar parameter <span class="math">\(\mu \in \mathbb{R}\)</span>, and the prediction for user <span class="math">\(i\)</span> and movie <span class="math">\(j\)</span> is simply:</p>
<div class="math">$$
\hat{y}_{ij} = \mu
$$</div>
<h3>Training Model M1</h3>
<p>To train model M1 for <span class="math">\(N\)</span> users and <span class="math">\(M\)</span> movies, we wish to optimize this <em>squared error</em> training objective:</p>
<div class="math">$$
\min_{\mu \in \mathbb{R}}  \sum_{i,j \in \mathcal{I}^{\text{train}}} (y_{ij} - \mu)^2
$$</div>
<p>In words, this means we want to minimize the squared error on the training set, between the predicted rating (simply <span class="math">\(\mu\)</span> here) and the observed rating <span class="math">\(y_{ij}\)</span>. </p>
<h3>Problem 1 Code Implementation Tasks</h3>
<p>Edit the starter code file: <code>CollabFilterMeanOnly.py</code>. Complete each required method (<code>init_parameter_dict</code>, <code>predict</code>, and <code>calc_loss_wrt_parameter_dict</code>), as described above in the background section above. </p>
<h3>Problem 1 Analysis Tasks</h3>
<p>Using SGD, train M1 with two different settings of batch size: 10000 examples and 100 examples.</p>
<p>For the best run of these, please record that M1 model's:</p>
<ul>
<li>value of parameter <span class="math">\(\mu\)</span></li>
<li>mean absolute error on the validation set</li>
<li>mean absolute error on the test set</li>
</ul>
<h3>Problem 1 Report Tasks</h3>
<p>As evidence of successfully completing Problem 1, include the following in your report:</p>
<p><strong>1a:</strong> Figure and caption: Trace plots showing <em>mean absolute error</em> vs. epoch completed for your SGD training runs.</p>
<p>In two side-by-side plots, you should compare the two batch sizes:</p>
<ul>
<li>Left plot: 10000 examples per batch</li>
<li>Right plot: 100 examples per batch</li>
<li>Each plot should have <em>two</em> lines (one for training MAE, one for validation MAE)</li>
</ul>
<p>Please <em>adjust the y axis</em> to focus on what happens after epoch 2. Avoid showing a plot where you cannot see some difference between the two curves.</p>
<p><strong>1b:</strong> Short answer: There is a closed-form operation we could apply to the training set to compute the optimal <span class="math">\(\mu\)</span> value (e.g. a one line computation in numpy involving the observed training ratings <span class="math">\(Y\)</span>). How would you compute this "exact" solution? Report the computed optimal <span class="math">\(\mu\)</span> value. Does this result agree with your SGD solution?</p>
<h2><a name="problem2"> Problem 2: One-Scalar-Per-Item Baseline with SGD and Autograd</a></h2>
<p>We now consider a second model "M2" with three parameters:</p>
<ul>
<li><span class="math">\(\mu\)</span> : scalar mean rating</li>
<li><span class="math">\(b_i\)</span> : scalar bias term for each user <span class="math">\(i\)</span></li>
<li><span class="math">\(c_j\)</span> : scalar bias term for each movie <span class="math">\(j\)</span></li>
</ul>
<p>Prediction under model "M2" becomes:</p>
<div class="math">$$
\hat{y}_{ij} = \mu + b_i + c_j
$$</div>
<h3>Training Model M2</h3>
<p>To train model M2 for <span class="math">\(N\)</span> users and <span class="math">\(M\)</span> movies, we wish to optimize the following <strong>squared error</strong> objective:</p>
<div class="math">$$
\min_{\mu \in \mathbb{R}, b \in \mathbb{R}^N, c \in \mathbb{R}^M}  \sum_{i,j \in \mathcal{I}^{\text{train}}} (y_{ij} - \mu - b_i - c_j)^2
$$</div>
<p>In words, this means we want to minimize the squared error on the training set, between the predicted rating and the observed rating <span class="math">\(y_{ij}\)</span>. </p>
<h3>Problem 2 Code Implementation Tasks</h3>
<p>Edit the starter code file: <code>CollabFilterOneScalarPerItem.py</code>. Complete each required method, as described above in the background section. </p>
<h3>Problem 2 Analysis Tasks</h3>
<p>Using SGD, train M2 with two different settings of batch size: 10000 examples and 100 examples.</p>
<p>You may need to adjust the SGD <code>step_size</code> or <code>n_epochs</code> hyperparameter here. You should aim to show runs that last until either training error visibly <em>converges</em> or until you see obvious overfitting.</p>
<p>For the best run of these, please record that M2 model's:</p>
<ul>
<li>mean absolute error on the validation set</li>
<li>mean absolute error on the test set</li>
<li>value of parameters <span class="math">\(\mu, b, c\)</span> (not needed for your report, but useful so you don't need to retrain the model if you have to remake a figure)</li>
</ul>
<h3>Problem 2: Report Tasks</h3>
<p>As evidence of successfully completing Problem 2, include the following in your report:</p>
<p><strong>2a:</strong> Figure and caption: Trace plots showing <em>mean absolute error</em> vs. epoch completed for your SGD training runs.</p>
<p>In two side-by-side plots, you should compare the two batch sizes:</p>
<ul>
<li>Left plot: 10000 examples per batch</li>
<li>Right plot: 100 examples per batch</li>
<li>Each plot should have <em>two</em> lines (one for training MAE, one for validation MAE)</li>
</ul>
<p>Please <em>adjust the y axis</em> to focus on what happens after epoch 2. Avoid showing a plot where you cannot see some difference between the two curves.</p>
<p><strong>2b:</strong> Figure and caption: For the model from 2a with best validation MAE, display the learned per-movie rating adjustment parameters <span class="math">\(c_j\)</span> for each of the movies in the short list in <code>select_movies.csv</code>. You might make a sorted list, showing each movie's title alongside its learned bias parameter.</p>
<p>In your caption, please answer: What kinds of movies have a large positive <span class="math">\(c_j\)</span> or large negative <span class="math">\(c_j\)</span> value? What does it mean for a <span class="math">\(c_j\)</span> to be large and negative or large and positive?</p>
<h2><a name="problem3"> Problem 3:  One-Vector-Per-Item Collaborative Filtering with SGD and Autograd</a></h2>
<p>We now consider a full matrix factorization model "M3" with five parameters:</p>
<ul>
<li><span class="math">\(\mu\)</span> : scalar mean rating</li>
<li><span class="math">\(b_i\)</span> : scalar bias term for each user <span class="math">\(i\)</span></li>
<li><span class="math">\(c_j\)</span> : scalar bias term for each movie <span class="math">\(j\)</span></li>
<li><span class="math">\(u_i\)</span> : K-dimensional vector for each user <span class="math">\(i\)</span></li>
<li><span class="math">\(v_j\)</span> : K-dimensional vector for each movie <span class="math">\(j\)</span></li>
</ul>
<p>Crucially, you'll now need to think about how to set <span class="math">\(K\)</span>, the number of "factors" or "dimensions" to learn when representing each user/movie in a vector space. Within the starter code, you set this with the <code>n_factors</code> keyword argument to the constructor. </p>
<p>Prediction under model "M3" for the rating that user <span class="math">\(i\)</span> will give to movie <span class="math">\(j\)</span> is:</p>
<div class="math">$$
\hat{y}_{ij} = \mu + b_i + c_j + \sum_{k=1}^K u_{ik} v_{jk}
$$</div>
<p>M3 is known as a collaborative filtering model for recommendation in the ML research literature.</p>
<h3>Training:</h3>
<p>To train model M3 for <span class="math">\(N\)</span> users and <span class="math">\(M\)</span> movies, we wish to optimize the following objective:</p>
<div class="math">$$
\min_{\mu, b, c, \{ u_i \}_{i=1}^N, \{ v_j \}_{j=1}^M}
\quad
\alpha \Big(
    \sum_{j} \sum_{k} v_{jk}^2 
    + \sum_{i} \sum_{k} u_{ik}^2 
    \Big)
+ \sum_{i,j \in \mathcal{I}^{\text{train}}}
    (y_{ij} - \mu - b_i - c_j - u_i^T v_j)^2
$$</div>
<p>Again, this is a <em>squared error</em> objective. Note that we have added L2 regularization penalties on the <span class="math">\(u\)</span> and <span class="math">\(v\)</span> vectors with strength <span class="math">\(\alpha \geq 0\)</span>. We'll investigate whether this helps us generalize to heldout data better.</p>
<p>In the starter code, the L2 penalty strength hyperparameter can be set via the keyword argument <code>alpha=...</code> in the constructor, and accessed via the attribute <code>alpha</code>.</p>
<h3>Problem 3 Code Implementation Tasks</h3>
<p>Edit the starter code file: <code>CollabFilterOneVectorPerItem.py</code>. Complete each required method, as described above in the background section. </p>
<h3>Problem 3 Analysis Tasks</h3>
<p>You should fix <code>batch_size=1000</code> throughout this problem.</p>
<p><strong>Analysis 3(i):</strong> First, with <em>no</em> regularization (<span class="math">\(\alpha=0\)</span>), train M3 using SGD. Try three possible values of <span class="math">\(K\)</span>: 2, 10, and 50.</p>
<p><strong>Analysis 3(ii):</strong> Second, train M3 with <span class="math">\(K=50\)</span> now <em>with moderate regularization</em> by setting strength <span class="math">\(\alpha &gt; 0\)</span> to try to eliminate overfitting you saw in <strong>3(i)</strong>. Focus <em>only</em> on <span class="math">\(K=50\)</span>.</p>
<p>For the best run at each <span class="math">\(K\)</span> (with/without <span class="math">\(\alpha &gt; 0\)</span>), please record that M3 model's:</p>
<ul>
<li>mean absolute error on the validation set</li>
<li>mean absolute error on the test set</li>
<li>value of parameters <span class="math">\(\mu, b, c, u, v\)</span> (not needed for your report, but useful so you don't need to retrain the model if you have to remake a figure)</li>
</ul>
<p>You may need to adjust the SGD <code>step_size</code> or <code>n_epochs</code> hyperparameter here. You should aim to show runs that last until either training error visibly <em>converges</em> or until you see obvious overfitting.</p>
<p><em>Hint:</em> You might consider using <em>early stopping</em>, to get the best possible model here.</p>
<h3>Problem 3 Report Tasks</h3>
<p>As evidence of successfully completing Problem 3, include the following in your report:</p>
<p><strong>3a:</strong> Figure and caption: Make trace plot showing <em>mean absolute error</em> vs. epoch completed when <span class="math">\(\alpha=0\)</span>.</p>
<p>You should include 3 figures side-by-side, one for <span class="math">\(K=2\)</span> factors, one for <span class="math">\(K=10\)</span>, and one for <span class="math">\(K=50\)</span>.</p>
<p>In your caption, reflect on what your trace plots suggest. Do you see underfitting? Overfitting? What happens as <span class="math">\(K\)</span> increases?</p>
<p><strong>3b:</strong> Figure and caption: Make trace plot showing <em>mean absolute error</em> vs. epoch completed when <span class="math">\(\alpha &gt; 0\)</span>.</p>
<p>Here, please plot one figure, for <span class="math">\(K=50\)</span>.</p>
<p>In your caption, please specify (1) which value of <span class="math">\(\alpha\)</span> you ultimately selected (you should try several) as well as the batchsize and step size, and (2) whether you ultimately get better heldout error with <span class="math">\(\alpha &gt; 0\)</span> than you did in 3a.</p>
<p><strong>3c:</strong> Table and caption: Report the MAE on validation set and test set for the "best version" of each of these models:</p>
<ul>
<li>M1</li>
<li>M2</li>
<li>M3 with <span class="math">\(K=2\)</span></li>
<li>M3 with <span class="math">\(K=10\)</span></li>
<li>M3 with <span class="math">\(K=50\)</span></li>
</ul>
<p>Below the figure, please discuss:</p>
<ul>
<li>How you determined the "best" version of each model</li>
<li>How many factors <span class="math">\(K\)</span> do you recommend for M3? Should we try even more than 50 factors?</li>
<li>Which of the 5 models is "best" overall and why</li>
</ul>
<p><strong>3d:</strong> Figure and caption: For the best M3 model with <span class="math">\(K=2\)</span> factors, consider the learned per-movie vectors <span class="math">\(v_j\)</span> for the short list of movies listed in <code>select_movies.csv</code>. Can you make a scatter plot of the 2-dimensional "embedding" vector <span class="math">\(v_j\)</span> of these movies (labeling each point with its movie title), like we saw in lecture? Do you notice any interpretable trends? </p>
<p><em>Hint</em>: See the very bottom of the <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/labs/day24_RecommenderSystems.ipynb">day 24 lab notebook</a> with help plotting a 2D visual of the embedding vectors.
Search for the "Make visualization of the learned embeddings of select movies" section.</p>
<h2><a name="problem4"> Problem 4: Open-Ended Recommendation Challenge </a></h2>
<p>The starter code includes an additional <em>leaderboard</em> heldout dataset -- <code>ratings_masked_leaderboard_set.csv</code> -- which you haven't used yet. This contains an additional 10,000 entries of user-movie pairs for the same set of users and movies as above. We've omitted the ratings here, so you won't be able to access them (they are "masked").</p>
<p>In this problem, your goal is to obtain the best possible prediction results on this heldout test data, in terms of mean absolute error.</p>
<p>You can try <em>any model</em> you want. You can make use of the other ratings you've already observed in the training set, as well as the user-specific info found in <code>user_info.csv</code> and the movie-specific attributes found in <code>item_info.csv</code>. You can use <code>autograd</code>, <code>sklearn</code>, <code>surprise</code>, or any other package. Your goal is to get the best score on our leaderboard.</p>
<h4>Example Problem 4 ideas</h4>
<ul>
<li>
<p>Try one of the implementations of <a href="https://surprise.readthedocs.io/en/stable/matrix_factorization.html">SVD in the <code>surprise</code> package</a>. This package offers models that look like our M3, plus some neat extensions of these. Can you train one of these to perform better? Please take advantage of surprise's extensive sklearn-like grid search tools.</p>
</li>
<li>
<p>Can you somehow use user-specific features (like gender and age, available in <code>user_info.csv</code>) or movie-specific features (like title and year, available in <code>movie_info.csv</code>) to improve your rating predictions?</p>
</li>
<li>
<p>Try one of the <a href="https://surprise.readthedocs.io/en/stable/knn_inspired.html">k-nearest neighbor approaches to recommendation</a> in <code>surprise</code></p>
</li>
<li>
<p>Try some other approach to recommendation you have read about.</p>
</li>
<li>
<p>Try a new loss function! Throughout this project, we have used mean-squared error in the <code>calc_loss...</code> method for training the models, but then evaluated with mean-absolute-error. What if we just used mean absolute error in the loss? This should be somewhat easy with <code>autograd</code>.</p>
</li>
</ul>
<h4><a name='leaderboard-instructions'> Problem 4: Leaderboard Submission Tasks </a></h4>
<p>You should submit your final predictions as a plain text file named <code>predicted_ratings_leaderboard.txt</code></p>
<ul>
<li>predicted_ratings_leaderboard.txt : a plain text file</li>
<li>
<ul>
<li>One line for each line in the released dataset <code>ratings_masked_leaderboard_set.csv</code></li>
</ul>
</li>
<li>
<ul>
<li>Can be loaded in with <code>np.loadtxt()</code> as a valid 1d array of floats with shape (10000,)</li>
</ul>
</li>
</ul>
<h4>Problem 4: Report Tasks</h4>
<p>Please include in your report</p>
<p><strong>4a</strong>: 1-2 paragraphs describing your proposed method (how it works, why you chose it, what training and hyperparameter selection is done, etc.). Show that you have mastered the core concepts and best practices of machine learning.</p>
<p><strong>4b</strong>: 1 figure (with caption) relevant to reporting how you trained the model <em>or</em> selected model complexity. This could be a trace plot or a hyperparameter selection plot.</p>
<p><strong>4c</strong>: 1 tables reporting your model's ultimate <em>mean absolute error</em> performance. Be sure to include the measured leaderboard performance (in terms of mean absolute error) as well as some internal performance number (e.g. on the test split of the development set).</p>
<p><strong>4d</strong>: 1 paragraph analyzing the <em>table of results from 4d</em></p>
<ul>
<li>Discuss how your leaderboard number compared to your internal validation efforts </li>
<li>Discuss how this method compared to your best model of M1, M2, and M3</li>
</ul>
<p><strong>4e</strong>: 1 paragraph discussing limitations and opportunities for future work</p>
<h2><a name="bonus-problem"> BONUS Problem 5: Predicting Gender from Learned Per-User Embedding Vectors </a></h2>
<p>Consider the best K-factor model you've trained (either with surprise or with your own implementation of M3 in Problem 3). Let <span class="math">\(U\)</span> be the learned matrix of user vectors, with its i-th row as the vector <span class="math">\(u_i\)</span> for user <span class="math">\(i\)</span>. Each row of <span class="math">\(U\)</span> can be seen as a "feature vector" for a particular user.</p>
<p>The question we'd like to investigate is this: do our learned per-user features that are optimized for predicting movie ratings contain anything to do with gender?</p>
<p>The provided data file <code>user_info.csv</code> contains an <code>is_male</code> column indicating which users in the dataset are male. Can you predict this signal given the features <span class="math">\(U\)</span>?</p>
<h4>Implementation Tasks</h4>
<p>Train a binary classifier to predict the <code>is_male</code> target variable given the fixed per-user embedding features <span class="math">\(U\)</span>. Feel free to use any sklearn binary classifier (logistic regression, SVM, random forest, etc.). You should use best practices for model selection (cross validation, hyperparameter search, etc).</p>
<p>You'll need to set up this classification task from scratch. You have info for all 943 users. Should you include them all in the training process? How will you fairly report the accuracy on heldout data? </p>
<h4>Bonus Problem Report Tasks</h4>
<p><strong>5a</strong>: Paragraph: Describe your method. How did you split the data (train/test)? What classifier did you choose and why? How did you tune hyperparameters?</p>
<p><strong>5b</strong>: Figure and caption: Show a confusion matrix for your gender-from-user-features classifier. What error rate do you get? Is it significantly better than chance for this dataset? </p>
<p><strong>5c</strong>: Paragraph: Is predicting a user's gender "harmless", or are there applications of a recommendation system where this might have real-world consequences? How should a responsible AI practitioner handle this? What questions should we ask to decide if this tool should be "released"?</p>
<h2><a name="rubric">Rubric for Evaluating PDF Report</a></h2>
<p>Earning full credit on this assignment requires a well-thought-out report that demonstrates you understand the models and training procedures we're studying here.</p>
<p>We're looking for the itemized content outlined above, with professional figures and captions that <em>tell a story</em>, concise but well-thought out short answers, and evidence of best practices throughout. </p>
<p>Points will be allocated across the various parts as follows:</p>
<ul>
<li>10%: Problem 1</li>
<li>20%: Problem 2</li>
<li>30%: Problem 3</li>
<li>40%: Problem 4</li>
</ul>
<p>The BONUS problem, if completed successfully, will be worth up to 8% <em>toward your overall project C grade</em>.</p>
<p>Note: You cannot get higher than an 105% on project C.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">



        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          MIT License
          /
          <a href="https://github.com/tufts-ml/tufts_ml_website">
          Source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">Powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>