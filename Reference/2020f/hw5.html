<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>HW5: Kernels for Regression, SVMs, and PCA | Introduction to Machine Learning
</title>
  <link rel="canonical" href="https://www.cs.tufts.edu/comp/135/2020f/hw5.html">



  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <script src="https://www.cs.tufts.edu/comp/135/2020f/theme/js/icsFormatter.js"></script>

  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/style.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/custom.css">


<meta name="description" content="Status: RELEASED. Due date: Wed. Dec. 2 at 11:59PM AoE (Anywhere on Earth) (Thu 12/3 at 07:59am in Boston) Updates 2020-12-01 : CLARIFIED in Task 6(i) what to report in Table 6 for the periodic nearest neighbor baseline on train+valid dataset 2020-11-25 : FIX to doctests updated â€¦">
</head>

<body>
  <header class="header">
    <nav class="navbar navbar-expand-lg navbar-expand-md navbar-light bg-light">
    <div class="container">
    <div class="row display-flex">
        <div class="col-2 col-sm-2 d-md-none"><!-- hidden if md or lg -->
        <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#my_collapsing_navbar"
            aria-controls="my_collapsing_navbar"
            aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon mw-100"></span>
        </button>
        </div>

        <div class="d-none d-md-block col-md-2">
          <a href="https://www.cs.tufts.edu/comp/135/2020f/">
            <img class="img-fluid mw-100" src=https://www.cs.tufts.edu/comp/135/2020f/images/tufts_ml_logo.png alt="Introduction to Machine Learning">
          </a>
        </div>

        <div class="col-10 col-sm-10 col-md-10">
          <h1 class="text-left" style="word-break:'break-all'">
            <a href="https://www.cs.tufts.edu/comp/135/2020f/">Introduction to Machine Learning</a>
          </h1>

          <p class="text-muted text-left d-none d-md-block mw-100">
            Tufts CS COMP 135 Intro ML | Fall 2020
          </p>



          <div class="collapse navbar-collapse" id="my_collapsing_navbar">
                <ul class="navbar-nav">
                  <li class="nav-item text-left">
                    <a href="index.html">Syllabus</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="schedule.html">Schedule</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="assignments.html">Assignments</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="office_hours.html">Office Hours</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="resources.html">Resources</a>
                  </li>

                </ul>
          </div>


        </div>

    </div>
    </div>
    </nav>

  </header>

  <div class="main">
    <div class="container">
      <h1>HW5: Kernels for Regression, SVMs, and PCA
</h1>
      <hr>
<article class="article">
  <div class="content">
        <p style="text-align:right">Last modified: 2020-12-01 09:40 </p>
    <p><strong>Status: RELEASED.</strong></p>
<p><strong>Due date</strong>: Wed. Dec. 2 at 11:59PM AoE (Anywhere on Earth) (Thu 12/3 at 07:59am in Boston)</p>
<p><strong> Updates </strong></p>
<ul>
<li>
<p>2020-12-01 : CLARIFIED in Task 6(i) what to report in Table 6 for the <a href="#updated-baseline-instructions">periodic nearest neighbor baseline on train+valid dataset</a></p>
</li>
<li>
<p>2020-11-25 : FIX to doctests updated to <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw5">sqexp_kernel.py in starter code repo</a></p>
</li>
</ul>
<h2>Overview</h2>
<p>In this HW, you'll complete the following:</p>
<p>Part I: Concept Questions (25% of your grade)</p>
<ul>
<li>Problem 0 has <em>conceptual</em> questions about SVMs</li>
<li>Problem 1 has <em>conceptual</em> questions about PCA</li>
</ul>
<p>Part II: Case Study on Kernel Methods for Temperature Forecasting</p>
<ul>
<li>Problem 2 has <em>coding</em> tasks (25% of your grade)</li>
<li>
<ul>
<li>You'll edit starter code in .py files.</li>
</ul>
</li>
<li>
<p>Problems 3-6 are <em>analysis</em> and <em>implementation</em> tasks (50% of your grade)</p>
</li>
<li>
<ul>
<li>You'll implement tasks in a Jupyter notebook and produce plots and tables for a report</li>
</ul>
</li>
</ul>
<p><strong>Turn-in links</strong>:</p>
<ul>
<li>PDF report turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/859942/">https://www.gradescope.com/courses/173055/assignments/859942/</a></li>
<li>ZIP file of source code turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/859941/">https://www.gradescope.com/courses/173055/assignments/859941/</a></li>
<li>Finally, complete your reflection here: <a href="https://docs.google.com/forms/d/1cBsuzUsTZ_moOKOtNaMMbuMlkoNmrrnIFIYTddVs73w">https://docs.google.com/forms/d/1cBsuzUsTZ_moOKOtNaMMbuMlkoNmrrnIFIYTddVs73w</a></li>
</ul>
<p><strong>Files to Turn In:</strong></p>
<p>PDF report:</p>
<ul>
<li>Prepare a short PDF report (no more than 5 pages).</li>
<li>This document will be manually graded.</li>
<li>Can use your favorite report writing tool (Word or G Docs or LaTeX or ....)</li>
<li>Should be <strong>human-readable</strong>. Do not include code. Do NOT just export a jupyter notebook to PDF.</li>
<li>Should have each subproblem <a href="https://www.youtube.com/watch?v=KMPoby5g_nE&amp;feature=youtu.be&amp;t=43">marked via the in-browser Gradescope annotation tool</a>)</li>
</ul>
<p>ZIP file of source code should contain:</p>
<ul>
<li>linear_kernel.py (will be autograded)</li>
<li>sqexp_kernel.py (will be autograded)</li>
<li>periodic_kernel.py (will be autograded)</li>
<li>hw5.ipynb (just for completeness, will not be autograded but will be manually assessed if necessary.)</li>
</ul>
<p><strong>Evaluation Rubric:</strong></p>
<p>See the PDF submission portal on Gradescope for the point values of each problem. Generally, tasks with more coding/effort will earn more potential points.</p>
<p><strong>Jump to</strong>: 
<a href="#background">Background</a> &nbsp; 
<a href="#problem-0">Problem 0</a> &nbsp; <a href="#problem-1">Problem 1</a> &nbsp; 
<a href="#dataset">Dataset</a> &nbsp; <a href="#starter-code">Starter Code</a> &nbsp; 
<a href="#problem-2">Problem 2</a> &nbsp; <a href="#problem-3">Problem 3</a>  &nbsp;
<a href="#problem-4">Problem 4</a> &nbsp; <a href="#problem-5">Problem 5</a> &nbsp; <a href="#problem-6">Problem 6</a></p>
<h2><a id="background">Background</a></h2>
<p>To complete this HW, you'll need some knowledge from the following sessions of class:</p>
<ul>
<li>SVMs (day18)</li>
<li>Kernel Methods (day19 and day20)</li>
<li>PCA (day22)</li>
</ul>
<p>Below, we have specified relevant kernel functions for feature vectors <span class="math">\(x \in \mathbb{R}^F\)</span> of size <span class="math">\(F\)</span>.</p>
<h4><a id="linear-kernel">Linear kernel or (dot product kernel) </a></h4>
<p>This is the simplest kernel: just the dot product (aka inner product) of the two original feature vectors:</p>
<div class="math">$$
k(x, z) = \sum_{f=1}^F x_f z_f = x^T z
$$</div>
<p>This kernel has no hyperparameters.</p>
<h4><a id="sqexp-kernel">Squared-exponential kernel</a></h4>
<p>This kernel signals its "similarity" by the squared distance between two feature vectors of size <span class="math">\(F\)</span>.</p>
<div class="math">$$
k(x, z) =  \text{exp} \left( - \frac{(x-z)^T(x-z)}{\ell^2} \right) = \text{exp} \left( - \frac{\sum_{f=1}^F (x_f - z_f)^2}{\ell^2} \right)
$$</div>
<p>This kernel has one hyperparameter: the length-scale <span class="math">\(\ell &gt; 0\)</span>.</p>
<p>We like the "length scale" hyperparameter because it is more interpretable on the scale of the observed features <span class="math">\(x\)</span>:</p>
<ul>
<li>A length scale of 1 means:</li>
<li>--- if x and z are about 1 apart (1 in squared distance), they have modest similarity of <span class="math">\(e^{-1} = \frac{1}{e} = 0.36\)</span></li>
<li>
<p>--- if x and z are further than 3 apart, they have pretty negligible similarity: <span class="math">\(e^{-3} = \frac{1}{e^3} = 0.05\)</span></p>
</li>
<li>
<p>A length scale of 3 means:</p>
</li>
<li>--- if x and z are about 3 apart (9 in squared distance), they have modest similarity of <span class="math">\(e^{-1} = \frac{1}{e} = 0.36\)</span></li>
<li>--- if x and z are further than 9 apart, they have pretty negligible similarity: <span class="math">\(e^{-3} = \frac{1}{e^3} = 0.05\)</span></li>
</ul>
<p>In other words, if we fix <span class="math">\(x\)</span> and fix a length scale <span class="math">\(\ell\)</span>, it is only points <span class="math">\(z\)</span> within say <span class="math">\(\pm 3 \ell\)</span> of <span class="math">\(x\)</span> that "matter" (all other points will have very low kernel function values and thus low similarity).</p>
<p><strong>Other names</strong>: Sometimes this kernel is called a "radial basis function" kernel or "Gaussian" kernel.</p>
<p><strong>Other parameterizations</strong>: Sometimes, this kernel written with an alternative hyperparameter <span class="math">\(\gamma = \frac{1}{\ell^2}\)</span>. </p>
<div class="math">$$
k( x, z) = \exp( -\gamma (x - z)^T(x-z) )
$$</div>
<h4><a id="periodic-kernel">Periodic kernel</a></h4>
<p>This kernel signals its "similarity" via a <em>periodic</em> function of two univariate values <span class="math">\(x\)</span> and <span class="math">\(z\)</span>:</p>
<div class="math">$$
k(x, z) = \text{exp} \left( - \frac{1}{2} \frac{ \left( \sin ( \frac{\pi}{p} (x - z) ) \right)^2 }{\ell^2} \right)
$$</div>
<p>The hyperparameter <span class="math">\(p\)</span> is the <em>period</em>, and the hyperparameter <span class="math">\(\ell\)</span> is a length scale.</p>
<p>Under this kernel, given fixed <span class="math">\(x\)</span>, the values <span class="math">\(x+p\)</span>, <span class="math">\(x+2p\)</span>, <span class="math">\(x+3p\)</span>, ... are equally "similar" under this kernel.</p>
<p>This kernel was originally suggested by David MacKay in a <a href="http://www.inference.org.uk/mackay/gpB.pdf#page=15">1998 paper</a></p>
<p>The plot below illustrates this kernel function at several possible hyperparameter values:</p>
<div class="row justify-content-md-center">
<div class="col-md-10">
    <div class="thumbnail">
<img src="images/hw5_periodic_kernel_illustration.png" class="img-fluid" alt="Periodic Kernel visualization">
    </div>
    <div class="caption">
        <p>Visualization of the periodic kernel at length-scale and period hyperparameters.
        </p>
    </div>
</div>
</div>

<h1>Part I: HW5 Concept Questions</h1>
<h2><a name="problem-0"> Problem 0: Conceptual Questions about SVMs </a></h2>
<p>Throughout Problem 0, you can assume a <em>binary</em> classification task with <span class="math">\(N\)</span> training examples, each with feature vector of size <span class="math">\(F\)</span>. Each feature is a numerical real value, and each feature has been separately normalized so the training set mean is 0 and variance is 1. You can guarantee that the maximum absolute value of any feature is at most 5.</p>
<p>Consider using a Support Vector Machine binary classifier with an RBF kernel in practice via the `SVC' implementation in sklearn. You could construct such a classifier as follows:</p>
<div class="highlight"><pre><span></span><span class="n">my_svm</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="k">C</span><span class="o">=</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p>Recall that the radial basis function (RBF) kernel is <a href="#sqexp-kernel">defined above in Background</a>. We emphasize that sklearn's RBF kernel implementation uses the "gamma" parameterization of the RBF, with hyperparameter <span class="math">\(\gamma &gt; 0\)</span>.</p>
<p>The hyperparameter <span class="math">\(C\)</span> controls the relative strength of the hinge loss in the training objective, relative to another term that penalizes the complexity of the weight parameters.</p>
<p>For further details about what these hyperparameters mean, consult the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">documentation for sklearn.svm.SVC</a>.</p>
<p>Please answer these questions as short answers in your PDF report:</p>
<p><strong>0a</strong>: Describe a setting of the hyperparameters <code>gamma</code> and <code>C</code> that is sure to <em>overfit</em> to a typical training set and achieve zero training error.</p>
<p><em>Answer the following True/False question, providing </em>one sentence<em> of explanation.</em></p>
<p><strong>0b True or False? Why?</strong>: An advantage of an SVM is that even with an L2 penalty on the weights, if we solve the "primal" optimization problem to train our model the optimal weight vector <span class="math">\(w \in \mathbb{R}^F\)</span> (a vector with one entry per feature) will typically be sparse. </p>
<h2><a name="problem-1">Problem 1: Conceptual Questions about Principal Components Analysis (PCA) </a></h2>
<p>Consider Principal Components Analysis, which is described in the following resources:</p>
<ul>
<li>ISL textbook Chapter 10 (specifically sections 10.1 and 10.2)</li>
<li>Jake VanderPlas' Python Data Science Handbook: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html</a></li>
</ul>
<p>You can assume you have a training dataset <span class="math">\(X\)</span> with <span class="math">\(N\)</span> examples, each one a feature vector with <span class="math">\(F\)</span> features. You can imagine that it is a nutrition application, with many measurements describing a patient's height, weight, regular daily food consumption, and many other attributes. </p>
<p>Please answer the following questions about PCA, as short answers in your report PDF.</p>
<p><strong>1a True or False? Why?</strong>: A recommended way to select the number of components <span class="math">\(K\)</span> is to find the <span class="math">\(K\)</span> value that minimizes reconstruction error on the training set. This choice will help us balance high-quality reconstructions and model complexity.</p>
<p><strong>1b</strong>: Suppose you make two versions of your dataset with <span class="math">\(N\)</span> examples and <span class="math">\(F &gt; 1\)</span> features. In the first version, you measure the height feature in <em>centimeters</em>. In the second, you convert the height feature to units of <em>meters</em> instead (but leave all other columns of data unchanged). Would the first K PCA components of these two datasets be the same, or different? Why?</p>
<p><strong>1c</strong>: Stella has a training dataset <span class="math">\(X = \{ x_n \}_{n=1}^N\)</span>, which has <span class="math">\(N\)</span> example feature vectors <span class="math">\(x_n\)</span> each with <span class="math">\(F\)</span> features. She applies PCA to her training set to learn a matrix <span class="math">\(W\)</span> of shape <span class="math">\((K, F)\)</span>, where each row represents the basis vector for component <span class="math">\(k\)</span>.</p>
<p>She would like to project her <strong>test</strong> dataset <span class="math">\(X' = \{ x'_t \}_{t=1}^T\)</span> using the same components <span class="math">\(W\)</span>. She remembers that she needs to <em>center</em> her data, so she applies the following 3 steps to project each <span class="math">\(F\)</span>-dimensional test vector <span class="math">\(x'_t\)</span> to a <span class="math">\(K\)</span>-dimensional vector <span class="math">\(z'_t\)</span>:</p>
<div class="math">\begin{align}
m &amp;= \frac{1}{T} \sum_{t=1}^T x'_t \qquad \text{(compute the mean)}
\\
\tilde{x}'_t &amp;= x'_t - m  \qquad \text{(center the data by subtracting mean)}
\\
z_t &amp;= W \tilde{x}'_t  \qquad \text{(project down to K-dimensions)}
\end{align}</div>
<p>Is this correct? If so, explain why. If not, explain what Stella should do differently.</p>
<h1>Part II: HW5 Case Study: Kernelized Linear Regression for Temperature Forecasting</h1>
<h4><a name="dataset"> Dataset: Daily Minimum Temperature in Melbourne, Australia </a></h4>
<p>In Problems 2-5, we'll try our new kernel regression skills to forecast the minimum-recorded outdoor temperature in Melbourne, Australia on a specific day in the future.</p>
<p>This is a supervised learning problem, where for each day (indexed by <span class="math">\(n\)</span>) we have:</p>
<ul>
<li>Input <span class="math">\(x_n \in \mathbb{R}\)</span>, of type <code>float</code>, is the number of fractional years between that day and Jan 1, 1985.</li>
<li>Output <span class="math">\(y_n \in \mathbb{R}\)</span>, of type <code>float</code>, is the lowest temperature (in degrees Celsius) recorded on that day at a weather station in Melbourne</li>
</ul>
<p>We're given a dataset of <span class="math">\(N=2400\)</span> training examples (from 1981-1986) and <span class="math">\(T=730\)</span> validation examples (from 1987 and 1988). All input,output pairs for both these datasets are shown here:</p>
<div class="row justify-content-md-center">
<div class="col-md-10">
    <div class="thumbnail">
<img src="images/hw5_melbourne_temp_dataset.png" class="img-fluid" alt="Dataset visualization: Melbourne temperatures 1981-1988">
    </div>
    <div class="caption">
        <p>Training and validation datasets for the daily temperature dataset. These are the minimum daily temperatures recorded in Melbourne, Australia.
        </p>
    </div>
</div>
</div>

<p>You are also given <span class="math">\(T=730\)</span> <em>test</em> examples (from 1989-1990). </p>
<p>Your goal is to accurately predict the temperature on this test set.</p>
<h3>Task: Regression</h3>
<p>We want to build a <em>forecasting</em> regression model, so that we can predict the future temperature on some arbitrary date in the future (say on May 5th 3 years after the training data was collected).</p>
<p>Critically, to properly assess our model's forecasting ability, we will:</p>
<ul>
<li>Train parameters on a training set (our available dataset is from 1981-1986)</li>
<li>Select hyperparameters on a validation dataset which comes from a time period <em>in the future</em> compared to the training set (1987-1988)</li>
<li>Report our final performance on a test dataset from a time period <em>in the future</em> compared to the validation set (1989-1990).</li>
</ul>
<h4><a name="starter-code"> Starter Code </a></h4>
<p>See the hw5 folder of the public assignments repo for this class:</p>
<p><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw5">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw5</a></p>
<p>This starter code includes a notebook to help you get started on your analysis for Part II.</p>
<h2><a name="problem-2">Problem 2: Implementation of Kernel Functions </a></h2>
<h4><a name="code-task-2A"> Code Task A </a> (30% of code grade): Implement <code>calc_linear_kernel</code></h4>
<p>See the <a href="#linear-kernel">Background on linear kernels</a>.</p>
<p>You'll need to edit the starter code file: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw5/linear_kernel.py">linear_kernel.py</a>.</p>
<h4><a name="code-task-2B"> Code Task B </a> (30% of code grade): Implement <code>calc_sqexp_kernel</code></h4>
<p>See the <a href="#sqexp-kernel">Background on squared exponential kernels</a>.</p>
<p>You'll need to edit the starter code file: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw5/sqexp_kernel.py">sqexp_kernel.py</a>.</p>
<h4><a name="code-task-2C"> Code Task C </a> (40% of code grade): Implement <code>calc_periodic_kernel</code></h4>
<p>See the <a href="#periodic-kernel">Background on periodic kernels</a>.</p>
<p>You'll need to edit the starter code file: <a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw5/periodic_kernel.py">periodic_kernel.py</a>.</p>
<h2><a name="problem-3">Problem 3: Linear Kernel + Ridge Regression </a></h2>
<p>In this problem, you'll train and evaluate a linear kernel + ridge regression pipeline.</p>
<p>Follow the starter code notebook for detailed suggestions. </p>
<h4>Task 3(i): Select Hyperparameters via Grid Search on Validation Set</h4>
<p>Perform a grid search for these hyperparameters:</p>
<ul>
<li>ridgeRegressor <code>alpha</code> in np.logspace(-5, 5, 11)</li>
</ul>
<p>Use the possible value grids specified in the starter notebook.</p>
<p>You want to find the value that maximizes <em>mean absolute error</em> on the validation set.</p>
<h4>Task 3(ii): Fit Model with Best Hyperparameters to Train+Validation</h4>
<p>Store this model for later use.</p>
<h4>Figure 3 in your report</h4>
<p>Plot the predictions of the best model, overlaid on the training and validation sets.</p>
<h4>Short Answer 3 in your report</h4>
<p>Reflect on Figure 3.
Given what you know about this kernel, do the predictions make sense?
How good is this model at <em>interpolating</em>?
How good is this model at <em>extrapolating</em>?</p>
<h2><a name="problem-4">Problem 4: Squared-Exponential Kernel + Ridge Regression </a></h2>
<p>In this problem, you'll train and evaluate a sqexp kernel + ridge regression pipeline.</p>
<p>Follow the starter code notebook for detailed suggestions. </p>
<h4>Task 4(i): Select Hyperparameters via Grid Search on Validation Set</h4>
<p>Perform a grid search for these possible hyperparameters:</p>
<ul>
<li>sqexpKernel <code>length_scale</code></li>
<li>ridgeRegressor <code>alpha</code></li>
</ul>
<p>Use the possible value grids specified in the starter notebook.</p>
<p>You want to find the value that maximizes <em>mean absolute error</em> on the validation set.</p>
<h4>Task 4(ii): Fit Model with Best Hyperparameters to Train+Validation</h4>
<p>Store this model for later use.</p>
<h4>Figure 4 in your report</h4>
<p>Plot the predictions of the best model, overlaid on the training and validation sets.</p>
<h4>Short Answer 4 in your report</h4>
<p>Reflect on Figure 4.
Given what you know about this kernel, do the predictions make sense?
How good is this model at <em>interpolating</em>?
How good is this model at <em>extrapolating</em>?</p>
<h2><a name="problem-5">Problem 5: Periodic Kernel + Ridge Regression </a></h2>
<p>In this problem, you'll train and evaluate a periodic kernel + ridge regression pipeline.</p>
<p>Follow the starter code notebook for detailed suggestions.</p>
<h4>Task 5(i): Select Hyperparameters via Grid Search on Validation Set</h4>
<p>Perform a grid search for these possible hyperparameters:</p>
<ul>
<li>periodicKernel <code>period</code></li>
<li>periodicKernel <code>length_scale</code></li>
<li>ridgeRegressor <code>alpha</code></li>
</ul>
<p>Use the possible value grids specified in the starter notebook.</p>
<p>You want to find the value that maximizes <em>mean absolute error</em> on the validation set.</p>
<h4>Task 5(ii): Fit Model with Best Hyperparameters to Train+Validation</h4>
<p>Store this model for later use.</p>
<h4>Figure 5 in your report</h4>
<p>Plot the predictions of the best model, overlaid on the training and validation sets.</p>
<h4>Short Answer 5 in your report</h4>
<p>Reflect on Figure 5.
Given what you know about this kernel, do the predictions make sense?
How good is this model at <em>interpolating</em>?
How good is this model at <em>extrapolating</em>?</p>
<h2><a name="problem-6"> Problem 6: Final Showdown </a></h2>
<p>In this problem, we'll compare the models we developed in Problems 3-5, in terms of performance on the <em>test</em> set.</p>
<h4>Task 6(i): Baseline prediction: "Periodic Nearest Neighbor" regression</h4>
<p>As a baseline prediction, consider predicting the temperature on some day in the test set by simply taking the <em>median</em> of all temperatures on the <em>same</em> calendar day on all available years in the development set (training plus validation together).</p>
<p>That is, if you want a prediction for December 2, 1989, simply consider the <em>median</em> of temperature values for:</p>
<ul>
<li>December 2, 1981 (the first year of the combined <em>train+valid</em> set)</li>
<li>December 2, 1982</li>
<li>...</li>
<li>December 2, 1987 (the last year of the combined <em>train+valid</em> set)</li>
</ul>
<p>Remember that the <em>x</em> feature value is in units of fractional years, so subtracting 1 moves you 365 days behind, subtracting 2 moves you 2*365 days behind, etc..</p>
<p>Thus, if you want a baseline prediction for <span class="math">\(x = 6.8\)</span>, you can look up its "neighbors" by finding the examples whos <span class="math">\(x\)</span> values are 6.8-2, 6.8-3, 6.8-4, 6.8-5, 6.8-6, 6.8-7, 6.8-8. This will give you the 7 values in the combined training+validation set which occur on the same day of the year. FYI There may be some off-by-one errors due to leap years (e.g. there may not be an exact match, but you should find a close match). </p>
<p>You should <em>implement</em> this baseline yourself (see starter notebook) and compute its <em>mean absolute error</em>.</p>
<p><a name="updated-baseline-instructions"></a>
<strong>Update 2020-12-01</strong> Strategy to compute this baseline on train+valid set:
For each train+valid example, take whichever years of 2 back, 3 back, 4 back, ... 8 back <em>actually exist</em> for your example in the dataset, and just use the median of those (e.g. to predict for a day in 1983 you would only the corresponding day in 1981, to predict for 1985 you would have 1981-83). If you cannot find any neighbors for an example, just omit that example from the calculation.</p>
<h4>Table 6</h4>
<p>Make a Table showing the performance on <em>train+valid</em> and <em>test</em> sets, for</p>
<p>Include a row for each of these:</p>
<ul>
<li>The linear KLR model from Figure 3</li>
<li>The squared-exponential KLR model from Figure 4</li>
<li>The periodic KLR model from Figure 5</li>
<li>The periodic-nearest-neighbor baseline from Task 6(i)</li>
<li>-- For this baseline, we care most about test set performance</li>
<li>-- You can either skip the train+valid number, or use the strategy above to compute that.</li>
</ul>
<h4>Short Answer 6a</h4>
<p>Reflect on the performance of the models in Table 6. Which one is best in terms of test-set performance? Why? </p>
<h4>Short Answer 6b</h4>
<p>Is the margin of improvement for the best temperature forecasting method over the linear model meaningful "in the real world"? 
To answer this think about both the <em>units</em> of this error, and how people use a temperature forecast.
For example, suppose this forecast was the one published to the local population: would this improvement lead to changes in human experience (e.g. how people dress? how people plan their day?)</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">



        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          MIT License
          /
          <a href="https://github.com/tufts-ml/tufts_ml_website">
          Source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">Powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>