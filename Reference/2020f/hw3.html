<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>HW3: Neural Networks and Stochastic Gradient Descent | Introduction to Machine Learning
</title>
  <link rel="canonical" href="https://www.cs.tufts.edu/comp/135/2020f/hw3.html">



  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <script src="https://www.cs.tufts.edu/comp/135/2020f/theme/js/icsFormatter.js"></script>

  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/style.css">
  <link rel="stylesheet" href="https://www.cs.tufts.edu/comp/135/2020f/theme/css/custom.css">


<meta name="description" content="Status: RELEASED. Due date: Wed. Oct. 28 at 11:59PM AoE (Anywhere on Earth) (Thu 10/29 at 07:59am in Boston) Updates: 2020-10-27 : Fixed HW3 reflection link. Overview In this HW, you'll complete the following: Complete Problems 1-3 and write a report. You'll submit this PDF report to the …">
</head>

<body>
  <header class="header">
    <nav class="navbar navbar-expand-lg navbar-expand-md navbar-light bg-light">
    <div class="container">
    <div class="row display-flex">
        <div class="col-2 col-sm-2 d-md-none"><!-- hidden if md or lg -->
        <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#my_collapsing_navbar"
            aria-controls="my_collapsing_navbar"
            aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon mw-100"></span>
        </button>
        </div>

        <div class="d-none d-md-block col-md-2">
          <a href="https://www.cs.tufts.edu/comp/135/2020f/">
            <img class="img-fluid mw-100" src=https://www.cs.tufts.edu/comp/135/2020f/images/tufts_ml_logo.png alt="Introduction to Machine Learning">
          </a>
        </div>

        <div class="col-10 col-sm-10 col-md-10">
          <h1 class="text-left" style="word-break:'break-all'">
            <a href="https://www.cs.tufts.edu/comp/135/2020f/">Introduction to Machine Learning</a>
          </h1>

          <p class="text-muted text-left d-none d-md-block mw-100">
            Tufts CS COMP 135 Intro ML | Fall 2020
          </p>



          <div class="collapse navbar-collapse" id="my_collapsing_navbar">
                <ul class="navbar-nav">
                  <li class="nav-item text-left">
                    <a href="index.html">Syllabus</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="schedule.html">Schedule</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="assignments.html">Assignments</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="office_hours.html">Office Hours</a>
                  </li>
                  <li class="nav-item text-left">
                    <a href="resources.html">Resources</a>
                  </li>

                </ul>
          </div>


        </div>

    </div>
    </div>
    </nav>

  </header>

  <div class="main">
    <div class="container">
      <h1>HW3: Neural Networks and Stochastic Gradient Descent
</h1>
      <hr>
<article class="article">
  <div class="content">
        <p style="text-align:right">Last modified: 2020-10-27 12:50 </p>
    <p><strong>Status: RELEASED.</strong></p>
<p><strong>Due date</strong>: Wed. Oct. 28 at 11:59PM AoE (Anywhere on Earth) (Thu 10/29 at 07:59am in Boston)</p>
<p><strong>Updates</strong>:</p>
<ul>
<li>2020-10-27 : Fixed HW3 reflection link.</li>
</ul>
<h2>Overview</h2>
<p>In this HW, you'll complete the following:</p>
<ul>
<li>Complete Problems 1-3 and write a <em>report</em>.</li>
<li>
<ul>
<li>You'll submit this PDF report to the Gradescope link below.</li>
</ul>
</li>
<li>
<ul>
<li>The goal here is to demonstrate broad understanding of how to use MLPs effectively.</li>
</ul>
</li>
<li>
<ul>
<li>Much of your analysis will use library code in sklearn with similar functionality as what you implement yourself earlier.</li>
</ul>
</li>
</ul>
<p>There is NO autograder for this homework! You should still submit your notebook for completeness.</p>
<p><strong>Turn-in links</strong>:</p>
<ul>
<li>PDF report turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/698014/">https://www.gradescope.com/courses/173055/assignments/698014/</a></li>
<li>ZIP file of source code turned in to: <a href="https://www.gradescope.com/courses/173055/assignments/698010">https://www.gradescope.com/courses/173055/assignments/698010</a></li>
<li>Finally, complete your reflection here: <a href="https://forms.gle/4KhoyVBwDg1SpQgo8">https://forms.gle/4KhoyVBwDg1SpQgo8</a></li>
<li>
<ul>
<li>Reflection requires a <code>tufts.edu</code> G-Suite account: <a href="http://systems.eecs.tufts.edu/logging-into-g-suite/">http://systems.eecs.tufts.edu/logging-into-g-suite/</a></li>
</ul>
</li>
</ul>
<p><strong>Files to Turn In:</strong></p>
<p>PDF report:</p>
<ul>
<li>Prepare a short PDF report (no more than 4 pages).</li>
<li>This document will be manually graded.</li>
<li>Can use your favorite report writing tool (Word or G Docs or LaTeX or ....)</li>
<li>Should be <strong>human-readable</strong>. Do not include code. Do NOT just export a jupyter notebook to PDF.</li>
<li>Should have each subproblem <a href="https://www.youtube.com/watch?v=KMPoby5g_nE&amp;feature=youtu.be&amp;t=43">marked via the in-browser Gradescope annotation tool</a>)</li>
</ul>
<p>ZIP file of source code should contain:</p>
<ul>
<li>hw3.ipynb (just for completeness, will not be autograded but will be manually assessed if necessary.)</li>
</ul>
<p><strong>Evaluation Rubric:</strong></p>
<p>See the PDF submission portal on Gradescope for the point values of each problem. Generally, tasks with more coding/effort will earn more potential points.</p>
<p><strong>Jump to</strong>: 
<a href="#background">Background</a> &nbsp;
<a href="#starter-code">Starter Code</a> &nbsp;
<a href="#problem-1">Problem 1</a> &nbsp; <a href="#problem-2">Problem 2</a> &nbsp; <a href="#problem-3">Problem 3</a></p>
<h4><a name="starter-code"> Starter Code </a></h4>
<p>See the hw3 folder of the public assignments repo for this class:</p>
<p><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw3">https://github.com/tufts-ml-courses/comp135-20f-assignments/tree/master/hw3</a></p>
<p>This starter code includes a notebook to help you get started on your analysis, plus a helper <code>.py</code> file to help you visualize learned MLPs for debugging.</p>
<h2><a id="background">Background</a></h2>
<p>To complete this HW, you'll need some knowledge from the following sessions of class:</p>
<ul>
<li>Gradient Descent (day06)</li>
<li>Logistic Regression (day09)</li>
<li>Neural Networks (day10)</li>
<li>Backpropagation (day11)</li>
<li>SGD and LBFGS (day12)</li>
</ul>
<h4>Optimization Algorithms</h4>
<p>We'll compare two popular ways to solve optimization problems in Problems 1-3.</p>
<p>First, we have "SGD" or Stochastic Gradient Descent, which we covered in the day12 readings and lecture.</p>
<p>The most important facts about SGD are:</p>
<ul>
<li>This method uses first derivative (aka gradient) information only to update parameters</li>
<li>Each step, we <em>estimate</em> gradient of our objective by using a randomly chosen set of examples (a "batch" or "minibatch") that is a <em>subset</em> of the full training dataset </li>
<li>Each step, the <em>direction</em> of our parameter update involves stepping directly in the <em>downhill</em> direction (in opposite direction of gradient, which points uphill).</li>
<li>Each step, the <em>magnitude</em> of our parameter update step (the total distance traveled between old and new parameter values) is determined by the <em>learning rate</em> (aka step size), a scalar hyperparameter which must be set in advance. </li>
</ul>
<p>Second, we have "L-BFGS" or the Limited-memory BFGS (Broyden–Fletcher–Goldfarb–Shanno) Algorithm, which we briefly covered in the day12 lectures. You can find more detailed technical introduction here <a href="http://aria42.com/blog/2014/12/understanding-lbfgs">http://aria42.com/blog/2014/12/understanding-lbfgs</a>. The most important facts about L-BFGS are:</p>
<ul>
<li>This method uses both first derivative information as well as (approximate) second derivative information to update parameters</li>
<li>
<ul>
<li>The way it uses second-order derivatives is inspired by <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton's method</a>, so we call it a "quasi-Newton" optimization algorithm</li>
</ul>
</li>
<li>Each step, we <em>exactly</em> compute the gradient of our objective using the <em>entire</em> dataset (all N training examples)</li>
<li>Each step, the <em>direction</em> of our parameter update involves stepping in a <em>modified</em> <em>downhill</em> direction (we use second-order information to modify the direction of the step). </li>
<li>Each step, the <em>magnitude</em> of our parameter update is determined using a <em>line search</em>, which is a smart way to pick the size of the current step in a smart way once we've fixed a direction. There is no learning rate hyperparameter.</li>
</ul>
<h4>Activation Functions</h4>
<p>You also might want to read about different activation functions, especially the Rectified linear unit or "ReLU". 
Many activation functions are defined and explained here: <a href="http://cs231n.github.io/neural-networks-1/#actfun">Stanford's CS231n Notes on Activation Functions</a></p>
<h4>Stochastic Gradient Descent in psuedocode</h4>
<p>The SGD algorithm has the following pseudocode.</p>
<div class="highlight"><pre><span></span><span class="nv">x_NF</span>, <span class="nv">y_N</span> <span class="o">=</span> <span class="nv">load_training_dataset</span><span class="ss">()</span>                            # <span class="nv">N</span> <span class="o">=</span> <span class="nv">total</span> <span class="nv">number</span> <span class="nv">of</span> <span class="nv">training</span> <span class="nv">examples</span>

<span class="nv">model</span>.<span class="nv">initialize_parameters</span><span class="ss">(</span><span class="nv">random_state</span><span class="ss">)</span>                      # <span class="nv">Initialize</span> <span class="nv">weight</span><span class="o">/</span><span class="nv">bias</span> <span class="nv">to</span> <span class="k">random</span> <span class="nv">values</span>

<span class="k">for</span> <span class="nv">cur_iter</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">max_iter</span><span class="ss">)</span>:

    <span class="nv">n_examples_seen_this_iter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nv">n_examples_seen_this_iter</span> <span class="o">&lt;</span> <span class="nv">N</span>:

        <span class="nv">xb_BF</span>, <span class="nv">yb_B</span> <span class="o">=</span> <span class="nv">draw_random_batch</span><span class="ss">(</span><span class="nv">x_NF</span>, <span class="nv">y_N</span>, <span class="nv">batch_size</span><span class="ss">)</span> # <span class="nv">B</span> <span class="o">=</span> <span class="nv">batch_size</span>
                                                               # <span class="nv">xb_BF</span>.<span class="nv">shape</span> <span class="nv">is</span> <span class="ss">(</span><span class="nv">B</span>,<span class="nv">F</span><span class="ss">)</span>, <span class="nv">yb_B</span>.<span class="nv">shape</span> <span class="nv">is</span> <span class="ss">(</span><span class="nv">B</span>,<span class="ss">)</span>

        <span class="nv">grad_arr</span> <span class="o">=</span> <span class="nv">calc_grad</span><span class="ss">(</span><span class="nv">xb_BF</span>, <span class="nv">yb_B</span><span class="ss">)</span>                      # <span class="nv">grad</span> : <span class="nv">array</span> <span class="nv">with</span> <span class="nv">entry</span> <span class="k">for</span> <span class="nv">each</span> <span class="nv">param</span>

        <span class="nv">model</span>.<span class="nv">update_parameters</span><span class="ss">(</span><span class="nv">grad_arr</span>, <span class="nv">lr</span><span class="ss">)</span>                  # <span class="nv">take</span> <span class="nv">step</span> <span class="nv">downhill</span>. <span class="nv">param</span> <span class="o">=</span> <span class="nv">param</span> <span class="o">-</span> <span class="nv">lr</span> <span class="o">*</span> <span class="nv">grad</span>

        <span class="nv">n_examples_seen_this_iter</span> <span class="o">+=</span> <span class="nv">B</span>                         # <span class="nv">increment</span> <span class="nv">counter</span> <span class="k">for</span> <span class="nv">current</span> <span class="nv">iteration</span>
</pre></div>


<h4>Vocabulary: What is an iteration in SGD?</h4>
<p>Sometimes an "iteration" means different things in different contexts. We'll focus on what it means using sklearn's implementation.</p>
<p>Each <em>iteration</em> (also called an <em>epoch</em>) represents one or more gradient computation and parameter update steps (see pseudocode above).</p>
<p>Each iteration is complete when the number of examples it has ``seen'' (and used for updates) is equal to (or slightly bigger than) the total number examples in the dataset N.</p>
<p>Thus, the number of parameter updates that happen per iteration depends on the <code>batch_size</code>.</p>
<h2>Dataset: The flower XOR toy dataset</h2>
<p>We consider a "toy" dataset where each example (indexed by <span class="math">\(n\)</span>) has:</p>
<ul>
<li>a 2-dimensional feature vector: <span class="math">\(x_n \in \mathbb{R}^2\)</span></li>
<li>a binary label: <span class="math">\(y_n \in \{0, 1\}\)</span></li>
</ul>
<p>We have included in the starter code repo a training set of 10000 feature-label pairs, and a heldout set of 2000 feature-label pairs. (Note: we labeled the heldout set as a "test" set, but in this homework we're really using it like a validation set because we use it to help us select hyperparameters).</p>
<p>The training dataset is shown here (blue points indicate a positive class label (y=1), red points indicate negative class label (y=0)):</p>
<div class="row justify-content-md-center">
<div class="col-md-10">
    <div class="thumbnail">
<img src="images/flower_xor_dataset.png" class="img-fluid" alt="Flower XOR dataset">
    </div>
    <div class="caption">
        <p>Training Dataset for "Flower XOR" binary classification task.
        </p>
    </div>
</div>
</div>

<p>Clearly, a single linear-boundary classifier would be quite bad here.</p>
<p>Can we train a neural network to solve this classification task well?</p>
<p>We'll see that effectively solving this requires:</p>
<ul>
<li>Picking the right "architecture" for our neural network (Problem 1)</li>
<li>Picking the right optimization procedures for training our neural network (Problem 2-3)</li>
</ul>
<h2><a id="problem-1">Problem 1</a>: MLPs with L-BFGS: What model size is effective?</h2>
<h4>Basic MLP Settings for Problem 1</h4>
<p>We will use sklearn's <code>MLPClassifier</code> implementation: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">docs for <code>sklearn.neural_network.MLPClassifier</code></a>.</p>
<p>Throughout Problem 1, use the following <em>fixed</em> settings (already in your starter code).</p>
<p>Defining the method and objective function:</p>
<ul>
<li><code>activation='relu'</code> : Use the ReLU activation function</li>
<li><code>alpha=0.0001</code> : Add a small penalty term on sum-of-squares of all weights</li>
</ul>
<p>Defining the optimization procedure:</p>
<ul>
<li><code>solver='lbfgs'</code> : Iterative gradient method that uses first and second-order gradient information.</li>
<li><code>tol=1e-5</code> : Defines threshold for deciding training has converged (by comparing change in loss over iterations).</li>
</ul>
<h4>Architecture Search: How many hidden units should we use?</h4>
<p>In this problem, you'll fit MLPs using the lbfgs solver with sklearn's <code>MLPClassifier</code>. Each MLP will have one hidden layer, which means two layers of parameters total when we include the output layer.</p>
<p>We'll consider 4 possible sizes for our hidden layer : 4, 16, 64, and 256. This is the number of neurons or "units" in the layer.</p>
<p><strong> Implementation Step 1A </strong>: At each size listed above, try 4 different runs with 4 different values of <code>random_state</code>.
Multiple "runs" will let us understand how the random initialization of the weight and bias parameters impacts performance.</p>
<p>See the starter code notebook, which sets up most of this for you:</p>
<ul>
<li><a href="https://github.com/tufts-ml-courses/comp135-20f-assignments/blob/master/hw3/hw3.ipynb">hw3.ipynb</a></li>
</ul>
<p><strong> Implementation Step 1B </strong>: After each run is finished, record the following so you can plot it later:</p>
<ul>
<li>base-2 log loss on training set and test set</li>
<li>error rate on training set and test set</li>
</ul>
<p><strong> Figure 1 in Report </strong></p>
<p>Figure 1 should compare the performance of each model you've trained as a function of the size.</p>
<p>As in the starter notebook, create two different subplots:</p>
<ul>
<li>on the left, show LOG LOSS (base 2) vs. model size</li>
<li>on the right, show ERROR RATE vs. model size</li>
</ul>
<p>In each plot, show two kinds of points:</p>
<ul>
<li>one color for the training-set performance (use color BLUE ('b') and style 'd')</li>
<li>one color for the test-set performance (use color RED ('r') and style 'd')</li>
</ul>
<p>Each dot in your plot will represent the final result of one "run" of the optimizer.
By looking across multiple dots at each size, we'll be able to see how sensitive the model is to its random initialization and to the model size.</p>
<p><strong> Short Answer 1a in Report </strong></p>
<p>Based on your Figure 1, what hidden layer size would you recommend to achieve the best log loss on heldout data? Do you see signs of overfitting? </p>
<p><strong> Short Answer 1b in Report </strong></p>
<p>Based on your Figure 1, what hidden layer size would you recommend to achieve the best error rate on heldout data? Do you see signs of overfitting?</p>
<p><strong> Short Answer 1c in Report </strong></p>
<p>Consider a typical L-BFGS run with 64 hidden units.
What final log loss on the training set does it reach (round to nearest 0.01)?
About how many seconds does it take to converge or complete its maximum iteration (round to nearest 0.1 seconds)? Does it converge?</p>
<p><strong> Short Answer 1d in Report </strong></p>
<p>You have fit an MLP with <code>hidden_layer_sizes=[64]</code> to this flower XOR dataset.
How many total weight parameters are in each layer? How many total bias or intercept parameters in each layer? 
Show your work or justify your answer.</p>
<h2><a id="problem-2">Problem 2</a>: MLPs with SGD: What batch size and step size?</h2>
<p>This problem requires <em>no implementation</em>. To give you a jump start, we <em>already ran</em> an thorough experiment for you, summarized in Figure 2 below.</p>
<p>Your job is to <em>interpret</em> this figure and draw useful conclusions from it.</p>
<p>To make Figure 2, at each possible batch size and learning rate setting, we ran 4 random initializations of an MLP with 64 hidden units on the same training data as in Problem 1 (the flower xor dataset with N=10000 training examples).</p>
<p>Our experiment studied how performance depended on the following settings:</p>
<ul>
<li>Batch size: We tried <code>batch_size</code> in [10000, 500, 25]</li>
<li>Learning rate: We tried <code>learning_rate_init</code> in [0.1, 0.3, 0.9, 2.7]</li>
</ul>
<h3>Figure 2: SGD training loss vs. elapsed wallclock time, varying batch size (rows) and learning rate (columns)</h3>
<div class="row justify-content-md-center">
<div class="col-md-10">
    <div class="thumbnail">
<img src="images/hw3_fig2_sgd_loss_vs_time.jpg" class="img-fluid" alt="SGD loss vs wallclock time (seconds)">
    </div>
    <div class="caption">
        <p>Figure 2: SGD Training Loss vs Elapsed Wallclock Time.
            <a href="images/hw3_fig2_sgd_loss_vs_time.pdf"> Click here for high-res PDF version. </a>
        </p>
    </div>
</div>
</div>

<p><strong> Short Answer 2a in Report </strong></p>
<p>Based on Figure 2, is the training objective function for MLPs convex or not convex? What evidence in Figure 2 supports this answer?</p>
<p><strong> Short Answer 2b in Report </strong></p>
<p>For each batch size in Figure 2, which learning rate(s) do you recommend? You should prioritize learning rates that produce good training loss values <em>quickly</em> and <em>consistently</em> across at least 3 of the 4 runs, without showing severe divergence.</p>
<p><strong> Short Answer 2c in Report </strong></p>
<p>Using the recommended learning rates you picked in 2b, report for each SGD batch size the time taken (in seconds, rounded to nearest whole number) to deliver a "good" training loss value of 0.1 (e.g the time when at least 3 out of 4 runs reach log loss of 0.1). If this good value was never achieved, just write "the method only reached a loss of X consistently after Y seconds". </p>
<p><strong> Short Answer 2d in Report </strong></p>
<p>Based on 2c, which batch size is fastest to deliver a good model? Can you explain why? What tradeoffs are at work?</p>
<p><strong> Short Answer 2e in Report </strong></p>
<p>Compare speed of the best SGD to what you observed for the best size 64 runs of LBFGS from Problem 1d. Which method is better for this problem? Why?</p>
<p><strong> Short Answer 2f in Report </strong></p>
<p>Think in general about training neural networks on classification tasks like this one.
List 2 reasons to prefer L-BFGS over SGD when optimizing your neural network, and 2 reasons to prefer SGD over L-BFGS.</p>
<h2><a id="problem-3">Problem 3</a>: Producing your own figure comparing batch size and learning rate</h2>
<p>In this problem, you'll try to replicate Figure 2 above yourself.</p>
<p>The starter notebook provides <em>all</em> the required code. We just want you to run the code on your machine, see it yourself, and create your own figure.</p>
<p><strong> Implementation Step 3A </strong></p>
<p>Using the starter code, for each of the batch sizes and learning rates in Figure 2, you'll run <em>2 random initializations</em> (you do NOT need to do all 4 in the provided Figure 2), either to convergence or until the maximum number of iterations specified in the starter code is reached. Use the <a href="#problem2-settings">hyperparameter settings for Problems 2-3</a> detailed below.</p>
<p><em>Warning: Time Consuming.</em> Running SGD on this problem may take a while. Perhaps about 30-45 minutes depending on your machine. Get started early. Maybe let it run overnight. Or see below for instructions on using Google's Colab cloud computing notebook resources so you don't need fast local hardware.</p>
<p><em>Warning: Unlikely to exactly reproduce.</em> We fully expect that this won't exactly reproduce Figure 2, because:</p>
<ul>
<li>your run times might be quite different, because your hardware is different</li>
<li>your random initializations might be different, because numpy's randomness can vary by platform</li>
</ul>
<p>The point of Problem 3 is just to help you understand where Figure 2 comes from and give you deeper experience in using SGD.</p>
<p><strong> How to Use Google Colab:</strong> If your local computer is too slow (e.g. Problem 1 took much longer than 10 minutes), we have made a version of the Problem 3 notebook you can run in your browser via Google Colab, that will use some cloud computing hardware instead of your own.</p>
<p>Follow this link: <a href="https://colab.research.google.com/drive/1cYzZGkf20vtlPI6OdWh1c9WyckZi-bST">Problem 3 notebook on Google Colab</a>.</p>
<p>To use this notebook, <em>save a copy</em> under your own google drive account. Then run it in your browser (you can skip over problem 1). It should take around 30 min. Then you can save the figure(s) to your local computer and write your report.</p>
<p><strong> How to Turn In if you use Colab: </strong></p>
<p>You do not need to upload colab notebook to gradescope. Instead, please just modify your local notebook used for Problem 1 to include a link to your colab notebook under Problem 3. We just want to be able to check that you've run it yourself (so please make sure your shared link works). You should still <strong>upload your notebook with your solution to Problem 1</strong>.</p>
<p><strong> Figure 3 in Report </strong></p>
<p>On the last page of your report, provide your completed Figure as "Figure 3".</p>
<p>Provide a caption with 2-3 complete sentences noting any significant changes between your figure and our Figure 2. Do the major conclusions from Problem 2 hold?</p>
<h4><a id="problem2-settings"> Basic MLP Settings for Problem 2-3 </a></h4>
<p>We will again use sklearn's <code>MLPClassifier</code> implementation: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">docs for <code>sklearn.neural_network.MLPClassifier</code></a>.</p>
<p>We'll study the impact of <em>learning rate</em> and <em>batch size</em>. As above in Figure 2, we consider the following settings</p>
<ul>
<li><code>batch_size</code> in [10000, 500, 25]</li>
<li><code>learning_rate_init</code> in [0.1, 0.3, 0.9, 2.7]</li>
</ul>
<p>Otherwise, we'll use the following fixed settings</p>
<ul>
<li>
<p>Fixed settings defining the method and objective function:</p>
</li>
<li>
<ul>
<li><code>hidden_layer_sizes = [64]</code> : Fix model size to 64 units</li>
</ul>
</li>
<li>
<ul>
<li><code>activation='relu'</code> : Use the ReLU activation function</li>
</ul>
</li>
<li>
<ul>
<li><code>alpha=0.0001</code> : Add a small penalty term on sum-of-squares of all weights</li>
</ul>
</li>
<li>
<p>Fixed settings defining the optimization procedure:</p>
</li>
<li>
<ul>
<li><code>solver='sgd'</code></li>
</ul>
</li>
<li>
<ul>
<li><code>tol=1e-5</code></li>
</ul>
</li>
<li>
<ul>
<li><code>learning_rate='adaptive'</code> : Read sklearn docs to understand. </li>
</ul>
</li>
<li>
<ul>
<li><code>momentum=0.0</code> : Turns off a more advanced feature we don't need.</li>
</ul>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">



        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          MIT License
          /
          <a href="https://github.com/tufts-ml/tufts_ml_website">
          Source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">Powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>