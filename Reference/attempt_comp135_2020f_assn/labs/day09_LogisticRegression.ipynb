{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day09: Logistic Regression\n",
    "\n",
    "# Objectives\n",
    "\n",
    "* Gain intuition for how specific values of weights and bias parameters impact predicted probabilities.\n",
    "* -- Given weight and bias values, you could sketch a roughly correct prediction function\n",
    "* Appreciate possible numerical issues that arise without careful implementation\n",
    "\n",
    "\n",
    "# Outline\n",
    "\n",
    "* [Part 1: Finding good weights for 1-dim. toy example](#part1)\n",
    "* [Part 2: Finding good bias for 1-dim. toy example](#part2)\n",
    "* [Part 3: Finding good parameters for 2-dim. features](#part3)\n",
    "\n",
    "We expect you'll get through the first 2 parts for sure during the in-class session.\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "* Logistic regression has linear decision boundaries\n",
    "* Adjusting the weight magnitudes impacts how sharply the probabilities change around the boundary\n",
    "* Adjusting the bias impacts where the boundary is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a simple classification task with 1-dim features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in day07, let's think about a classification task where:\n",
    "\n",
    "Each input is just scalar $x$ between -1 and +1.\n",
    "\n",
    "The \"true\" label assignment function is as follows:\n",
    "\n",
    "$$\n",
    "y(x) = \\begin{cases} \n",
    "1 & \\text{if} ~ x > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The true labeling process also has some noise: after assigning a label with the above function, each example has a ~15% chance of the opposite label. \n",
    "\n",
    "This noise makes our classification interesting. The \"best case\" error rate is about ~15%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training set for 1-dim. toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generated this training set for you.\n",
    "\n",
    "N = 12\n",
    "\n",
    "x_N = np.asarray([\n",
    "    -0.975, -0.825, -0.603, -0.378, -0.284, -0.102,\n",
    "     0.169,  0.311,  0.431,  0.663,  0.795,  0.976])\n",
    "x_N1 = x_N.reshape((N,1)) # need an (N,1) shaped array for later use with sklearn\n",
    "\n",
    "y_N = np.asarray([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_N, y_N, 'ks', label='training set');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n",
    "plt.legend(bbox_to_anchor=(1.0, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for computing the log loss (aka binary cross entropy)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b) = \\sum_{n=1}^N \\text{log_loss}(y_n, \\sigma(w * x_n + b) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_and_plot_probas(w=0.0, b=0.0):\n",
    "    ''' Compute the log loss function at specific parameters\n",
    "    \n",
    "    Averages over the N training examples\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    w : float\n",
    "        Value of the weights parameters\n",
    "    b : float\n",
    "        Value of intercept/bias parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_loss_val : float\n",
    "        Value of log loss function at provided parameters\n",
    "    '''\n",
    "    lr = sklearn.linear_model.LogisticRegression()\n",
    "    lr.fit(x_N1, y_N) # just make sure the dimensions are set up correctly\n",
    "    # Deterministically set the weights and bias parameters\n",
    "    lr.coef_[:] = w\n",
    "    lr.intercept_[:] = b\n",
    "    # Make predictions (will use the weight and bias we just set)\n",
    "    yproba1_N = lr.predict_proba(x_N1)[:,1]\n",
    "    \n",
    "    # Compute average log loss over all N examples\n",
    "    base_e_loss_val = sklearn.metrics.log_loss(y_N, yproba1_N)\n",
    "    base_2_loss_val = base_e_loss_val / np.log(2.0)\n",
    "\n",
    "    # Compute log loss at each example (useful for visualization/debugging)\n",
    "    log_loss_N = [\n",
    "        sklearn.metrics.log_loss([y_N[n]], [yproba1_N[n]], labels=[0,1]) / np.log(2.0)\n",
    "        for n in range(N)]\n",
    "\n",
    "    # Make plot of the probabilities\n",
    "    fig, axgrid = plt.subplots(nrows=1, ncols=1, figsize=(4,2))\n",
    "    plt.plot(x_N, y_N, 'ks', label='training set');\n",
    "    L = 101\n",
    "    xgrid_L1 = np.linspace(-2.0, 2.0, L).reshape((L,1));\n",
    "    plt.plot(xgrid_L1, lr.predict_proba(xgrid_L1)[:,1], 'b.-', label='predicted Pr(y=1|x) on dense x grid')\n",
    "    plt.legend(bbox_to_anchor=(1.0, 0.5));\n",
    "    \n",
    "    # Create a dataframe of all x, y, proba1, and loss values\n",
    "    # So that we can understand things in tabular view\n",
    "    yhat_N = np.asarray(yproba1_N >= 0.5, dtype=np.int32)\n",
    "    pd.set_option('precision', 3)\n",
    "    df = pd.DataFrame(np.hstack([\n",
    "            x_N,\n",
    "            y_N,\n",
    "            yproba1_N,\n",
    "            log_loss_N,\n",
    "            yhat_N,\n",
    "            y_N != yhat_N,\n",
    "        ]).reshape((6,N)).T,\n",
    "        columns=['x', 'y', 'p(y=1|x)', 'base2_log_loss', 'yhat', 'is_error'])\n",
    "    print(df)\n",
    "    \n",
    "    error_rate = np.mean(y_N != yhat_N)\n",
    "    return base_2_loss_val, error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: A manual search for the best weight\n",
    "\n",
    "Remember that with 1-dimensional features, your logistic regression has TWO parameters:\n",
    "\n",
    "* weight coefficient $w$\n",
    "* bias coefficient $b$\n",
    "\n",
    "Here, we'll learn how these values impact the shape of the predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w = 0.0 and b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(w=0.0, b=0.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w = + 2.0 and b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(w=2.0, b=0.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w = - 2.0 and b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(w=-2.0, b=0.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1a: Looking at examples above, do you see how the average log loss is an upper bound on the error rate? Is this a \"tight\" upper bound, or is there often a substantial difference? What kind of examples is the difference largest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO write your answer here, then discuss with your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b: Manual training of the weight. \n",
    "\n",
    "For this exercise, keep $b=0.0$. \n",
    "\n",
    "You should manually try many possible values for $w$ (e.g. -0.2, -0.1, 0.0, 0.1, 0.2, 0.5, 1.0, 2.0, ...)\n",
    "\n",
    "Can you find the *best possible* weight, in terms of the average log loss on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(\n",
    "    w=0.0, # TODO try different values for w here,\n",
    "    b=0.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO write down your optimal weight here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Manual training of the bias parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w = 0.0 and b = 1.0\n",
    "\n",
    "(Compare to w = 0.0, b = 0.0 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(w=0.0, b=1.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w = 2.0 and b = -1.0\n",
    "\n",
    "(Compare to w = 2.0, b = 0.0 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(w=2.0, b=-1.0)\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2a: What does the bias coefficient determine about the shape of the probability curve?\n",
    "\n",
    "What can you say about how moving the bias value higher or lower impacts the resulting predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b: Manual training of the bias. \n",
    "\n",
    "For this exercise, keep your weight *fixed* to the value you found in 1b. \n",
    "\n",
    "You should manually try many possible values for $b$ (e.g. -0.2, -0.1, 0.0, 0.1, 0.2, 0.5, 1.0, 2.0, ...)\n",
    "\n",
    "Can you find the *best possible* bias value, in terms of the average log loss on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_probas(\n",
    "    w=0.0, # TODO FIX to your result from 1b\n",
    "    b=0.0, # TODO try different values of b here\n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c: sklearn training of the weight and bias\n",
    "\n",
    "Here, we'll compare your manual guess-and-check search above with the result of sklearn's built in gradient descent procedure (it uses a fancy second-order gradient descent method called L-BFGS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large C value means effectively no penalty on the weights\n",
    "lr = sklearn.linear_model.LogisticRegression(C=1000000.0, solver='lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO here you should call the 'fit' method of the lr object\n",
    "# Use the existing arrays x_N1, y_N as input \n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_N1, y_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code will pretty print the w and b values found as a result of calling 'fit'\n",
    "print(\"Results from LogisticRegression\")\n",
    "print(\"Best w value: %.4f\" % (lr.coef_[0]))\n",
    "print(\"Best b value: %.4f\" % (lr.intercept_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2d: How did your guess-and-check compare?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO reflect with your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Manual training with F=2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Parts 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define simple dataset of points in 2D space\n",
    "\n",
    "Don't worry about the details of this setup.\n",
    "Just try to understand the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_dataset(N=100, noise_stddev=0.1, random_state=0):\n",
    "    random_state = np.random.RandomState(int(random_state))\n",
    "\n",
    "    mA_2 = np.asarray([1, 0])\n",
    "    covA_22 = np.square(noise_stddev) * np.eye(2)\n",
    "    \n",
    "    mB_2 = np.asarray([1, 1])\n",
    "    covB_22 = np.square(noise_stddev) * np.eye(2)\n",
    "\n",
    "    mC_2 = np.asarray([0, 1])\n",
    "    covC_22 = np.square(noise_stddev) * np.eye(2)\n",
    "\n",
    "    # Draw data from 3 \"Gaussian\" blobs\n",
    "    xA_N2 = random_state.multivariate_normal(mA_2, covA_22, size=N)\n",
    "    xB_N2 = random_state.multivariate_normal(mB_2, covB_22, size=N)\n",
    "    xC_N2 = random_state.multivariate_normal(mC_2, covC_22, size=N)\n",
    "\n",
    "    x_N2 = np.vstack([xA_N2, xB_N2, xC_N2])\n",
    "    y_N = np.hstack([np.ones(xA_N2.shape[0]), np.zeros(xB_N2.shape[0]), np.ones(xC_N2.shape[0])])\n",
    "    \n",
    "    return x_N2, y_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple F=2 dataset and visualize it\n",
    "\n",
    "Training data will be stored in these arrays:\n",
    "\n",
    "* x_tr_N2\n",
    "* y_tr_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_N2, y_tr_N = create_2d_dataset(N=100, noise_stddev=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pretty_data_colored_by_labels(x_N2, y_N):\n",
    "    plt.plot(x_N2[y_N==0,0], x_N2[y_N==0,1], color='r', marker='x', linestyle='', markersize=5, mew=2, label='y=0');\n",
    "    plt.plot(x_N2[y_N==1,0], x_N2[y_N==1,1], color='b', marker='+', linestyle='', markersize=8, mew=2, label='y=1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pretty_data_colored_by_labels(x_tr_N2, y_tr_N);\n",
    "plt.legend(bbox_to_anchor=(1.0, 0.5));\n",
    "plt.xlabel('x_1');\n",
    "plt.ylabel('x_2');\n",
    "plt.gca().set_aspect(1.0);\n",
    "plt.xticks([0, 1, 2]);\n",
    "plt.yticks([0, 1, 2]);\n",
    "plt.title(\"Binary classification example with 2-dim feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to make pretty plots of predicted probability color fields\n",
    "\n",
    "You don't need to understand this in detail. Just a utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_and_plot_pretty_probabilities(\n",
    "        w1=0.0,\n",
    "        w2=0.0,\n",
    "        b=0.0,\n",
    "        do_show_colorbar=True,\n",
    "        x1_ticks=np.asarray([0, 2, 4]),\n",
    "        x2_ticks=np.asarray([0, 2, 4]),\n",
    "        c_levels=np.linspace(0, 1, 100),\n",
    "        c_ticks=np.asarray([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        x1_grid=np.linspace(-1, 2.3, 100),\n",
    "        x2_grid=np.linspace(-1, 2.3, 100)):\n",
    "    cur_ax = plt.gca()\n",
    "    G = x1_grid.size\n",
    "    H = x2_grid.size\n",
    "    \n",
    "    # Get regular grid of G x H points, where each point is an (x1, x2) location\n",
    "    x1_GH, x2_GH = np.meshgrid(x1_grid, x2_grid)\n",
    "    \n",
    "    # Combine the x1 and x2 values into one array\n",
    "    # Flattened into M = G x H rows\n",
    "    # Each row of x_M2 is a 2D vector [x_m1, x_m2]\n",
    "    x_M2 = np.hstack([x1_GH.flatten()[:,np.newaxis], x2_GH.flatten()[:,np.newaxis]])\n",
    "    \n",
    "    # Create a convenience classifier object\n",
    "    # Whose weights we'll manually fill in\n",
    "    clf = sklearn.linear_model.LogisticRegression(C=10000.0)\n",
    "    clf.fit(x_tr_N2[::10], y_tr_N[::10]) # just need to initialize the clf and ensure dimensionality is right\n",
    "    clf.coef_[:] = np.asarray([w1, w2])\n",
    "    clf.intercept_[:] = b\n",
    "    \n",
    "    # Predict proba for each point in the flattened grid\n",
    "    yproba1_M = clf.predict_proba(x_M2)[:,1]\n",
    "    \n",
    "    # Reshape the M probas into the GxH 2D field\n",
    "    yproba1_GH = np.reshape(yproba1_M, x1_GH.shape)\n",
    "    \n",
    "    cmap = plt.cm.RdYlBu\n",
    "    my_contourf_h = plt.contourf(x1_GH, x2_GH, yproba1_GH, levels=c_levels, vmin=0, vmax=1.0, cmap=cmap)\n",
    "    \n",
    "    plt.xticks(x1_ticks, x1_ticks);\n",
    "    plt.yticks(x2_ticks, x2_ticks);\n",
    "    \n",
    "    if do_show_colorbar:\n",
    "        left, bottom, width, height = plt.gca().get_position().bounds\n",
    "        cax = plt.gcf().add_axes([left+1.1*width, bottom, 0.03, height])\n",
    "        plt.colorbar(my_contourf_h, orientation='vertical', cax=cax, ticks=c_ticks);\n",
    "        plt.sca(cur_ax);\n",
    "    plot_pretty_data_colored_by_labels(x_tr_N2, y_tr_N);\n",
    "    plt.legend(bbox_to_anchor=(1.0, 0.5));\n",
    "    plt.xlabel('x_1');\n",
    "    plt.ylabel('x_2');\n",
    "    plt.gca().set_aspect(1.0);\n",
    "    plt.xticks([0, 1, 2]);\n",
    "    plt.yticks([0, 1, 2]);\n",
    "    plt.title(\"w1= % 7.3f      w2=% 7.3f      b=% 7.3f\" % (w1, w2, b))\n",
    "    # Compute average log loss over all N examples in training set\n",
    "    base_e_loss_val = sklearn.metrics.log_loss(y_tr_N, clf.predict_proba(x_tr_N2)[:,1])\n",
    "    base_2_loss_val = base_e_loss_val / np.log(2.0)\n",
    "    error_rate = sklearn.metrics.zero_one_loss(y_tr_N, clf.predict_proba(x_tr_N2)[:,1] >= 0.5)\n",
    "    return base_2_loss_val, error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w1 = 0.0, w2=2.0, b=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_pretty_probabilities(\n",
    "    w1=0.0,\n",
    "    w2=2.0,\n",
    "    b=0.0, \n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w1 = 0.0, w2=10.0, b=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_pretty_probabilities(\n",
    "    w1=0.0,\n",
    "    w2=10.0,\n",
    "    b=0.0, \n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try w1 = -1.0, w2=0.0, b=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_pretty_probabilities(\n",
    "    w1=-1.0,\n",
    "    w2=0.0,\n",
    "    b=2.0, \n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3a: Imagine drawing a linear separator.\n",
    "\n",
    "Imagine drawing *one line* above, so that above the line would be one label prediction and below would be another.\n",
    "You want to draw this line to minimize error rate on the training set above.\n",
    "\n",
    "What line would you draw? Describe it graphically (e.g. at value $x_1 = ?$, the line would hit $x_2 = ?$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO provide a description of line and discuss with your group "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3b: Imagine your predicted probabilities\n",
    "\n",
    "Given your line above, how do you imagine the probabilities changing as you move *perpendicularly* from that line?\n",
    "\n",
    "Do you want the probabilities to saturate rapidly (e.g. so they become solid blue (y=1) or solid red (y=0) very close to the line)?\n",
    "\n",
    "Or do you want the probabilities to change gradually (e.g. so most of the picture would not be solid blue or solid red)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO provide description and discuss with your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3b: Manually finding parameters for this toy dataset via guess-and-check\n",
    "\n",
    "Can you manually search for w1, w2, and b values that give lowest loss on the training set?\n",
    "\n",
    "Just keep guessing values for each parameter until you find a good solution\n",
    "\n",
    "**Hint**: you should be able to get error rate less than 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_pretty_probabilities(\n",
    "    w1=-1.0, # TODO edit this value!\n",
    "    w2=0.0,  # TODO also edit this value\n",
    "    b=2.0,   # TODO also edit this value\n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3c: Compare to optimal weights from sklearn\n",
    "\n",
    "Using sklearn, we now want to fit a logistic regression model to the training set.\n",
    "\n",
    "This will minimize the average log loss via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LogisticRegression(C=1000000., solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call fit using the training set x_tr_N2, y_tr_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code will pretty print the w and b values found as a result of calling 'fit'\n",
    "print(\"Results from LogisticRegression\")\n",
    "print(\"Best w1 value: %.4f\" % (lr.coef_[0,0]))\n",
    "print(\"Best w2 value: %.4f\" % (lr.coef_[0,1]))\n",
    "print(\"Best b value: %.4f\" % (lr.intercept_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plug the estimated weights and bias in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error_rate = calc_loss_and_plot_pretty_probabilities(\n",
    "    w1=lr.coef_[0,0],\n",
    "    w2=lr.coef_[0,1],\n",
    "    b=lr.intercept_[0],\n",
    "    )\n",
    "print(\"average log loss on training set   = %.3f\" % loss)\n",
    "print(\"average error rate on training set = %.3f\" % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3c: How did your guess-and-check compare?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO reflect with your group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
