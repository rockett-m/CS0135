{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 : Neural Networks and Stochastic Gradient Descent\n",
    "\n",
    "This is the starter notebook for HW3.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "The authoritative HW3 instructions are on the course website:\n",
    "\n",
    "http://www.cs.tufts.edu/comp/135/2020f/hw3.html\n",
    "\n",
    "Please report any questions to Piazza.\n",
    "\n",
    "We've tried to make random seeds set explicitly so you can reproduce these exact results, but remember that some differences across platforms might occur, so the exact count of how many runs \"converge\" or not may not be precise (though hopefully in the same ballpark).\n",
    "\n",
    "### Outline of this Notebook\n",
    "\n",
    "\n",
    "* [Problem 1: MLP with LBFGS](#part1): How many hidden units should we pick?\n",
    "* [Problem 2: MLP with SGD](#part2): What batch size and learning rate should we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for some bonus visualizations\n",
    "\n",
    "from viz_tools_for_binary_classifier import plot_pretty_probabilities_for_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the \"Flower XOR\" dataset\n",
    "\n",
    "\n",
    "10,000 labeled examples in training set\n",
    "\n",
    "Each example has 2 features ($x_{n1}, x_{n2}$)\n",
    "\n",
    "Each label is binary (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data_flower_xor_2class\" # TODO fix to path on your local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "x_tr_N2 = np.loadtxt(os.path.join(DATA_DIR, 'x_train.csv'), skiprows=1, delimiter=',')\n",
    "y_tr_N = np.loadtxt(os.path.join(DATA_DIR, 'y_train.csv'), skiprows=1, delimiter=',')\n",
    "\n",
    "# Load test data\n",
    "x_te_T2 = np.loadtxt(os.path.join(DATA_DIR, 'x_test.csv'), skiprows=1, delimiter=',')\n",
    "y_te_T = np.loadtxt(os.path.join(DATA_DIR, 'y_test.csv'), skiprows=1, delimiter=',')\n",
    "\n",
    "assert x_tr_N2.shape[0] == y_tr_N.shape[0]\n",
    "assert x_te_T2.shape[0] == y_te_T.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = plt.subplots(figsize=(4,4));\n",
    "plt.plot(x_tr_N2[y_tr_N == 0,0], x_tr_N2[y_tr_N == 0,1], 'r.', label='y=0');\n",
    "plt.plot(x_tr_N2[y_tr_N == 1,0], x_tr_N2[y_tr_N == 1,1], 'b.', label='y=1');\n",
    "plt.legend(bbox_to_anchor=[1.0, 0.5], fontsize=15);\n",
    "plt.xlabel('x_1'); plt.ylabel('x_2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MLP with L-BFGS solver - What model size is best?\n",
    "\n",
    "Let's train an MLP at each of several different sizes.\n",
    "\n",
    "For this problem, we'll use the `solver='lbfgs'`\n",
    "\n",
    "We'll try multiple runs of the optimizer, to see what the impact of different random initializations of parameters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_list = [4, 16, 64, 256]\n",
    "n_runs = 4\n",
    "\n",
    "S = len(size_list)\n",
    "\n",
    "tr_loss_arr = 0.11 * np.ones((S, n_runs)) # Pre allocate with arbitrary values, just so code doesnt break\n",
    "te_loss_arr = 0.22 * np.ones((S, n_runs))\n",
    "\n",
    "tr_err_arr = 0.08 * np.ones((S, n_runs))\n",
    "te_err_arr = 0.13 * np.ones((S, n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter_per_run = 1000\n",
    "\n",
    "for size in size_list:\n",
    "    \n",
    "    for run_id, random_state in enumerate(range(n_runs)):\n",
    "        \n",
    "        start_time_sec = time.time()\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=[size],\n",
    "            activation='relu',\n",
    "            alpha=0.0001,\n",
    "            tol=1e-5,\n",
    "            max_iter=max_iter_per_run,\n",
    "            random_state=random_state,\n",
    "            solver='lbfgs', batch_size=10000,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            mlp.fit(x_tr_N2, y_tr_N)\n",
    "        elapsed_time_sec = time.time() - start_time_sec\n",
    "        mlp.did_converge = True if len(warn_list) == 0 else False\n",
    "\n",
    "        # TODO compute the log loss on training set and test set\n",
    "        # Can use code like this:\n",
    "        #      sklearn.metrics.log_loss(..., ...) / np.log(2)\n",
    "        # Remember we divide to convert to base-2, so that it is an upper bound on error rate\n",
    "        tr_log_loss = 0.1  # TODO fixme\n",
    "        te_log_loss = 0.05 # TODO fixme\n",
    "        \n",
    "        # TODO compute the error rate on the training set and test set\n",
    "        # Can use code like this:\n",
    "        #      sklearn.metrics.zero_one_loss(..., ...)\n",
    "        tr_err = 4.44 # TODO fixme\n",
    "        te_err = 5.55 # TODO fixme\n",
    "        \n",
    "        # TODO store this mlp's performance so you can plot it later\n",
    "        # You can use the provided `tr_loss_arr', or find another way that makes sense to you\n",
    "        \n",
    "        print('finished run %2d/%d after %5.1f sec | %13s after %3d iter | tr err %.3f | te err %.3f | layer_sizes = %3d' % (\n",
    "            run_id + 1, n_runs, elapsed_time_sec,\n",
    "            'converged' if mlp.did_converge else 'NOT converged',\n",
    "            max_iter_per_run,\n",
    "            tr_err, te_err, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For fun and understanding, visualize the latest run's learned MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_handle = plt.subplots(nrows=1, ncols=1, figsize=(4, 4));\n",
    "plot_pretty_probabilities_for_clf(mlp, x_tr_N2, y_tr_N, ax=ax_handle, do_show_colorbar=True);\n",
    "plt.xlabel('x_1'); plt.ylabel('x_2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1 (left): Log loss vs size\n",
    "\n",
    "TODO if you've filled in the `tr_loss_arr` and `te_loss_arr` correctly so it has shape (S,R) with S sizes and R runs, this code should make a nice plot for you.\n",
    "\n",
    "Each dot in the plot represents one completed run of the optimizer from a random initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4), sharex=True, sharey=True)\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    tr_label = 'train log loss' if run_id == 0 else ''\n",
    "    te_label = 'test log loss' if run_id == 0 else ''\n",
    "    \n",
    "    loss_ax.plot(np.log2(size_list), tr_loss_arr[:,run_id], 'bd', label=tr_label)\n",
    "    loss_ax.plot(np.log2(size_list), te_loss_arr[:,run_id], 'rd', label=te_label)\n",
    "\n",
    "loss_ax.set_xticks(np.log2(size_list));\n",
    "loss_ax.set_xticklabels(size_list);\n",
    "loss_ax.xaxis.grid(False);\n",
    "\n",
    "loss_ax.set_ylim([0, 0.8]); # Don't touch this please\n",
    "loss_ax.set_yticks(np.arange(0, 0.8, 0.1));\n",
    "\n",
    "loss_ax.set_ylabel('log loss');\n",
    "loss_ax.set_xlabel('size');\n",
    "loss_ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1 (right): Error vs size\n",
    "\n",
    "TODO if you've filled in the `tr_err_arr` and `te_err_arr` correctly so it has shape (S,R) with S sizes and R runs, this code should make a nice plot for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, err_ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4), sharex=True, sharey=True)\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    tr_label = 'train err' if run_id == 0 else ''\n",
    "    te_label = 'test err' if run_id == 0 else ''\n",
    "    \n",
    "    err_ax.plot(np.log2(size_list), tr_err_arr[:,run_id], 'bd', label=tr_label)\n",
    "    err_ax.plot(np.log2(size_list), te_err_arr[:,run_id], 'rd', label=te_label)\n",
    "\n",
    "err_ax.set_xticks(np.log2(size_list));\n",
    "err_ax.set_xticklabels(size_list);\n",
    "err_ax.xaxis.grid(False);\n",
    "\n",
    "err_ax.set_ylim([0, 0.25]); # Don't touch this please\n",
    "\n",
    "err_ax.set_ylabel('error rate');\n",
    "err_ax.set_xlabel('size');\n",
    "err_ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Problem 2\n",
    "\n",
    "Helper code for plotting training loss as more data is seen.\n",
    "\n",
    "The second argument 'x' can be adjusted by user so you plot loss versus *time* or loss versus *epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_plot_loss_curve_vs_x(mlp_list_by_lr, x='elapsed_time_sec', xlim_max=65, ylim_max=0.5):\n",
    "    ''' Create pretty plot of loss as more data is seen.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    mlp_list_by_lr : dict of lists\n",
    "        Each key is a learning rate.\n",
    "        Each value is a list of MLPClassifier objects\n",
    "    x : str\n",
    "        Either 'elapsed_time_sec' or 'epoch'\n",
    "        \n",
    "    Post Condition\n",
    "    --------------\n",
    "    Creates new matplotlib figure and plots a line:\n",
    "    * loss on y-axis\n",
    "    * the specified x variable on x-axis\n",
    "    '''\n",
    "\n",
    "    ncols = len(mlp_list_by_lr)\n",
    "    fig, ax = plt.subplots(\n",
    "        nrows=1, ncols=ncols, figsize=(ncols*4, 4),\n",
    "        squeeze=False,\n",
    "        sharex=True, sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for ii, (lr, mlp_list) in enumerate(mlp_list_by_lr.items()):\n",
    "        for mlp in mlp_list:\n",
    "            if x.count(\"time\"):\n",
    "                ax[ii].plot(\n",
    "                    mlp.elapsed_time_sec_ * np.linspace(1.0/mlp.n_iter_, 1.0, mlp.n_iter_),\n",
    "                    mlp.loss_curve_ / np.log(2.0), '-')\n",
    "                ax[ii].set_xlabel('elapsed time (sec)');\n",
    "            else:\n",
    "                ax[ii].plot(\n",
    "                    np.arange(mlp.n_iter_),\n",
    "                    mlp.loss_curve_ / np.log(2.0), '-')\n",
    "                ax[ii].set_xlabel('epochs completed');\n",
    "\n",
    "        ax[ii].set_title('lr %.3f \\n batch_size %d' % (lr, mlp.batch_size))\n",
    "\n",
    "        if ii == 0:\n",
    "            ax[ii].set_ylabel('log loss');\n",
    "        ax[ii].set_ylim([0.0, ylim_max]);\n",
    "        ax[ii].set_xlim([0.0, xlim_max]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-3: MLP with SGD : Which batch size and learning rate is best?\n",
    "\n",
    "### Goal of this problem: Can we become more scalable with SGD?\n",
    "\n",
    "There are N=10000 training examples in our training set. Can we use SGD to train our model faster?\n",
    "\n",
    "We'll fix our model size to 64 hidden units.\n",
    "\n",
    "Let's try out classic \"sgd\" (stochastic gradient descent), at a few different batch sizes and learning rates.\n",
    "\n",
    "Remember, what happens in SGD is the following pseudocode.\n",
    "\n",
    "```\n",
    "x_NF, y_N = load_training_dataset()                            # N = total number of training examples\n",
    "\n",
    "model.initialize_parameters(random_state)                      # Initialize weight/bias to random values\n",
    "\n",
    "for cur_iter in range(max_iter):\n",
    "\n",
    "    n_examples_seen_this_iter = 0\n",
    "    while n_examples_seen_this_iter < N:\n",
    "        \n",
    "        xb_BF, yb_B = draw_random_batch(x_NF, y_N, batch_size) # B = batch_size\n",
    "                                                               # xb_BF.shape is (B,F), yb_B.shape is (B,)\n",
    "        \n",
    "        grad_arr = calc_grad(xb_BF, yb_B)                      # grad : array with entry for each param\n",
    "        \n",
    "        model.update_parameters(grad_arr, lr)                  # take step downhill. param = param - lr * grad\n",
    "        \n",
    "        n_examples_seen_this_iter += B                         # increment counter for current iteration\n",
    "        \n",
    "   \n",
    "```\n",
    "\n",
    "### Vocabulary: What is an iteration in SGD?\n",
    "\n",
    "Sometimes this word means different things in different contexts. We'll focus on what it means using sklearn's implementation.\n",
    "\n",
    "Each *iteration* (also called an *epoch*) represents one or more gradient computation and parameter update steps (see pseudocode above).\n",
    "\n",
    "Each iteration is complete when the number of examples it has ``seen'' (and used for updates) is equal to (or slightly bigger than) the total number examples in the dataset N.\n",
    "\n",
    "Thus, the number of parameter updates that happen per iteration depends on the `batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Figure 2a: Try with batch_size = 10000 (each batch sees 100% of the dataset)\n",
    "\n",
    "We expect each separate \"run\" will take between 1-3 minutes.\n",
    "\n",
    "We want you to try at least 2 runs at each of 4 learning rates.\n",
    "\n",
    "So this block may take ~10-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [64]\n",
    "n_runs = 2\n",
    "lr_list = [0.10, 0.30, 0.90, 2.70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "max_iter_per_run = 8000 # Don't touch. Has been set so wallclock times are comparable.\n",
    "mlp_list_by_lr_10000 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lr_list:\n",
    "    mlp_list_by_lr_10000[lr] = list()\n",
    "        \n",
    "    for run_id, random_state in enumerate(range(n_runs)):\n",
    "\n",
    "        start_time_sec = time.time()\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=layer_sizes,\n",
    "            activation='relu',\n",
    "            alpha=0.0001,\n",
    "            tol=1e-5,\n",
    "            n_iter_no_change=50,\n",
    "            max_iter=max_iter_per_run,\n",
    "            random_state=random_state,\n",
    "            solver='sgd',\n",
    "            batch_size=batch_size,\n",
    "            learning_rate='adaptive', learning_rate_init=lr, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            mlp.fit(x_tr_N2, y_tr_N)\n",
    "        mlp.elapsed_time_sec_ = time.time() - start_time_sec\n",
    "        mlp.n_epoch_ = len(mlp.loss_curve_)\n",
    "        mlp.did_converge = True if len(warn_list) == 0 else False\n",
    "\n",
    "        # Add to the list\n",
    "        mlp_list_by_lr_10000[lr].append(mlp)\n",
    "        \n",
    "        # Pretty print summary of this run\n",
    "        msg = 'finished run %2d/%d after %5.1f sec | %13s after %4d iter | tr loss % .3f | lr = %.3f' % (\n",
    "            run_id + 1, n_runs,\n",
    "            mlp.elapsed_time_sec_,\n",
    "            'converged' if mlp.did_converge else 'NOT converged',\n",
    "            mlp.n_epoch_,\n",
    "            mlp.loss_ / np.log(2.0),\n",
    "            lr)\n",
    "        mlp.msg = msg\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss trace versus elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_10000, x='elapsed_time_sec', xlim_max=65, ylim_max=0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Figure 2b: Try with batch_size = 500 (each batch sees ~5% of the dataset)\n",
    "\n",
    "We expect each separate \"run\" will take between 1-3 minutes.\n",
    "\n",
    "We want you to try at least 2 runs at each of 4 learning rates.\n",
    "\n",
    "So this block may take ~10-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "\n",
    "max_iter_per_run = 2000 # Don't touch. Has been set so wallclock times are comparable.\n",
    "\n",
    "mlp_list_by_lr_500 = dict()\n",
    "for lr in lr_list:\n",
    "    mlp_list_by_lr_500[lr] = list()\n",
    "    for run_id, random_state in enumerate(range(n_runs)):\n",
    "        start_time_sec = time.time()\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=layer_sizes,\n",
    "            activation='relu',\n",
    "            alpha=0.0001,\n",
    "            tol=1e-5,\n",
    "            n_iter_no_change=50,\n",
    "            max_iter=max_iter_per_run,\n",
    "            random_state=random_state,\n",
    "            solver='sgd',\n",
    "            batch_size=batch_size,\n",
    "            learning_rate='adaptive', learning_rate_init=lr, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            mlp.fit(x_tr_N2, y_tr_N)\n",
    "        mlp.elapsed_time_sec_ = time.time() - start_time_sec\n",
    "        mlp.n_epoch_ = len(mlp.loss_curve_)\n",
    "        mlp.did_converge = True if len(warn_list) == 0 else False\n",
    "\n",
    "        # Add to the list\n",
    "        mlp_list_by_lr_500[lr].append(mlp)\n",
    "        \n",
    "        # Pretty print summary of this run\n",
    "        print('finished run %2d/%d after %5.1f sec | %13s after %4d iter | tr loss % .3f | lr = %.3f' % (\n",
    "            run_id + 1, n_runs,\n",
    "            mlp.elapsed_time_sec_,\n",
    "            'converged' if mlp.did_converge else 'NOT converged',\n",
    "            mlp.n_epoch_,\n",
    "            mlp.loss_ / np.log(2.0),\n",
    "            lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss trace versus elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_500, x='elapsed_time_sec', xlim_max=65, ylim_max=0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Figure 2c: Try with batch_size = 25 (tiny! each batch sees ~0.2% of the dataset)\n",
    "\n",
    "We expect each separate \"run\" will take between 1-3 minutes.\n",
    "\n",
    "We want you to try at least 2 runs at each of 4 learning rates.\n",
    "\n",
    "So this block may take ~10-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "max_iter_per_run = 500 # Don't touch. Has been set so wallclock times are comparable.\n",
    "\n",
    "mlp_list_by_lr_25 = dict()\n",
    "for lr in lr_list:\n",
    "    mlp_list_by_lr_25[lr] = list()\n",
    "    for run_id, random_state in enumerate(range(n_runs)):\n",
    "        start_time_sec = time.time()\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=layer_sizes,\n",
    "            activation='relu',\n",
    "            alpha=0.0001,\n",
    "            tol=1e-5,\n",
    "            n_iter_no_change=50,\n",
    "            max_iter=max_iter_per_run,\n",
    "            random_state=random_state,\n",
    "            solver='sgd',\n",
    "            batch_size=batch_size,\n",
    "            learning_rate='adaptive', learning_rate_init=lr, momentum=0.0,\n",
    "            )\n",
    "        with warnings.catch_warnings(record=True) as warn_list:\n",
    "            mlp.fit(x_tr_N2, y_tr_N)\n",
    "        mlp.elapsed_time_sec_ = time.time() - start_time_sec\n",
    "        mlp.n_epoch_ = len(mlp.loss_curve_)\n",
    "        mlp.did_converge = True if len(warn_list) == 0 else False\n",
    "\n",
    "        # Add to the list\n",
    "        mlp_list_by_lr_25[lr].append(mlp)\n",
    "        \n",
    "        # Pretty print summary of this run\n",
    "        print('finished run %2d/%d after %5.1f sec | %13s after %4d iter | tr loss % .3f | lr = %.3f' % (\n",
    "            run_id + 1, n_runs,\n",
    "            mlp.elapsed_time_sec_,\n",
    "            'converged' if mlp.did_converge else 'NOT converged',\n",
    "            mlp.n_epoch_,\n",
    "            mlp.loss_ / np.log(2.0),\n",
    "            lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_25, x='elapsed_time_sec', xlim_max=65, ylim_max=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Plot loss vs iterations\n",
    "\n",
    "Just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_10000, x='epochs', xlim_max=2000)\n",
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_500, x='epochs', xlim_max=2000)\n",
    "pretty_plot_loss_curve_vs_x(mlp_list_by_lr_25, x='epochs', xlim_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
