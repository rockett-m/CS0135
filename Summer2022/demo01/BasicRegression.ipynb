{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linear/Polynomial Regression Demo\n",
    "\n",
    "Demonstrates the basic use of the linear regression model from sklearn:\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "Simple one-dimensional data prediction.  Basic model and data based upon an example by S. Srinidhi:\n",
    "\n",
    "[https://medium.com/@contactsunny/linear-regression-in-python-using-scikit-learn-f0f7b125a204](https://medium.com/@contactsunny/linear-regression-in-python-using-scikit-learn-f0f7b125a204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "pandas.set_option('display.precision', 2) # number precision for pandas\n",
    "plt.style.use('seaborn') # pretty matplotlib plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and display data-set\n",
    "data = pandas.read_csv('salaryData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out the inputs (x) and outputs (y)\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs (x) should be a columnar arrangement, one data-point per row\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs (y) is a single array of values, one per row of x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data: 2/3 to train model, 1/3 to test for validation\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the basic linear model, fit to the training data;\n",
    "# finds min-error coefficient for x\n",
    "linearRegression = LinearRegression()\n",
    "linearRegression.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predictions the model makes on input sets; \n",
    "# compare predictions on yTrain to correct values, above\n",
    "yTestPredict = linearRegression.predict(xTest)\n",
    "yTrainPredict = linearRegression.predict(xTrain)\n",
    "yTrainPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted line for training set\n",
    "plt.scatter(xTrain, yTrain, color='red')\n",
    "plt.plot(xTrain, yTrainPredict, color='blue')\n",
    "plt.title('Salary vs Experience (Train data)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall squared error on training\n",
    "mean_squared_error(yTrain, yTrainPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted line for test data\n",
    "plt.scatter(xTest, yTest, color='red')\n",
    "plt.plot(xTest, yTestPredict, color='blue')\n",
    "plt.title('Salary vs Experience (Test data)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall squared error on testing;\n",
    "# will *generally* be somewhat larger than training error,\n",
    "# but should be close (if we are doing things correctly)\n",
    "# and *can* be less (due to luck)\n",
    "mean_squared_error(yTest, yTestPredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a higher-order polynomial regression\n",
    "\n",
    "We can modify our input data features to increase their dimensionality.  One way to do this is to use the `PolynomialFeatures` libary, which can transform data by adding higher-degree polynomial values for each input point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for proper plots of the higher-order values, it is necessary to first sort\n",
    "# the data by the original x-component (ensuring that the matching y-values\n",
    "# are sorted accordingly, as well)\n",
    "import operator\n",
    "sorted_zip = sorted(zip(xTrain, yTrain), key=operator.itemgetter(0))\n",
    "xTrain, yTrain = zip(*sorted_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic degree-2 transform adds the square of the data to the original\n",
    "# (along with a bias vector of 1's for the 0th weight)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "xTrain2 = poly2.fit_transform(xTrain)\n",
    "xTrain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegression2 = LinearRegression()\n",
    "linearRegression2.fit(xTrain2, yTrain)\n",
    "yTrainPredict2 = linearRegression2.predict(xTrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted line for training set\n",
    "plt.scatter(xTrain, yTrain, color='red')\n",
    "\n",
    "plt.plot(xTrain, yTrainPredict2, color='blue')\n",
    "plt.title('Salary vs Experience (Train data)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the order-2 polynomial doesn't make much difference here,\n",
    "# we can now extend things to include even higher-order terms\n",
    "poly3 = PolynomialFeatures(degree=3)\n",
    "xTrain3 = poly3.fit_transform(xTrain)\n",
    "linearRegression3 = LinearRegression()\n",
    "linearRegression3.fit(xTrain3, yTrain)\n",
    "yTrainPredict3 = linearRegression3.predict(xTrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted line for training set\n",
    "plt.scatter(xTrain, yTrain, color='red')\n",
    "\n",
    "plt.plot(xTrain, yTrainPredict3, color='blue')\n",
    "plt.title('Salary vs Experience (Train data)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the order-2 polynomial doesn't make much difference here,\n",
    "# we can now extend things to include even higher-order terms\n",
    "poly10 = PolynomialFeatures(degree=10)\n",
    "xTrain10 = poly10.fit_transform(xTrain)\n",
    "linearRegression10 = LinearRegression()\n",
    "linearRegression10.fit(xTrain10, yTrain)\n",
    "yTrainPredict10 = linearRegression10.predict(xTrain10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted line for training set\n",
    "plt.scatter(xTrain, yTrain, color='red')\n",
    "\n",
    "plt.plot(xTrain, yTrainPredict10, color='blue')\n",
    "plt.title('Salary vs Experience (Train data)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for overfitting\n",
    "\n",
    "Are polynomial transformations always a good idea?  Not so much.  As we increase the dimensions, it is increasingly possible that we move our model from a robust general solution to one that is highly overfit to the training data.  This can start to show up as training error continues to decrease, while testing error goes the other way at some point.\n",
    "\n",
    "**Note**: in this example, we are normalizing the MSE by dividing by the maximum $$y$$-value seen.  This is not normally the procedure we would follow, but here it makes comparing the results a bit easier, since we don't have such large numbers (the comparison is still telling, since each result is scaled by the same fixed amount.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,12):\n",
    "    polyTransform = PolynomialFeatures(degree=i)\n",
    "    xTrainTransform = polyTransform.fit_transform(xTrain)\n",
    "    xTestTransform = polyTransform.fit_transform(xTest)\n",
    "    linearRegressionModel = LinearRegression()\n",
    "    linearRegressionModel.fit(xTrainTransform, yTrain)\n",
    "    yTrainPredict = linearRegressionModel.predict(xTrainTransform)\n",
    "    yTestPredict = linearRegressionModel.predict(xTestTransform)\n",
    "    mseTrain = mean_squared_error(yTrain, yTrainPredict) / max(y)\n",
    "    mseTest = mean_squared_error(yTest, yTestPredict) / max(y)\n",
    "    print('Degree {:02d}: Train: {:f}, Test: {:f}'.format(i, mseTrain, mseTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food for thought\n",
    "In the above code, increasing the polynomial degree even higher, to 20 or 30, can show trends that are quite different.  At some point the both sorts of error actually start to grow.  Why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
