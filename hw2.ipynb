{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP68es0F2sK/ZHpZXThLmGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockett-m/CS0135/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jtCDJz_I0pKC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A coding assignment that combines splitting data into train, test, and validation sets using scikit-learn (sklearn):\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "You are given a dataset containing information about different types of iris flowers (sepal length, sepal width,\n",
        "petal length, and petal width). Your task is to build a machine learning model to classify the iris flowers into\n",
        "three different classes (setosa, versicolor, virginica) based on their physical characteristics.\n",
        "\n",
        "You are to do the following:\n",
        "Step 1: Load the iris dataset\n",
        "    Use the following code to load the iris dataset into a pandas dataframe:\n",
        "\n",
        "Step 2: Split the data into training, validation, and test sets\n",
        "    Use the following code to split the data into training (60%), validation (20%), and test (20%) sets:\n",
        "\n",
        "Step 3: Train a classifier (this function is provided for you)\n",
        "    Use the following code to train a support vector machine (SVM) classifier on the training set:\n",
        "\n",
        "Step 4: Evaluate the model on the validation set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the validation set:\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "Step 5: Evaluate the model on the test set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the test set:\n",
        "\n",
        "This assignment is designed to help you understand the basics of splitting data into train, validation, and test sets,\n",
        "as well as training a simple machine learning model using scikit-learn. The assignment can be extended by trying\n",
        "different classifiers, tuning the parameters, or exploring the dataset further.\n",
        "\n",
        "Step 6: Then, do the same for K-Fold cross validation.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "5. class:\n",
        "-- Iris Setosa  \n",
        "-- Iris Versicolour  \n",
        "-- Iris Virginica  \n",
        "https://archive.ics.uci.edu/ml/datasets/iris"
      ],
      "metadata": {
        "id": "AVKFPzuF2AZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "'feature_names' = list\n",
        "['sepal length (cm)',\n",
        " 'sepal width (cm)',\n",
        " 'petal length (cm)',\n",
        " 'petal width (cm)']\n",
        "\n",
        "'target_names' =\n",
        " array(['setosa', 'versicolor', 'virginica'],\n"
      ],
      "metadata": {
        "id": "CIYcuIRP6kXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "Returns:\n",
        "    df_out (pandas dataframe): The iris dataset\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#TODO load data, convert to dataframe, and return\n",
        "iris = load_iris()\n",
        "print(type(iris))\n",
        "print(iris.items, len(iris))\n",
        "\n",
        "for elem in iris:\n",
        "    print(f'{elem = }')\n",
        "\n",
        "# print(f'{iris.data = }')\n",
        "# print(f'{iris.data = }')\n",
        "data = iris.data # (150, 4)\n",
        "target = iris.target # (150,)\n",
        "target_names = iris.target_names # (3,)\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "\n",
        "df_data = pd.DataFrame(data=iris.data)\n",
        "# df_data.head()\n",
        "# df_data.describe()\n",
        "\n",
        "# iris\n",
        "# iris.info()\n",
        "# print(iris)\n",
        "# df = pd.read()\n",
        "# return df_data\n",
        "# pass\n",
        "\"\"\"\n",
        "Classes: 3\n",
        "Samples per class: 50\n",
        "Samples total: 150\n",
        "Dimensionality: 4\n",
        "Features: real, positive\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "zAMEANZK4kxR",
        "outputId": "38edc269-7ca2-49dc-e147-81cddd788d02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils.Bunch'>\n",
            "<built-in method items of Bunch object at 0x7f399adfff90> 8\n",
            "elem = 'data'\n",
            "elem = 'target'\n",
            "elem = 'frame'\n",
            "elem = 'target_names'\n",
            "elem = 'DESCR'\n",
            "elem = 'feature_names'\n",
            "elem = 'filename'\n",
            "elem = 'data_module'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nClasses: 3\\nSamples per class: 50\\nSamples total: 150\\nDimensionality: 4\\nFeatures: real, positive\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "        df_out (pandas dataframe): The iris dataset\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    #TODO load data, convert to dataframe, and return\n",
        "    iris = load_iris()\n",
        "    df_data = pd.DataFrame(iris.data)\n",
        "    # df_data.head()\n",
        "    # df_data.describe()\n",
        "\n",
        "    # iris\n",
        "    # iris.info()\n",
        "    # print(iris)\n",
        "    # df = pd.read()\n",
        "    return df_data\n",
        "    # pass"
      ],
      "metadata": {
        "id": "YuDWJlng1VLa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df_in):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets\n",
        "\n",
        "    Parameters:\n",
        "    df_in (pandas dataframe): The iris dataset\n",
        "\n",
        "    Returns:\n",
        "    X_train_out (numpy array): Training set features\n",
        "    X_val_out (numpy array): Validation set features\n",
        "    X_test_out (numpy array): Test set features\n",
        "    y_train_out (numpy array): Training set targets\n",
        "    y_val_out (numpy array): Validation set targets\n",
        "    y_test_out (numpy array): Test set targets\n",
        "    \"\"\"\n",
        "    # TODO split train and test, and then split train and val\n",
        "    X_train_out = X_val_out = X_test_out = y_train_out = y_val_out = y_test_out = np.float(0)\n",
        "    \n",
        "    return X_train_out, X_val_out, X_test_out, y_train_out, y_val_out, y_test_out"
      ],
      "metadata": {
        "id": "7i7qv9bD1b0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(X_train_in, y_train_in):\n",
        "    \"\"\"\n",
        "    Train a support vector machine (SVM) classifier on the training set\n",
        "\n",
        "    Parameters:\n",
        "    X_train_in (numpy array): Training set features\n",
        "    y_train_in (numpy array): Training set targets\n",
        "\n",
        "    Returns:\n",
        "    clf_out (SVM classifier): Trained SVM classifier\n",
        "    \"\"\"\n",
        "    clf_out = SVC(kernel='linear', C=1, random_state=0)\n",
        "    clf_out.fit(X_train_in, y_train_in)\n",
        "\n",
        "    return clf_out"
      ],
      "metadata": {
        "id": "2U9hFRJO0xKo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(clf, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the trained classifier on a given set\n",
        "\n",
        "    Parameters:\n",
        "    clf (SVM classifier): Trained SVM classifier\n",
        "    X (numpy array): Features of the set to evaluate the classifier on\n",
        "    y (numpy array): Targets of the set to evaluate the classifier on\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (float): Accuracy of the classifier on the given set\n",
        "    \"\"\"\n",
        "    # TODO evaluate the model on data\n",
        "    accuracy_test_out = ''\n",
        "\n",
        "    return accuracy_test_out"
      ],
      "metadata": {
        "id": "bl08M4k71lYj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_validation(df, k=5):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross validation on the iris dataset\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas dataframe): The iris dataset\n",
        "    k (int, optional): The number of folds. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (list): A list of accuracy_out scores for each fold\n",
        "    \"\"\"\n",
        "    # TODO do 5-fold on dataset and return the accuracy across the five folds\n",
        "    accuracy_five_folds = ''\n",
        "    \n",
        "    return accuracy_five_folds"
      ],
      "metadata": {
        "id": "Wv6QIIz71nm9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the iris dataset\n",
        "    df_data = load_data()\n",
        "\n",
        "    # Train, Val, Test\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_data) \n",
        "    # test data does not change! (20%)\n",
        "    # 80% to training\n",
        "    # take 20% of training data into val data\n",
        "    # call split twice !!!\n",
        "\n",
        "    # Train a classifier\n",
        "    clf = train_classifier(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracy_val = evaluate_model(clf, X_val, y_val)\n",
        "    print(\"Accuracy on validation set:\", accuracy_val)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy_test = evaluate_model(clf, X_test, y_test)\n",
        "    print(\"Accuracy on test set:\", accuracy_test)\n",
        "\n",
        "    # K-FOLD\n",
        "    # Perform k-fold cross validation\n",
        "    accuracy = k_fold_cross_validation(df_data)\n",
        "\n",
        "    # Print the accuracy scores for each fold\n",
        "    for i, acc in enumerate(accuracy):\n",
        "        print(\"Accuracy on fold\", i + 1, \":\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "5Zj8idL21pWL",
        "outputId": "98f4f50c-5cb9-47c5-91ff-85e9336b3e1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ff0f6a4f4ab3>:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train_out = X_val_out = X_test_out = y_train_out = y_val_out = y_test_out = np.float(0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d349681d363d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train a classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Evaluate the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-78ab220b82d0>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(X_train_in, y_train_in)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0mclf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mclf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclf_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    762\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=0.0.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQQ606PQ2YCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}