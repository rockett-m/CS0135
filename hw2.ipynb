{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUfUIQqsDhdEbZuC806ZqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockett-m/CS0135/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtCDJz_I0pKC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A coding assignment that combines splitting data into train, test, and validation sets using scikit-learn (sklearn):\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "You are given a dataset containing information about different types of iris flowers (sepal length, sepal width,\n",
        "petal length, and petal width). Your task is to build a machine learning model to classify the iris flowers into\n",
        "three different classes (setosa, versicolor, virginica) based on their physical characteristics.\n",
        "\n",
        "You are to do the following:\n",
        "Step 1: Load the iris dataset\n",
        "    Use the following code to load the iris dataset into a pandas dataframe:\n",
        "\n",
        "Step 2: Split the data into training, validation, and test sets\n",
        "    Use the following code to split the data into training (60%), validation (20%), and test (20%) sets:\n",
        "\n",
        "Step 3: Train a classifier (this function is provided for you)\n",
        "    Use the following code to train a support vector machine (SVM) classifier on the training set:\n",
        "\n",
        "Step 4: Evaluate the model on the validation set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the validation set:\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "Step 5: Evaluate the model on the test set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the test set:\n",
        "\n",
        "This assignment is designed to help you understand the basics of splitting data into train, validation, and test sets,\n",
        "as well as training a simple machine learning model using scikit-learn. The assignment can be extended by trying\n",
        "different classifiers, tuning the parameters, or exploring the dataset further.\n",
        "\n",
        "Step 6: Then, do the same for K-Fold cross validation.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "5. class:\n",
        "-- Iris Setosa  \n",
        "-- Iris Versicolour  \n",
        "-- Iris Virginica  \n",
        "https://archive.ics.uci.edu/ml/datasets/iris"
      ],
      "metadata": {
        "id": "AVKFPzuF2AZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "'feature_names' = list\n",
        "['sepal length (cm)',\n",
        " 'sepal width (cm)',\n",
        " 'petal length (cm)',\n",
        " 'petal width (cm)']\n",
        "\n",
        "'target_names' =\n",
        " array(['setosa', 'versicolor', 'virginica'],\n"
      ],
      "metadata": {
        "id": "CIYcuIRP6kXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_dataset(debug=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "        df_out (pandas dataframe): The iris dataset\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    #TODO load data, convert to dataframe, and return\n",
        "    iris = load_iris()\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{type(iris) = }, {iris.items = }, {len(iris) = }')\n",
        "\n",
        "        for elem in iris:\n",
        "            print(f'{elem = } {iris[elem] = }')\n",
        "\n",
        "    # print(f'{iris.data = }')\n",
        "    # print(f'{iris.data = }')\n",
        "    data = iris.data # (150, 4)\n",
        "    target = iris.target # (150,)\n",
        "    target_names = iris.target_names # (3,)\n",
        "    feature_names = iris.feature_names\n",
        "\n",
        "    df_data = pd.DataFrame(data)\n",
        "\n",
        "    if debug:\n",
        "\n",
        "        print(f'{df_data.head() = }')\n",
        "        print(f'{df_data.describe() = }')\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{df_data = }')\n",
        "\n",
        "    # df_data = pd.DataFrame(data=iris.data)\n",
        "\n",
        "    # test data does not change! (20%)\n",
        "    # 80% to training\n",
        "    # take 20% of training data into val data\n",
        "    # call split twice !!!\n",
        "    # test = df_data[len(df_data):]  # 80% to the end\n",
        "\n",
        "    # X - data\n",
        "    # X_data = iris.data[:][:]\n",
        "    # Y_data = iris.data[:][1]\n",
        "    # Y - target\n",
        "    # target\n",
        "\n",
        "    # iris\n",
        "    # iris.info()\n",
        "    # print(iris)\n",
        "    # df = pd.read()\n",
        "    # return df_data\n",
        "    # pass\n",
        "    \"\"\"\n",
        "    Classes: 3\n",
        "    Samples per class: 50\n",
        "    Samples total: 150\n",
        "    Dimensionality: 4\n",
        "    Features: real, positive\n",
        "    \"\"\"\n",
        "view_dataset(debug=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAMEANZK4kxR",
        "outputId": "2b3d2f2c-119e-4b98-f7f2-a5cfcc137bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_data.head() =      0    1    2    3\n",
            "0  5.1  3.5  1.4  0.2\n",
            "1  4.9  3.0  1.4  0.2\n",
            "2  4.7  3.2  1.3  0.2\n",
            "3  4.6  3.1  1.5  0.2\n",
            "4  5.0  3.6  1.4  0.2\n",
            "df_data.describe() =                 0           1           2           3\n",
            "count  150.000000  150.000000  150.000000  150.000000\n",
            "mean     5.843333    3.057333    3.758000    1.199333\n",
            "std      0.828066    0.435866    1.765298    0.762238\n",
            "min      4.300000    2.000000    1.000000    0.100000\n",
            "25%      5.100000    2.800000    1.600000    0.300000\n",
            "50%      5.800000    3.000000    4.350000    1.300000\n",
            "75%      6.400000    3.300000    5.100000    1.800000\n",
            "max      7.900000    4.400000    6.900000    2.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "        df_out (pandas dataframe): The iris dataset\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    #TODO load data, convert to dataframe, and return\n",
        "    iris = load_iris()\n",
        "    df_out = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df_out['target'] = iris.target\n",
        "\n",
        "    return df_out\n",
        "# df_loaded = load_data()"
      ],
      "metadata": {
        "id": "YuDWJlng1VLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df_in):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets\n",
        "\n",
        "    Parameters:\n",
        "    df_in (pandas dataframe): The iris dataset\n",
        "\n",
        "    Returns:\n",
        "    X_train_out (numpy array): Training set features\n",
        "    X_val_out   (numpy array): Validation set features\n",
        "    X_test_out  (numpy array): Test set features\n",
        "    y_train_out (numpy array): Training set targets\n",
        "    y_val_out   (numpy array): Validation set targets\n",
        "    y_test_out  (numpy array): Test set targets\n",
        "    \"\"\"\n",
        "    # TODO split train and test, and then split train and val\n",
        "\n",
        "    # test data does not change! (20%)\n",
        "    # 80% to training\n",
        "    # take 20% of training data into val data\n",
        "    # call split twice !!!\n",
        "\n",
        "    # 150, 4 full df in\n",
        "    # 120, 4 train dims [0:119, 4] # first 120 rows of df_in\n",
        "    #  30, 4 val dims   [90:119, 4] # last 30 rows of train dims\n",
        "    #  90, 4 train dims [0:89, 4]  # after div into train and val these are final train dims\n",
        "    #  30, 4 test dims (lock it away!) [120:149, 4] # last 30 rows of df_in\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X= , y= , test_size=0.8)\n",
        "\n",
        "    # separate X and y from dataframe\n",
        "    X, y = df_in.iloc[:,:4], df_in['target']\n",
        "    # send 80% to train and 20% to test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80) \n",
        "    # 25% of (80% df in train) to get 20% total for val\n",
        "    X_train_out, X_val_out, y_train_out, y_val_out = train_test_split(X_train, y_train, train_size=0.75)\n",
        "\n",
        "    return X_train_out, X_val_out, X_test, y_train_out, y_val_out, y_test\n",
        "\n",
        "# X_train_out, X_val_out, X_test, y_train_out, y_val_out, y_test = split_data(df_out)"
      ],
      "metadata": {
        "id": "7i7qv9bD1b0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(X_train_in, y_train_in):\n",
        "    \"\"\"\n",
        "    Train a support vector machine (SVM) classifier on the training set\n",
        "\n",
        "    Parameters:\n",
        "    X_train_in (numpy array): Training set features\n",
        "    y_train_in (numpy array): Training set targets\n",
        "\n",
        "    Returns:\n",
        "    clf_out (SVM classifier): Trained SVM classifier\n",
        "    \"\"\"\n",
        "    clf_out = SVC(kernel='linear', C=1, random_state=0)\n",
        "    clf_out.fit(X_train_in, y_train_in)\n",
        "\n",
        "    return clf_out"
      ],
      "metadata": {
        "id": "2U9hFRJO0xKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(clf, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the trained classifier on a given set\n",
        "\n",
        "    Parameters:\n",
        "    clf (SVM classifier): Trained SVM classifier\n",
        "    X (numpy array): Features of the set to evaluate the classifier on\n",
        "    y (numpy array): Targets of the set to evaluate the classifier on\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (float): Accuracy of the classifier on the given set\n",
        "    \"\"\"\n",
        "    # TODO evaluate the model on data\n",
        "    accuracy_test_out = clf.score(X, y)\n",
        "\n",
        "    return accuracy_test_out"
      ],
      "metadata": {
        "id": "bl08M4k71lYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_validation(df, k=5):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross validation on the iris dataset\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas dataframe): The iris dataset\n",
        "    k (int, optional): The number of folds. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (list): A list of accuracy_out scores for each fold\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    from sklearn.model_selection import KFold\n",
        "    # TODO do 5-fold on dataset and return the accuracy across the five folds\n",
        "    # X=iris.data, y=iris.target\n",
        "    X, y = df.iloc[:,:4], df['target']\n",
        "    print(X)\n",
        "    clf_out = SVC(kernel='linear', C=1, random_state=0)\n",
        "\n",
        "    # (n_splits=5, *, shuffle=False, random_state=None)\n",
        "    accuracy_five_folds = []\n",
        "    kf = KFold(n_splits=k) # k == n_splits\n",
        "\n",
        "\n",
        "    X = df.iloc[:,:4].values\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        clf = train_classifier(X_train, y_train)\n",
        "        accuracy_val = evaluate_model(clf, X_test, y_test)\n",
        "        accuracy_five_folds.append(accuracy_val)\n",
        "\n",
        "    # accuracy_five_folds = cross_val_score(clf_out, X=iris.data, y=iris.target, cv=k, scoring=\"accuracy\")\n",
        "    # accuracy_five_folds = cross_val_score(clf_out, X, y, cv=k, scoring=\"accuracy\")\n",
        "\n",
        "    return accuracy_five_folds"
      ],
      "metadata": {
        "id": "Wv6QIIz71nm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the iris dataset\n",
        "    df_in = load_data()\n",
        "\n",
        "    # Train, Val, Test\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_in)\n",
        "\n",
        "    # Train a classifier\n",
        "    clf = train_classifier(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracy_val = evaluate_model(clf, X_val, y_val)\n",
        "    print(\"Accuracy on validation set:\", accuracy_val)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy_test = evaluate_model(clf, X_test, y_test)\n",
        "    print(\"Accuracy on test set:\", accuracy_test)\n",
        "\n",
        "    # K-FOLD\n",
        "    # Perform k-fold cross validation\n",
        "    accuracy = k_fold_cross_validation(df_in)\n",
        "\n",
        "    # Print the accuracy scores for each fold\n",
        "    for idx, acc_score in enumerate(accuracy):\n",
        "        print(\"Accuracy on fold\", idx + 1, \":\", acc_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Zj8idL21pWL",
        "outputId": "8e147898-72d0-4dfe-9680-c13ca0b05c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation set: 0.9666666666666667\n",
            "Accuracy on test set: 1.0\n",
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-f9c2ace7ada2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Load the iris dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Train, Val, Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-b6d71c7ea986>\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(df, k)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mPerform\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfold\u001b[0m \u001b[0mcross\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0miris\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mParameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3416\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_column_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[1;32m   1269\u001b[0m         \u001b[0mTransform\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mof\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0man\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m         \"\"\"\n\u001b[1;32m   1333\u001b[0m         \u001b[0mCheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([ 30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\\n            ...\\n            140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\\n           dtype='int64', length=120)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTfySfbMPh8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}