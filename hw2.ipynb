{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockett-m/CS0135/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A coding assignment that combines splitting data into train, test, and validation sets using scikit-learn (sklearn):\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "You are given a dataset containing information about different types of iris flowers (sepal length, sepal width,\n",
        "petal length, and petal width). Your task is to build a machine learning model to classify the iris flowers into\n",
        "three different classes (setosa, versicolor, virginica) based on their physical characteristics.\n",
        "\n",
        "You are to do the following:\n",
        "Step 1: Load the iris dataset\n",
        "    Use the following code to load the iris dataset into a pandas dataframe:\n",
        "\n",
        "Step 2: Split the data into training, validation, and test sets\n",
        "    Use the following code to split the data into training (60%), validation (20%), and test (20%) sets:\n",
        "\n",
        "Step 3: Train a classifier (this function is provided for you)\n",
        "    Use the following code to train a support vector machine (SVM) classifier on the training set:\n",
        "\n",
        "Step 4: Evaluate the model on the validation set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the validation set:\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "Step 5: Evaluate the model on the test set\n",
        "    Use the following code to evaluate the performance of the trained classifier on the test set:\n",
        "\n",
        "This assignment is designed to help you understand the basics of splitting data into train, validation, and test sets,\n",
        "as well as training a simple machine learning model using scikit-learn. The assignment can be extended by trying\n",
        "different classifiers, tuning the parameters, or exploring the dataset further.\n",
        "\n",
        "Step 6: Then, do the same for K-Fold cross validation.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "        df_out (pandas dataframe): The iris dataset\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    #TODO load data, convert to dataframe, and return\n",
        "    iris = load_iris()\n",
        "    df_out = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df_out['target'] = iris.target\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def split_data(df_in):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets\n",
        "\n",
        "    Parameters:\n",
        "    df_in (pandas dataframe): The iris dataset\n",
        "\n",
        "    Returns:\n",
        "    X_train_out (numpy array): Training set features\n",
        "    X_val_out   (numpy array): Validation set features\n",
        "    X_test_out  (numpy array): Test set features\n",
        "    y_train_out (numpy array): Training set targets\n",
        "    y_val_out   (numpy array): Validation set targets\n",
        "    y_test_out  (numpy array): Test set targets\n",
        "    \"\"\"\n",
        "    # TODO split train and test, and then split train and val\n",
        "\n",
        "    # test data does not change! (20%)\n",
        "    # 80% to training\n",
        "    # take 20% of training data into val data\n",
        "    # call split twice !!!\n",
        "\n",
        "    # 150, 4 full df in\n",
        "    # 120, 4 train dims [0:119, 4] # first 120 rows of df_in\n",
        "    #  30, 4 val dims   [90:119, 4] # last 30 rows of train dims\n",
        "    #  90, 4 train dims [0:89, 4]  # after div into train and val these are final train dims\n",
        "    #  30, 4 test dims (lock it away!) [120:149, 4] # last 30 rows of df_in\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X= , y= , test_size=0.8)\n",
        "\n",
        "    # separate X and y from dataframe\n",
        "    X, y = df_in.iloc[:,:4], df_in['target']\n",
        "    # send 80% to train and 20% to test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80) \n",
        "    # 25% of (80% df in train) to get 20% total for val\n",
        "    X_train_out, X_val_out, y_train_out, y_val_out = train_test_split(X_train, y_train, train_size=0.75)\n",
        "\n",
        "    return X_train_out, X_val_out, X_test, y_train_out, y_val_out, y_test\n",
        "\n",
        "\n",
        "def train_classifier(X_train_in, y_train_in):\n",
        "    \"\"\"\n",
        "    Train a support vector machine (SVM) classifier on the training set\n",
        "\n",
        "    Parameters:\n",
        "    X_train_in (numpy array): Training set features\n",
        "    y_train_in (numpy array): Training set targets\n",
        "\n",
        "    Returns:\n",
        "    clf_out (SVM classifier): Trained SVM classifier\n",
        "    \"\"\"\n",
        "    clf_out = SVC(kernel='linear', C=1, random_state=0)\n",
        "    clf_out.fit(X_train_in, y_train_in)\n",
        "\n",
        "    return clf_out\n",
        "\n",
        "\n",
        "def evaluate_model(clf, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the trained classifier on a given set\n",
        "\n",
        "    Parameters:\n",
        "    clf (SVM classifier): Trained SVM classifier\n",
        "    X (numpy array): Features of the set to evaluate the classifier on\n",
        "    y (numpy array): Targets of the set to evaluate the classifier on\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (float): Accuracy of the classifier on the given set\n",
        "    \"\"\"\n",
        "    # TODO evaluate the model on data\n",
        "    accuracy_test_out = clf.score(X, y)\n",
        "\n",
        "    return accuracy_test_out\n",
        "\n",
        "\n",
        "def k_fold_cross_validation(df, k=5):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross validation on the iris dataset\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas dataframe): The iris dataset\n",
        "    k (int, optional): The number of folds. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    accuracy_out (list): A list of accuracy_out scores for each fold\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    from sklearn.model_selection import KFold\n",
        "    # TODO do 5-fold on dataset and return the accuracy across the five folds\n",
        "    # X=iris.data, y=iris.target\n",
        "    X, y = df.iloc[:,:4].values, df['target']\n",
        "    clf_out = SVC(kernel='linear', C=1, random_state=0)\n",
        "\n",
        "    # (n_splits=5, *, shuffle=False, random_state=None)\n",
        "    accuracy_five_folds = []\n",
        "    kf = KFold(n_splits=k) # k == n_splits\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        clf = train_classifier(X_train, y_train)\n",
        "        accuracy_val = evaluate_model(clf, X_test, y_test)\n",
        "        accuracy_five_folds.append(accuracy_val)\n",
        "\n",
        "    # accuracy_five_folds = cross_val_score(clf_out, X=iris.data, y=iris.target, cv=k, scoring=\"accuracy\")\n",
        "    # accuracy_five_folds = cross_val_score(clf_out, X, y, cv=k, scoring=\"accuracy\")\n",
        "\n",
        "    return accuracy_five_folds\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the iris dataset\n",
        "    df_in = load_data()\n",
        "\n",
        "    # Train, Val, Test\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_in)\n",
        "\n",
        "    # Train a classifier\n",
        "    clf = train_classifier(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracy_val = evaluate_model(clf, X_val, y_val)\n",
        "    print(\"Accuracy on validation set:\", accuracy_val)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy_test = evaluate_model(clf, X_test, y_test)\n",
        "    print(\"Accuracy on test set:\", accuracy_test)\n",
        "\n",
        "    # K-FOLD\n",
        "    # Perform k-fold cross validation\n",
        "    accuracy = k_fold_cross_validation(df_in)\n",
        "\n",
        "    # Print the accuracy scores for each fold\n",
        "    for idx, acc_score in enumerate(accuracy):\n",
        "        print(\"Accuracy on fold\", idx + 1, \":\", acc_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zj8idL21pWL",
        "outputId": "0fd667c6-c4e7-4e6f-d3cb-4993656df7d1"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation set: 0.9333333333333333\n",
            "Accuracy on test set: 1.0\n",
            "Accuracy on fold 1 : 1.0\n",
            "Accuracy on fold 2 : 1.0\n",
            "Accuracy on fold 3 : 0.8666666666666667\n",
            "Accuracy on fold 4 : 1.0\n",
            "Accuracy on fold 5 : 0.8666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribute Information:\n",
        "\n",
        "sepal length in cm\n",
        "sepal width in cm\n",
        "petal length in cm\n",
        "petal width in cm\n",
        "class:\n",
        "-- Iris Setosa\n",
        "-- Iris Versicolour\n",
        "-- Iris Virginica\n",
        "https://archive.ics.uci.edu/ml/datasets/iris\n",
        "\n",
        "'feature_names' = list ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
        "\n",
        "'target_names' = array(['setosa', 'versicolor', 'virginica'],"
      ],
      "metadata": {
        "id": "4i--5YnharYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def view_dataset(debug=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Load the iris dataset and return it as a pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "        df_out (pandas dataframe): The iris dataset\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    #TODO load data, convert to dataframe, and return\n",
        "    iris = load_iris()\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{type(iris) = }, {iris.items = }, {len(iris) = }')\n",
        "\n",
        "        for elem in iris:\n",
        "            print(f'{elem = } {iris[elem] = }')\n",
        "\n",
        "    # print(f'{iris.data = }')\n",
        "    # print(f'{iris.data = }')\n",
        "    data = iris.data # (150, 4)\n",
        "    target = iris.target # (150,)\n",
        "    target_names = iris.target_names # (3,)\n",
        "    feature_names = iris.feature_names\n",
        "\n",
        "    df_data = pd.DataFrame(data)\n",
        "\n",
        "    if debug:\n",
        "\n",
        "        print(f'{df_data.head() = }')\n",
        "        print(f'{df_data.describe() = }')\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{df_data = }')\n",
        "\n",
        "    # df_data = pd.DataFrame(data=iris.data)\n",
        "\n",
        "    # test data does not change! (20%)\n",
        "    # 80% to training\n",
        "    # take 20% of training data into val data\n",
        "    # call split twice !!!\n",
        "    # test = df_data[len(df_data):]  # 80% to the end\n",
        "\n",
        "    # X - data\n",
        "    # X_data = iris.data[:][:]\n",
        "    # Y_data = iris.data[:][1]\n",
        "    # Y - target\n",
        "    # target\n",
        "\n",
        "    # iris\n",
        "    # iris.info()\n",
        "    # print(iris)\n",
        "    # df = pd.read()\n",
        "    # return df_data\n",
        "    # pass\n",
        "    \"\"\"\n",
        "    Classes: 3\n",
        "    Samples per class: 50\n",
        "    Samples total: 150\n",
        "    Dimensionality: 4\n",
        "    Features: real, positive\n",
        "    \"\"\"\n",
        "view_dataset(debug=True, verbose=False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAMEANZK4kxR",
        "outputId": "2b3d2f2c-119e-4b98-f7f2-a5cfcc137bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_data.head() =      0    1    2    3\n",
            "0  5.1  3.5  1.4  0.2\n",
            "1  4.9  3.0  1.4  0.2\n",
            "2  4.7  3.2  1.3  0.2\n",
            "3  4.6  3.1  1.5  0.2\n",
            "4  5.0  3.6  1.4  0.2\n",
            "df_data.describe() =                 0           1           2           3\n",
            "count  150.000000  150.000000  150.000000  150.000000\n",
            "mean     5.843333    3.057333    3.758000    1.199333\n",
            "std      0.828066    0.435866    1.765298    0.762238\n",
            "min      4.300000    2.000000    1.000000    0.100000\n",
            "25%      5.100000    2.800000    1.600000    0.300000\n",
            "50%      5.800000    3.000000    4.350000    1.300000\n",
            "75%      6.400000    3.300000    5.100000    1.800000\n",
            "max      7.900000    4.400000    6.900000    2.500000\n"
          ]
        }
      ]
    }
  ]
}